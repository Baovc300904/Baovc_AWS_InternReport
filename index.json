[{"uri":"https://thienluhoan.github.io/workshop-template/3-blogstranslated/3.1-blog1/","title":"Blog 1","tags":[],"description":"","content":" ‚ö†Ô∏è Note: The information below is for reference purposes only. Please do not copy verbatim for your report, including this warning.\nExploring the latest features of the Amazon Q Developer CLI by Brian Beach on 20 MAY 2025 in Amazon Q Developer, Announcements\nIt\u0026rsquo;s been a few weeks since my last post about the Amazon Q Developer Command Line Interface (CLI), and I\u0026rsquo;m excited to share all the great new features and improvements the team has been working on. The CLI has been evolving rapidly with a focus on enhancing user experience, improving context management, and adding powerful new capabilities. In this post, I\u0026rsquo;ll walk you through the most significant changes that make the Amazon Q Developer CLI even more powerful and user-friendly.\nConversation Persistence One of the most requested features has been the ability to persist conversations, and I\u0026rsquo;m thrilled to share that this is now available. With the new q chat --resume command, your conversations are now automatically saved by a working directory. This means you can pick up right where you left off when you return to a project, without having to rebuild context or repeat information.\nQ Developer has also added two new commands to give you more control over your conversation state:\n/save allows you to explicitly save the current conversation state /load lets you restore a previously saved conversation These commands make it easier to manage multiple conversation threads related to different aspects of your project. You can save a conversation about one feature, switch to working on something else, and then load the previous conversation when you\u0026rsquo;re ready to continue.\nMCP and Tool Use Enhancements The Model Context Protocol (MCP) is a key part of the Amazon Q Developer CLI, allowing for extensibility through additional tools and servers. Q Developer has made several improvements to how MCP servers are loaded and managed:\nFirst, Q Developer has implemented background MCP server loading, which significantly improves startup time for q chat. Instead of waiting for all MCP servers to initialize before you can start interacting with Q Developer, the CLI now loads servers in the background while you begin your conversation. This means you can start working immediately, with tools becoming available as their servers finish loading.\nThe team has also added a new subcommand, q mcp, which provides a dedicated interface for updating and managing your MCP server configuration. This makes it easier to add, remove, or modify the MCP servers that extend your CLI\u0026rsquo;s capabilities.\nFor more granular control over which tools can be used, Q Developer has added the /tools command in q chat. This allows you to manage permissions for individual tools, giving you more control over what Q Developer can do in your environment. You can also reset permissions for a specific tool if you change your mind.\nImproved Context Control Context is crucial for getting the most out of Q Developer, and the team has made several improvements to how you can manage and view context:\nThe file selection in q chat\u0026rsquo;s fuzzy finder is now git-aware, making it easier to include relevant files from your repository. This is particularly useful when working with large codebases, as it helps you focus on the files that matter for your current task.\nQ Developer has added fuzzy search for slash commands with Ctrl + s, allowing you to quickly find and execute commands without remembering their exact syntax. This makes the CLI more accessible, especially for new users or those who don\u0026rsquo;t use certain commands frequently.\nThe /context show --expand command has been improved to provide more detailed information about the current context, helping you understand what Q Developer knows about your environment. The team has also enhanced the context file display in q chat to make it more informative and easier to read.\nOne of the most exciting additions is the new capability for dynamically adding context to messages with context hooks. This allows the CLI to automatically include relevant context based on your conversation, improving the quality of responses without requiring manual context management.\nContext Window Awareness and Optimization As conversations grow longer, managing the context window becomes increasingly important. Q Developer has added two new commands to help with this:\n/usage displays an estimate of the context window usage, helping you understand how much of the available context space you\u0026rsquo;re using /compact summarizes the conversation history, allowing you to reduce the size of the context while preserving the important information These tools help you make the most of the available context window, ensuring that Q Developer has access to the most relevant information without running into token limits.\nImage Support I\u0026rsquo;m particularly excited to announce that q chat now supports images! This opens up a whole new dimension of interaction, allowing you to share screenshots, diagrams, or other visual information with Q Developer. This can be incredibly useful for debugging UI issues, discussing design concepts, or explaining complex ideas that are difficult to convey through text alone.\nEditor for Long Prompts For complex queries or detailed instructions, you may want multiple paragraphs. Q Developer supports Ctrl + j, allowing you to add a newline character to the prompt. In addition, the team has added the /editor command, which opens your configured text editor for composing prompts. This makes it much easier to craft detailed, multi-paragraph prompts or to edit and refine your questions before sending them to Q Developer.\nExpanded Region Support I\u0026rsquo;m happy to announce that Q Developer has expanded its regional availability. Professional tier users can now access Q Developer in the Frankfurt region (eu-central-1). This expansion is part of Q Developer\u0026rsquo;s ongoing effort to provide lower latency and better service to customers across the globe. By adding support for the Frankfurt region, Amazon Q Developer is more accessible to European customers, allowing them to benefit from reduced latency and improved performance.\nAbility to Manage Issues in CLI Amazon Q Developer has made it easier to report issues directly from the CLI with two new features:\nThe /issue command in q chat allows you to create new GitHub issues The report_issue tool provides a programmatic way for Q Developer to help you create detailed issue reports These features streamline the feedback process, making it easier for you to report bugs or request features, and for the team to improve the CLI based on your input.\nKeeping Up with Future Changes To help you stay informed about new features and improvements, Q Developer has added a --changelog flag to the q version command. This displays the change log directly from the CLI, making it easy to see what\u0026rsquo;s new without having to visit the GitHub repository or read blog posts like this one.\nConclusion The Amazon Q Developer CLI continues to evolve rapidly, with new features and improvements that make it an even more powerful tool for developers. From conversation persistence to image support, these updates reflect Q Developer\u0026rsquo;s commitment to building a CLI that helps you be more productive and effective in your daily work. I encourage you to try out these new features by installing the Amazon Q Developer CLI. Thank you for your continued support and feedback, which helps make Amazon Q Developer better every day.\n"},{"uri":"https://thienluhoan.github.io/workshop-template/5-workshop/5.3-s3-vpc/5.3.1-create-gwe/","title":"Creating Gateway Endpoint for S3","tags":[],"description":"","content":" ‚ö†Ô∏è Note: The information below is for reference purposes only. Please do not copy verbatim for your report, including this warning.\nIn this section, you\u0026rsquo;ll configure a Gateway VPC Endpoint to enable private connectivity between your VPC and Amazon S3. This eliminates the need for internet gateway, NAT devices, or VPN connections, while keeping traffic within the AWS network.\nWhat is a Gateway Endpoint? Gateway endpoints are route-based endpoints that appear as targets in your VPC route tables. When you create a gateway endpoint for S3 or DynamoDB, AWS automatically updates the specified route tables with routes directing traffic to the endpoint. This approach:\nIncurs no additional data transfer charges Doesn\u0026rsquo;t require elastic network interfaces (ENIs) Scales automatically to handle your workload Benefits:\nCost Efficiency: No NAT Gateway charges for S3 access Security: Traffic never leaves the AWS network Performance: Lower latency compared to internet-based access Simplified Architecture: No need to manage NAT or internet gateway for S3 access Step 1: Navigate to VPC Endpoints Console Access the Amazon VPC Console in the us-east-1 region From the left navigation menu, locate and click on Endpoints Click the Create endpoint button in the upper right Expected Observation: You should see 6 pre-existing VPC endpoints in the list. These are Interface Endpoints for AWS Systems Manager (SSM) services that were automatically provisioned by the CloudFormation template during setup. These SSM endpoints enable secure, private connectivity to Systems Manager without requiring public internet access, allowing you to manage EC2 instances through Session Manager.\nStep 2: Configure Endpoint Basic Settings In the Create endpoint configuration page:\nName tag: Enter a descriptive name: s3-gateway-endpoint\nThis helps identify the endpoint\u0026rsquo;s purpose in a multi-endpoint environment Service category: Select AWS services\nThis option allows you to choose from AWS-managed service endpoints Step 3: Select the S3 Gateway Service In the Services section, use the search functionality:\nType s3 in the filter box The list will display S3-related endpoint options Important: Select the service entry where:\nService Name contains com.amazonaws.us-east-1.s3 Type column shows Gateway Do NOT select the Interface type endpoint for S3.\nUnderstanding the Difference:\nGateway endpoint (what we\u0026rsquo;re creating): Uses route table entries, free data transfer Interface endpoint: Uses ENIs, incurs PrivateLink charges Step 4: Associate VPC and Route Tables VPC Selection:\nFrom the VPC dropdown menu, select VPC Cloud This is the VPC where your cloud resources reside Route Tables Configuration:\nIn the Route tables section, you\u0026rsquo;ll see all route tables associated with the selected VPC Select the route table that shows association with 2 subnets Important: This is NOT the main route table of the VPC. It\u0026rsquo;s a custom route table created by CloudFormation specifically for the private subnets where your EC2 instances are running. The gateway endpoint will automatically add a route to this table directing S3 traffic (pl-63a5400a prefix list) to the endpoint.\nWhat Happens Behind the Scenes: When you select the route table, AWS will automatically add an entry like:\nDestination: pl-63a5400a (S3 prefix list) Target: vpce-xxxxx (your gateway endpoint ID) Step 5: Configure Endpoint Policy Policy Type: Leave the default Full access option selected\nThis grants unrestricted access to all S3 operations and buckets through this endpoint.\nNote: In a later section of this workshop, you will learn how to implement restrictive VPC endpoint policies to limit access to specific S3 buckets or operations. This demonstrates the principle of least privilege and shows how endpoint policies provide an additional security layer beyond IAM and bucket policies.\nPolicy Options Explained:\nFull access: Allows all S3 actions on all resources Custom: Lets you define specific allowed/denied actions and resources using JSON policy Step 6: Review and Create Endpoint Tags (Optional): Skip adding tags for this workshop\nIn production, consider adding tags like Environment: Workshop, Owner: YourName Click Create endpoint to provision the gateway endpoint\nConfirmation: You should see a success message indicating the endpoint was created\nClick the X or Close button to return to the endpoints list\nVerification:\nThe new endpoint appears in your endpoints list with status available Check the associated route table to see the new S3 prefix list route Understanding What Was Created Your gateway endpoint is now active and has automatically:\nUpdated the route table with an entry pointing S3 traffic to the endpoint Created a managed prefix list containing all S3 IP ranges for us-east-1 Enabled private S3 access for resources in the associated subnets Next Steps: In the following section, you\u0026rsquo;ll test this endpoint by accessing S3 from an EC2 instance in your VPC and verify that traffic flows through the private endpoint instead of the internet.\n"},{"uri":"https://thienluhoan.github.io/workshop-template/5-workshop/5.4-s3-onprem/5.4.1-prepare/","title":"Environment Preparation for Hybrid Connectivity","tags":[],"description":"","content":" ‚ö†Ô∏è Note: The information below is for reference purposes only. Please do not copy verbatim for your report, including this warning.\nIn this section, you\u0026rsquo;ll prepare the environment to simulate hybrid cloud connectivity between an on-premises datacenter and AWS. This setup demonstrates how organizations can securely access AWS services like S3 from their corporate networks through private connections.\nWhat You\u0026rsquo;ll Configure:\nDeploy additional DNS infrastructure using CloudFormation Configure VPN routing to enable on-premises to cloud connectivity Architecture Context: This configuration simulates a real-world hybrid architecture where:\nAn on-premises datacenter connects to AWS via Site-to-Site VPN DNS queries from on-premises are resolved through Route 53 S3 access from on-premises flows through Interface VPC Endpoints All traffic remains private without internet exposure Part 1: Deploy DNS Infrastructure with CloudFormation To enable proper DNS resolution in the hybrid environment, you\u0026rsquo;ll deploy a CloudFormation stack that creates Route 53 resolver endpoints and private hosted zones.\nWhat This Stack Creates:\nThe CloudFormation template provisions three critical DNS components:\nRoute 53 Private Hosted Zone\nHosts DNS records (Alias records) for the S3 Interface Endpoint Allows on-premises systems to resolve S3 endpoint names to private IP addresses Scoped to your VPC for private resolution Route 53 Inbound Resolver Endpoint\nDeployed in \u0026ldquo;VPC Cloud\u0026rdquo; Receives DNS queries from on-premises environments Forwards queries to the Private Hosted Zone for resolution Creates elastic network interfaces (ENIs) in your VPC subnets Route 53 Outbound Resolver Endpoint\nDeployed in \u0026ldquo;VPC On-prem\u0026rdquo; Forwards DNS queries for S3 domains to \u0026ldquo;VPC Cloud\u0026rdquo; Enables conditional DNS forwarding based on domain names Routes queries through the VPN tunnel to the Inbound Resolver DNS Flow Diagram:\nDeployment Steps:\nLaunch CloudFormation Stack:\nClick this link to open the CloudFormation console with pre-configured template: Deploy DNS Infrastructure Stack\nThe template URL is pre-populated and the stack name is set to PLOnpremSetup.\nReview and Accept Defaults:\nAll parameters are pre-configured with optimal values The template will automatically detect your VPC IDs and subnet configurations Scroll to the bottom of the page Acknowledge and Create:\nCheck the acknowledgment box: \u0026ldquo;I acknowledge that AWS CloudFormation might create IAM resources.\u0026rdquo; Click Create stack Monitor Deployment (Optional):\nStack creation typically takes 3-5 minutes You can monitor progress in the Events tab You don\u0026rsquo;t need to wait - proceed to the next section while this deploys Background Processing: The CloudFormation stack creates resolver endpoints which can take a few minutes. The next configuration step (routing table update) can be completed in parallel while CloudFormation runs.\nWhat Gets Created:\n2 Resolver Endpoint ENIs (Inbound in VPC Cloud) 2 Resolver Endpoint ENIs (Outbound in VPC On-prem) 1 Private Hosted Zone for S3 endpoint DNS resolution Resolver rules for conditional DNS forwarding Part 2: Configure VPN Routing for On-Premises Connectivity Now you\u0026rsquo;ll configure the routing table in \u0026ldquo;VPC On-prem\u0026rdquo; to direct traffic destined for the cloud environment through the VPN tunnel.\nUnderstanding the VPN Setup:\nThis workshop uses a software-based VPN solution running on an EC2 instance to simulate the connection between your simulated on-premises environment and AWS:\nVPN Software: strongSwan (open-source IPsec-based VPN) VPN Gateway Instance: An EC2 instance acting as the customer gateway Connection Type: Site-to-Site VPN tunnel Purpose: Simulates a real datacenter\u0026rsquo;s VPN appliance Production Note: In real-world deployments, you would use:\nPhysical or virtual VPN appliances in your datacenter AWS Site-to-Site VPN with redundant connections AWS Transit Gateway for centralized connectivity Multiple VPN tunnels for high availability Configuration Steps:\nStep 1: Identify the VPN Gateway Instance\nOpen the Amazon EC2 Console\nLocate and select the EC2 instance with the name infra-vpngw-test\nThis instance is running strongSwan VPN software It\u0026rsquo;s configured to terminate the VPN tunnel from \u0026ldquo;VPC Cloud\u0026rdquo; In the Details tab (bottom pane), locate the Instance ID\nFormat: i-0123456789abcdef0 Copy this Instance ID to your clipboard or text editor You\u0026rsquo;ll need this ID in the next steps Instance Role: This EC2 instance has IP forwarding enabled and is configured with IPsec tunnels. It acts as the gateway between the simulated on-premises network and AWS Transit Gateway.\nStep 2: Navigate to VPC Route Tables\nUse the search box at the top of the AWS Console Type VPC and press Enter From the VPC Dashboard left menu, click Route Tables Step 3: Update On-Premises Route Table\nLocate the correct route table:\nFilter or search for the route table named RT Private On-prem This is the route table associated with the private subnets in \u0026ldquo;VPC On-prem\u0026rdquo; Click on the route table to select it Access route editing:\nClick on the Routes tab in the bottom details pane Click Edit routes button Add new route entry:\nClick Add route button Configure the new route:\nDestination: Enter your Cloud VPC CIDR block\nFormat: 10.0.0.0/16 (or whatever CIDR your Cloud VPC uses) This tells the route table where cloud resources are located Target: Select Instance, then choose the infra-vpngw-test instance\nPaste the Instance ID you copied earlier Alternatively, start typing the instance name and select from the dropdown Save the configuration: Click Save changes The route table is immediately updated Understanding What You Configured:\nThis route entry tells instances in \u0026ldquo;VPC On-prem\u0026rdquo; that:\nTraffic destined for Cloud VPC CIDR ‚Üí Send to VPN Gateway Instance Traffic Flow:\nOn-Prem EC2 Instance ‚Üí Route Table Lookup ‚Üí VPN Gateway EC2 ‚Üí IPsec Tunnel ‚Üí Transit Gateway ‚Üí VPC Cloud Routing Configured! Now any traffic from \u0026ldquo;VPC On-prem\u0026rdquo; destined for resources in \u0026ldquo;VPC Cloud\u0026rdquo; will be routed through the VPN tunnel. This simulates how your corporate network would route traffic to AWS over a Site-to-Site VPN connection.\nVerification Steps Verify CloudFormation Stack:\nReturn to CloudFormation Console Check that PLOnpremSetup stack shows CREATE_COMPLETE status Click on the stack and view the Resources tab to see created components Verify Route Table:\nGo back to the RT Private On-prem route table Confirm the new route appears with: Destination: Cloud VPC CIDR Target: VPN Gateway instance ID Status: Active What\u0026rsquo;s Next: With DNS infrastructure and routing in place, you\u0026rsquo;re ready to:\nCreate an Interface VPC Endpoint for S3 Test S3 access from the simulated on-premises environment Verify DNS resolution through Route 53 Resolver Confirm all traffic flows through private connections Architecture Summary You\u0026rsquo;ve now established the foundational hybrid connectivity:\nDNS Resolution Path:\nOn-Prem Application ‚Üí Outbound Resolver Endpoint ‚Üí VPN Tunnel ‚Üí Inbound Resolver Endpoint ‚Üí Private Hosted Zone ‚Üí S3 Endpoint IP Data Path:\nOn-Prem Application ‚Üí VPN Gateway ‚Üí IPsec Tunnel ‚Üí Transit Gateway ‚Üí VPC Cloud ‚Üí Interface Endpoint ‚Üí S3 This architecture demonstrates AWS best practices for:\nPrivate connectivity to AWS services from on-premises DNS resolution in hybrid environments Secure data transfer without internet exposure Scalable VPN connectivity through Transit Gateway "},{"uri":"https://thienluhoan.github.io/workshop-template/4-eventparticipated/4.1-event1/","title":"Event 1","tags":[],"description":"","content":" ‚ö†Ô∏è Note: The information below is for reference purposes only. Please do not copy it verbatim into your report, including this warning.\nSummary Report: \u0026ldquo;AWS Community Day Vietnam 2025\u0026rdquo; Venue: Saigon Exhibition and Convention Center (SECC), Ho Chi Minh City\nTime: 09:00 ‚Äì 17:00, Saturday, September 20, 2025\nOrganizers: AWS User Group Vietnam, First Cloud Journey\nCoordinators: Phuong Nguyen, Minh Tran, Khanh Le\nEvent Objectives Connect AWS community members across Vietnam Share real-world experiences with AWS cloud services Introduce new AWS features and best practices Provide hands-on workshops for beginners and intermediate users Foster knowledge sharing among developers, architects, and cloud enthusiasts Speakers Nguyen Van Thanh ‚Äì Cloud Solutions Architect, AWS Vietnam Tran Thi Lan ‚Äì Senior DevOps Engineer, VNG Corporation Le Quoc Hung ‚Äì CTO, CloudTech Solutions Pham Minh Duc ‚Äì AWS Community Hero Key Highlights Identifying the drawbacks of legacy application architecture Long product release cycles ‚Üí Lost revenue/missed opportunities Inefficient operations ‚Üí Reduced productivity, higher costs Non-compliance with security regulations ‚Üí Security breaches, loss of reputation Transitioning to modern application architecture ‚Äì Microservices Migrating to a modular system ‚Äî each function is an independent service communicating via events, built on three core pillars:\nQueue Management: Handle asynchronous tasks Caching Strategy: Optimize performance Message Handling: Flexible inter-service communication Domain-Driven Design (DDD) Four-step method: Identify domain events ‚Üí arrange timeline ‚Üí identify actors ‚Üí define bounded contexts Bookstore case study: Demonstrates real-world DDD application Context mapping: 7 patterns for integrating bounded contexts Event-Driven Architecture 3 integration patterns: Publish/Subscribe, Point-to-point, Streaming Benefits: Loose coupling, scalability, resilience Sync vs async comparison: Understanding the trade-offs Compute Evolution Shared Responsibility Model: EC2 ‚Üí ECS ‚Üí Fargate ‚Üí Lambda Serverless benefits: No server management, auto-scaling, pay-for-value Functions vs Containers: Criteria for appropriate choice Amazon Q Developer SDLC automation: From planning to maintenance Code transformation: Java upgrade, .NET modernization AWS Transform agents: VMware, Mainframe, .NET migration Key Takeaways Design Mindset Business-first approach: Always start from the business domain, not the technology Ubiquitous language: Importance of a shared vocabulary between business and tech teams Bounded contexts: Identifying and managing complexity in large systems Technical Architecture Event storming technique: Practical method for modeling business processes Use event-driven communication instead of synchronous calls Integration patterns: When to use sync, async, pub/sub, streaming Compute spectrum: Criteria for choosing between VM, containers, and serverless Modernization Strategy Phased approach: No rushing ‚Äî follow a clear roadmap 7Rs framework: Multiple modernization paths depending on the application ROI measurement: Cost reduction + business agility Applying to Work Apply DDD to current projects: Event storming sessions with business teams Refactor microservices: Use bounded contexts to define service boundaries Implement event-driven patterns: Replace some sync calls with async messaging Adopt serverless: Pilot AWS Lambda for suitable use cases Try Amazon Q Developer: Integrate into the dev workflow to boost productivity Event Experience Attending the ‚ÄúGenAI-powered App-DB Modernization‚Äù workshop was extremely valuable, giving me a comprehensive view of modernizing applications and databases using advanced methods and tools. Key experiences included:\nLearning from Experienced Practitioners Speakers shared practical insights from real production environments, not just theoretical concepts Hearing about actual challenges and solutions from VNG Corporation and other companies was particularly valuable AWS Community Heroes provided mentorship and career guidance for aspiring cloud engineers Hands-on Workshop Experience Building a complete web application from scratch reinforced my understanding of AWS services Troubleshooting issues during the workshop helped me learn debugging techniques Seeing how different AWS services integrate (VPC, EC2, RDS, ALB) gave me a holistic view Workshop materials were well-organized and easy to follow Understanding Real-world Architecture The e-commerce demo with 10,000+ concurrent users showed scalability in action Learning about Auto Scaling policies helped me understand how applications handle traffic spikes Security session highlighted mistakes I should avoid in my own projects CI/CD demos illustrated modern software delivery practices Networking with Community Met other students and junior developers passionate about cloud computing Exchanged contacts with experienced engineers who offered to help with learning questions Discovered local AWS User Group meetups for continued learning Connected with FCJ members and discussed internship experiences Inspiration and Motivation Seeing Vietnamese companies successfully using AWS was inspiring Realized cloud engineering is a viable career path with growing opportunities Understood that continuous learning is essential in cloud technology Motivated to pursue AWS certifications after gaining more hands-on experience Practical Skills Gained Configured VPC networking including subnets, route tables, and internet gateway Launched EC2 instances with proper security group configurations Set up RDS database with appropriate backup and maintenance windows Deployed applications behind Application Load Balancer Implemented basic Auto Scaling policies Some event photos Event photos would be added here\nOverall, AWS Community Day Vietnam 2025 was more than just a learning event ‚Äì it was an opportunity to immerse myself in the AWS ecosystem, connect with passionate community members, and gain practical knowledge that directly applies to my internship and future career.\n"},{"uri":"https://thienluhoan.github.io/workshop-template/","title":"Internship Report","tags":[],"description":"","content":"Internship Report ‚ö†Ô∏è Note: The information below is for reference purposes only. Please do not copy verbatim for your report, including this warning.\nStudent Information: Full Name: Vu Chi Bao\nPhone Number: 0937839123\nEmail: Baovcse182733@fpt.edu.vn\nUniversity: FPT University\nMajor: Information Technology\nClass: AWS082025\nInternship Company: Amazon Web Services Vietnam Co., Ltd.\nInternship Position: FCJ Cloud Intern\nInternship Duration: From 08/09/2025 to 08/12/2025\nReport Content Worklog Proposal Translated Blogs Events Participated Workshop Self-evaluation Sharing and Feedback "},{"uri":"https://thienluhoan.github.io/workshop-template/1-worklog/1.1-week1/","title":"Week 1 Worklog","tags":[],"description":"","content":" ‚ö†Ô∏è Note: The following information is for reference purposes only. Please do not copy verbatim for your own report, including this warning.\nWeek 1 Objectives: Connect and get acquainted with members of First Cloud Journey. Understand basic AWS services, how to use the console \u0026amp; CLI. Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 2 - Get acquainted with FCJ members - Read and take note of internship unit rules and regulations - Set up development environment (Git, VS Code) 08/09/2025 08/09/2025 FCJ Internal Documentation 3 - Learn about AWS and its types of services + Compute (EC2, Lambda) + Storage (S3, EBS) + Networking (VPC) + Database (RDS, DynamoDB) 09/09/2025 09/09/2025 https://cloudjourney.awsstudygroup.com/ 4 - Create AWS Free Tier account - Learn about AWS Console \u0026amp; AWS CLI - Practice: + Create AWS account + Install \u0026amp; configure AWS CLI + Test basic CLI commands 10/09/2025 10/09/2025 https://cloudjourney.awsstudygroup.com/ 5 - Learn basic EC2: + Instance types + AMI + EBS + Security Groups - SSH connection methods to EC2 - Learn about Elastic IP 11/09/2025 11/09/2025 https://cloudjourney.awsstudygroup.com/ 6 - Practice: + Launch an EC2 instance + Connect via SSH + Attach an EBS volume + Test basic operations 12/09/2025 12/09/2025 https://cloudjourney.awsstudygroup.com/ Week 1 Achievements: Understood AWS Cloud Computing basics:\nLearned what AWS is and its role as a cloud service provider Explored core service categories: Compute (EC2, Lambda) Storage (S3, EBS) Networking (VPC) Database (RDS, DynamoDB) Security (IAM, Security Groups) Set up AWS account and environment:\nCreated AWS Free Tier account successfully Configured basic security settings Set up billing alerts to monitor costs Got familiar with AWS Management Console:\nLearned to navigate the AWS Console interface Found and accessed different AWS services Understood basic service dashboards Installed and configured AWS CLI:\nInstalled AWS CLI version 2 on local machine Created IAM user credentials Configured AWS CLI with: Access Key ID Secret Access Key Default Region (ap-southeast-1) Verified configuration with basic commands Practiced with AWS CLI commands:\naws sts get-caller-identity - Check account info aws ec2 describe-regions - List AWS regions aws ec2 describe-instances - View EC2 instances aws s3 ls - List S3 buckets aws ec2 describe-key-pairs - Manage SSH keys Learned EC2 basics:\nUnderstood different EC2 instance types (t2, t3, m5) Learned about AMI (Amazon Machine Images) Studied EBS volumes and their types Understood Security Groups for firewall rules Completed EC2 hands-on practice:\nLaunched EC2 instance (t2.micro) Configured Security Group for SSH access Connected to EC2 via SSH Created and attached EBS volume Tested basic Linux commands on EC2 Gained foundational AWS skills:\nCan use both Console and CLI to manage resources Understand basic AWS concepts and terminology Ready to learn more advanced topics in Week 2 "},{"uri":"https://thienluhoan.github.io/workshop-template/1-worklog/","title":"Worklog","tags":[],"description":"","content":" ‚ö†Ô∏è Note: The information below is for reference purposes only. Please do not copy verbatim for your report, including this warning.\nOn this page, you will need to introduce your worklog. How did you complete it? How many weeks did you take to complete the program? What did you do in those weeks?\nTypically, and as a standard, a worklog is carried out over about 3 months (throughout the internship period) with weekly contents as follows:\nWeek 1: Getting familiar with AWS and basic AWS services\nWeek 2: Doing task A\u0026hellip;\nWeek 3: Doing task B\u0026hellip;\nWeek 4: Doing task C\u0026hellip;\nWeek 5: Doing task D\u0026hellip;\nWeek 6: Doing task E\u0026hellip;\nWeek 7: Doing task G\u0026hellip;\nWeek 8: Doing task H\u0026hellip;\nWeek 9: Doing task I\u0026hellip;\nWeek 10: Doing task L\u0026hellip;\nWeek 11: Doing task M\u0026hellip;\nWeek 12: Doing task N\u0026hellip;\n"},{"uri":"https://thienluhoan.github.io/workshop-template/5-workshop/5.1-workshop-overview/","title":"Workshop Overview","tags":[],"description":"","content":" ‚ö†Ô∏è Note: The information below is for reference purposes only. Please do not copy verbatim for your report, including this warning.\nUnderstanding VPC Endpoints VPC Endpoints enable private connectivity between your Virtual Private Cloud (VPC) and AWS services without requiring traffic to traverse the public internet. These endpoints are:\nHighly Available: Built with redundancy across multiple Availability Zones Scalable: Automatically scale to handle your workload demands Secure: Keep traffic within the AWS network, reducing exposure to internet-based threats There are two primary types of VPC endpoints:\nGateway Endpoints: Used for connecting to Amazon S3 and DynamoDB. These are specified as route table targets for traffic destined to supported AWS services. Interface Endpoints (AWS PrivateLink): Enable private connectivity to services powered by AWS PrivateLink, including many AWS services, your own services, and SaaS applications. These create elastic network interfaces (ENIs) in your subnets. Lab Architecture Overview This hands-on workshop demonstrates secure access to Amazon S3 from multiple network environments using VPC endpoints. The lab architecture consists of two distinct Virtual Private Clouds:\nCloud VPC Environment:\nHosts cloud-native AWS resources including EC2 instances for testing Contains a Gateway VPC Endpoint providing direct, private access to Amazon S3 Represents your production AWS cloud infrastructure Demonstrates how cloud workloads can securely access S3 without internet exposure On-Premises Simulation VPC:\nEmulates a traditional on-premises data center or branch office environment Includes an EC2 instance configured with VPN software (OpenSwan/strongSwan) to establish secure connectivity Connected to AWS through a Site-to-Site VPN via AWS Transit Gateway Features an Interface VPC Endpoint enabling on-premises resources to access S3 privately over the VPN connection Simulates hybrid cloud scenarios where on-premises applications need secure S3 access Network Connectivity: The two VPCs are interconnected through AWS Transit Gateway, which acts as a cloud router to enable secure communication between your cloud and simulated on-premises environments. The Site-to-Site VPN connection ensures encrypted traffic flow between the environments.\nImportant Notes:\nThis lab uses a single VPN tunnel for cost efficiency and simplicity For production deployments, AWS strongly recommends implementing redundant VPN connections across multiple devices and Availability Zones for high availability The architecture follows AWS Well-Architected Framework principles for security and reliability This workshop will guide you through configuring both Gateway and Interface VPC Endpoints, testing connectivity from both environments, and understanding best practices for securing S3 access in hybrid cloud architectures.\n"},{"uri":"https://thienluhoan.github.io/workshop-template/2-proposal/","title":"Proposal","tags":[],"description":"","content":" Note: The information below is for reference purposes only, please do not copy verbatim for your report including this warning.\nDevteria Game Store Platform AWS Cloud E-commerce Solution for Digital Game Distribution Project Documentation üìÑ Devteria Shop Game Project With AWS (Word)\n1. Executive Summary Devteria Game Store is a scalable e-commerce platform for digital game licensing. Built on AWS, it delivers secure authentication, real-time inventory, automated order processing, and global content delivery. Supports thousands of concurrent users with 99.9% uptime and cost efficiency through serverless architecture.\n2. Problem Statement Current Challenges:\nTraditional stores struggle with traffic spikes Complex auth/payment reduces conversions Manual inventory causes overselling Lack of real-time analytics High infrastructure costs for peak capacity Solution: Devteria leverages AWS: CloudFront + S3 (fast delivery), Cognito (secure auth), API Gateway + Lambda (serverless backend), RDS + S3 (reliable storage), SQS + SNS (async processing), CodePipeline (CI/CD).\n3. Solution Architecture Core Components:\nFrontend: CloudFront CDN + S3 (React app, global cache, \u0026lt;2s load) Backend: API Gateway + Lambda (auto-scaling logic) + ALB + EC2 (microservices) Data: RDS PostgreSQL (users, catalog, orders) + S3 (game files, assets) + SQS/SNS (async processing) Security: Cognito (auth with MFA) + IAM (access control) + CloudWatch (monitoring) CI/CD: GitLab CodePipeline CodeBuild Deploy User Flow: Access site Login (Cognito) Browse games (API/Lambda/RDS) Add to cart Checkout License generation (SQS) Email (SNS) Secure download (S3)\n4. AWS Services Service Purpose Configuration CloudFront CDN 10M requests, 50GB transfer S3 Storage 100GB (frontend + assets) API Gateway API Management 1M requests/month Lambda Serverless Compute 5M invocations, 512MB EC2 Microservices 2x t3.medium RDS Database db.t3.small Multi-AZ ALB Load Balancer 1 ALB Cognito Authentication 10K users SQS + SNS Queue + Notifications 5M + 100K messages CloudWatch Monitoring Metrics + logs CodePipeline CI/CD 1 pipeline 5. Implementation Timeline (6 months) Month Milestones 1 Infrastructure: Setup AWS, VPC, RDS, S3, Cognito 2-3 Backend: Lambda APIs (auth, catalog, orders) + API Gateway 3 Frontend: React/Next.js app + Cognito integration 4 Advanced: Payment gateway + Admin dashboard + CI/CD 5 Testing: Load tests + Security audits + Performance tuning 6 Launch: Beta release Public launch 6. Budget Estimate Monthly Cost (10K users, 1K orders/month): ~$228\nService Cost CloudFront + S3 + API Gateway + Lambda $32 EC2 (2x t3.medium) + RDS (t3.small) $110 ALB + Cognito $50 SQS + SNS + CloudWatch + Other $36 Scaling: 50K users ($650/month), 100K users ($1,200/month)\nOne-time: Development ($5K-8K), Domain ($15/year), SSL (Free via ACM)\n7. Risk Assessment Risk Mitigation DDoS attacks AWS Shield, CloudFront, rate limiting Data breaches Encryption, IAM, regular audits Payment fraud 3D Secure, fraud detection Lambda cold starts Provisioned concurrency Cost overruns Budget alerts, auto-scaling limits Contingency: RDS automated backups, Multi-AZ deployment, CodePipeline rollback, static maintenance page\n8. Expected Outcomes Technical:\nPerformance: \u0026lt;2s page load globally Scalability: Handle 10x traffic spikes Reliability: 99.9% uptime Security: Zero breaches, PCI-ready Business:\n40% reduction in cart abandonment 60% less infrastructure management time 25-35% revenue increase from better UX Global market reach via CDN Long-term: Scale to 100K+ users, team gains AWS expertise, reusable microservices, rapid feature development\nNext Steps Proposal approval AWS account setup Team assembly Start Phase 1 Weekly progress reviews "},{"uri":"https://thienluhoan.github.io/workshop-template/3-blogstranslated/3.2-blog2/","title":"Blog 2","tags":[],"description":"","content":" ‚ö†Ô∏è Note: The information below is for reference purposes only. Please do not copy verbatim for your report, including this warning.\nOptimizing fleet operations using Amazon SageMaker AI and Amazon Bedrock by Manny Sidhu and Stephen Mistele on 28 MAY 2025\nin Amazon Bedrock, Amazon SageMaker AI, Architecture, AWS IoT Greengrass, Generative AI, Intermediate (200)\nEvery year in the United States, distracted driving claims thousands of lives and causes immense financial damage. More than 1.6 million accidents annually are caused by cell phone use while driving, and another 1.5 million result from drowsy drivers falling asleep at the wheel. These devastating‚Äîand preventable‚Äîaccidents have sparked a major push for enhanced driver safety.\nThis initiative is particularly crucial in the commercial fleet industry, as accidents involving a large truck are often more dangerous and can cost hundreds of thousands of dollars. This post explores an innovative solution that leverages Amazon SageMaker AI and Amazon Bedrock to revolutionize driver coaching and enhance fleet efficiency. By harnessing the power of machine learning and artificial intelligence, we demonstrate how fleet operators can transform raw dashcam footage into actionable insights, empowering real-time driver monitoring and proactive safety measures ‚Äì reducing costly accidents. Our approach combines AWS Artificial Intelligence (AI) and Internet of Things (IoT) services to create a comprehensive solution that not only detects distracted driving but also continuously improves its performance over time. Through this solution, we aim to show how fleet managers can significantly reduce distracted driving incidents, improve operational efficiency, and ultimately drive down costs in their commercial vehicle operations.\nThe Challenge: Effectively managing multiple dashcam feeds from commercial vehicle fleet Today\u0026rsquo;s commercial vehicles are equipped with multi-camera systems that provide comprehensive coverage: inward-facing cameras monitor driver behavior, outward-facing cameras track oncoming traffic, and side/rear cameras detect cross-traffic and potential rear-end collisions. The sheer volume of video data generated by thousands of vehicles daily creates significant management and analysis challenges. While fleet operators traditionally use this dashcam footage for reactive purposes ‚Äì such as law enforcement reporting, insurance claims, and driver exoneration ‚Äì many organizations are missing a significant opportunity to leverage this data. As commercial fleets accumulate more miles, they generate rich datasets that can be used to train AI models capable of facilitating proactive safety improvements.\nIn this post, we\u0026rsquo;ll explore how to maximize the value of dashcam footage through best practices for implementing and managing Computer Vision systems in commercial fleet operations. We\u0026rsquo;ll demonstrate how to build and deploy edge-based machine learning models that provide real-time alerts for distracted driving behaviors, while effectively collecting, processing, and analyzing footage to train these AI models. This approach transforms fleet operations from reactive incident management to proactive safety enhancement, helping organizations convert raw video data into actionable insights that reduce safety incidents and improve overall fleet operational efficiency and cost-effectiveness.\nSolution overview A Distracted Driving Incident can occur when drivers engage in unsafe behaviors such as speeding, rolling stops, harsh braking, and aggressive acceleration. Fleet managers need to understand not just what happened during these incidents, but also the driver\u0026rsquo;s state of attention ‚Äì whether they were focused on the road or distracted by activities like using a cellphone, eating, drinking, or experiencing fatigue common in long-haul driving.\nOur solution leverages AWS services to create an end-to-end workflow capable of detecting and mitigating distracted driving. The steps involved include:\nIncident capture, ingestion, and labeling Model training, optimization, and deployment Continuous testing and improvement Solution deep dive This solution relies on a mix of AWS IoT, AI and generative AI services to build a scalable and cost-effective solution. Let\u0026rsquo;s start by looking at high level solution architecture and build the solution step-by-step.\nIncident capture, ingestion, and labeling To start the process of ingesting videos from a driver\u0026rsquo;s dashboard camera into the cloud, we capture the dashcam\u0026rsquo;s feed using the IoT Greengrass Kinesis Video Streamer Component. The video is streamed into the AWS Cloud using Kinesis Video Streams and stored in Amazon S3 by leveraging Kinesis Firehose. The videos are then converted into individual frames, analyzed by the Amazon Bedrock Nova Pro model to determine driver distraction, and sorted by an AWS Lambda function into an S3 bucket based on the analysis results. The sorted frames will next be used to train an AI model for edge deployment to detect distracted driving.\nFrom a security perspective, it\u0026rsquo;s good practice to encrypt data in Amazon S3 buckets using AWS Key Management Service (KMS). You can enforce this by setting up SSE-KMS as the default encryption method to automatically encrypt uploaded objects. We also recommend implementing fine-grained AWS Identity \u0026amp; Access Management (IAM) roles to grant scoped access to images and videos. For data in transit between the edge and the cloud, you can use AWS IoT Greengrass certificates to encrypt your data and enforce identity verification. These measures can help protect against unauthorized access.\nEdge-to-cloud architecture for real-time driver monitoring using AWS IoT, Kinesis, and ML services\nWith this process in place, we are continually collecting data from our fleet of commercial vehicles (while keeping security in mind). This data is automatically categorized and labeled based on the analysis from our Nova Pro model, and conveniently stored in S3, enabling us to seamlessly train an AI model ‚Äì a process which we will describe next.\nModel training, optimization, and deployment The following diagram illustrates the process of training and deploying a distracted driver detection model. The process runs inside of an Amazon SageMaker Pipelines Workflow, which allows for seamless orchestration of other Amazon SageMaker AI services. This workflow begins with labeled driver images stored in Amazon S3, generated from the previously described workflow. This labeled dataset ‚Äì consisting of driver images labeled as \u0026ldquo;distracted\u0026rdquo; or \u0026ldquo;not distracted\u0026rsquo; ‚Äì is used to train a ResNet50 model using Amazon SageMaker Training Jobs running on a Trn1 instance for price performance. As we train, the model learns how to identify distracted drivers. Once complete, the trained model is then quantized to INT8 using SageMaker Processing Jobs, and optimized for our specific type of edge hardware using SageMaker Neo. The optimized model is then stored in the SageMaker Model Registry for version control and governance (this will be helpful later when we iterate on our model with new training data). Finally, the model is pushed to S3 where AWS IoT Greengrass can initiate a deployment to the fleet of edge devices.\nRunning on the edge, the model performs inference multiple times a second on frames from the inward facing dashcam. (Inference speed calculated assuming edge compute has specs comparable to a Raspberry-Pi class of device.) If the driver is found to be distracted, the system alerts the driver by means of a noise. (ex. driver was falling asleep, and alert awakens them).\nEnd-to-end AWS architecture for distracted driver detection: from model training to edge deployment\nWith this process in place, we have successfully leveraged the dataset we generated in the first diagram to train, optimize, and deploy our custom model to the \u0026rsquo;edge\u0026rsquo; ‚Äì in this case, to each vehicle in our fleet. Our model is now alerting drivers of dangerous behavior and helping to proactively prevent collisions. But our model likely isn\u0026rsquo;t perfect ‚Äì perhaps it misses a dangerous behavior that wasn\u0026rsquo;t in the training dataset, or alerts unnecessarily. To validate our model is working well and further improve it to reduce errors, we implement continuous testing and improvement procedures.\nContinuous testing and improvement We need to continue to ingest driver dashcam data and compare our edge model\u0026rsquo;s predictions with our original source of \u0026lsquo;ground truth\u0026rsquo; ‚Äì Nova Pro.\nThe system collects frames for model validation in two scenarios: when vehicle telemetry detects incidents (hard braking, crashes) or when the edge model identifies distracted driving. These frames are sent to Amazon Bedrock for a \u0026lsquo;fact check\u0026rsquo; to see if the edge model performed optimally. The comparative results between Amazon Bedrock and the edge model are stored in a dedicated S3 bucket for model evaluation. When sufficient new validated data is collected, or when the model\u0026rsquo;s agreement with Amazon Bedrock falls below a threshold, Amazon EventBridge triggers the previously described SageMaker Pipelines Workflow to fine tune, optimize, and re-deploy the improved model to the edge, now powered by our newly collected \u0026lsquo;disagreement data\u0026rsquo;.\nEdge-to-cloud feedback loop for ML model validation using AWS IoT, Bedrock, and SageMaker services\nWe should also perform comparative analysis of our new model against our historical models stored in the Amazon SageMaker Model Registry to validate that our latest model actually performs better than historical models, verifying we don\u0026rsquo;t see a regression. If our latest model doesn\u0026rsquo;t outperform historical models, we should not deploy it, and instead investigate if we are suffering from overfitting or bad training data. In summary, we now have a model running inside fleet vehicles capable of alerting drivers to unsafe behavior. This could effectively reduce drowsy driving accidents by keeping drivers awake and alert, while also warning drivers about unsafe decisions like eating or using a cell phone while driving. This system is also self-training and self-improving, so it will continue to get better over time. Additionally, fleet management companies could aggregate safety data and reward top drivers to further incentivize safe driving habits.\nConclusion In this post, we\u0026rsquo;ve explored an innovative solution that leverages AWS services to revolutionize driver coaching and fleet operations. By combining the power of Amazon SageMaker and Amazon Bedrock with AWS IoT and edge computing capabilities, we\u0026rsquo;ve demonstrated how to create a comprehensive, scalable solution for monitoring and improving driver behavior in real-time. This solution addresses the challenges of managing vast amounts of dashcam footage from commercial vehicle fleets, transforming raw video data into actionable insights. By implementing an end-to-end workflow that includes incident capture, categorization, model training, deployment, and continuous improvement, fleet operators can shift from reactive incident management to proactive safety enhancement. The benefits of this approach include:\nEnhanced safety: Real-time detection of distracted driving behaviors allows for immediate intervention and coaching. Improved efficiency: Automated analysis of dashcam footage reduces manual review time and costs. Scalability: The solution can handle large fleets and growing datasets with ease. Continuous improvement: The system learns and adapts over time, becoming more accurate and effective. Cost-effectiveness: By leveraging edge computing and optimized models, the solution minimizes compute costs. As the transportation industry continues to evolve, solutions like this will play a crucial role in improving road safety, reducing operational costs, and enhancing overall fleet performance. By harnessing the power of AI and cloud computing, fleet operators can create safer, more efficient driving environments that benefit not only their businesses but also society as a whole. The future of fleet operations is here, and it\u0026rsquo;s driven by intelligent, data-driven systems that turn every mile driven into an opportunity for improvement and innovation.\nLearn more by exploring AWS code samples to build hands-on SageMaker expertise. See the service in action through practical examples that demonstrate how to optimize model training and deployment across various use cases. Understand the financial advantages by conducting a cloud economics TCO analysis comparing traditional infrastructure against SageMaker\u0026rsquo;s managed services. This exercise reveals how SageMaker alleviates hidden costs while accelerating your ML development cycle.\nReady to take the next step? Connect with your AWS Solutions Architect to arrange a SageMaker AI Immersion Day tailored to your team\u0026rsquo;s specific challenges. These expert-led sessions provide personalized guidance that will help you implement SageMaker effectively within your organization\u0026rsquo;s unique context. For deeper dive into other relevant services Amazon Kinesis Video Streams, AWS IoT Greengrass, Amazon Bedrock.\nAbout the authors Manny Sidhu\nManny Sidhu is an Enterprise Solutions Architect at Amazon Web Services in San Francisco Bay Area, California. Manny specializes in IoT, Generative/AI \u0026amp; Supply Chain. He enjoys geeking out, outdoor activities and traveling with his family.\nStephen Mistele\nStephen Mistele is an AI Infrastructure Engineer at Amazon Web Services in Seattle. As a member of the SageMaker AI Training team, he specializes in building infrastructure to support large-scale ML workloads. When he\u0026rsquo;s not working, you can catch him on the ski slopes.\nTAGS: Amazon Bedrock, Amazon SageMaker AI, AWS IoT Greengrass, Machine Learning, Computer Vision\n"},{"uri":"https://thienluhoan.github.io/workshop-template/5-workshop/5.4-s3-onprem/5.4.2-create-interface-enpoint/","title":"Creating Interface VPC Endpoint for S3 Access","tags":[],"description":"","content":" ‚ö†Ô∏è Important: The information provided here is for reference purposes only. Please do not copy verbatim for your report including this warning message.\nIn this section, you will deploy an S3 Interface VPC Endpoint within the cloud VPC. This endpoint enables private connectivity from your simulated on-premises network to Amazon S3, eliminating the need for internet gateways or NAT devices.\nWhy Interface Endpoints for Hybrid Access?\nInterface VPC Endpoints create elastic network interfaces (ENIs) with private IP addresses in your VPC. This architecture provides several advantages for hybrid cloud scenarios:\nPrivate IP Addressing: On-premises systems can resolve S3 service names to private IPs within your VPC CIDR range DNS Integration: Works seamlessly with Route 53 Resolver for hybrid DNS resolution Security: Traffic flows through private connections (VPN/Direct Connect) without traversing the public internet PrivateLink Technology: Leverages AWS PrivateLink for secure, scalable service access Fine-Grained Access Control: Apply security groups and endpoint policies for granular access management Interface vs Gateway Endpoints: While you previously used a Gateway Endpoint (which modifies route tables), Interface Endpoints use ENIs with private IPs. This makes them suitable for on-premises access through VPN/Direct Connect, as they can be reached via standard IP routing.\nStep-by-Step Interface Endpoint Creation Step 1: Navigate to VPC Endpoints Console\nOpen the Amazon VPC Console In the left navigation menu, click Endpoints Click the Create endpoint button in the top-right corner Step 2: Configure Endpoint Basic Settings\nIn the Create Endpoint configuration page:\nName your endpoint (optional but recommended):\nEnter a descriptive name: S3-Interface-Endpoint-HybridAccess This helps identify the endpoint\u0026rsquo;s purpose in production environments Select Service Category:\nChoose AWS services This filters the service list to AWS-managed endpoints Naming Convention Best Practice: Use descriptive names that indicate the service, endpoint type, and purpose, e.g., S3-Interface-CloudVPC or S3-InterfaceEP-Production.\nStep 3: Locate and Select the S3 Interface Service\nSearch for S3 service:\nIn the Services search box, type: S3 Press Enter or click the search icon Identify the correct service:\nLook for the service named: com.amazonaws.us-east-1.s3 Verify the Type column shows \u0026ldquo;Interface\u0026rdquo; (not Gateway) Click the radio button to select this service Critical Selection: AWS offers both Gateway and Interface endpoint types for S3. Ensure you select the Interface type. Gateway endpoints won\u0026rsquo;t work for on-premises access scenarios because they only modify VPC route tables.\nStep 4: Configure VPC and DNS Settings\nSelect Target VPC: From the VPC dropdown, select VPC Cloud Do NOT select \u0026ldquo;VPC On-prem\u0026rdquo; - the endpoint must be in the cloud VPC VPC Selection is Critical: The Interface Endpoint must be created in VPC Cloud, not VPC On-prem. On-premises systems will access this endpoint through the VPN tunnel you configured earlier.\nConfigure DNS Settings: Expand the Additional settings section Uncheck \u0026ldquo;Enable DNS name\u0026rdquo; (disable it) Why? You\u0026rsquo;ll manually configure DNS using Route 53 Private Hosted Zones for more control DNS Configuration Choice: In this workshop, you\u0026rsquo;re using Route 53 Resolver with Private Hosted Zones for DNS resolution (configured in section 5.4.1). This provides greater flexibility and mirrors enterprise hybrid DNS architectures. Alternatively, enabling \u0026ldquo;Enable DNS name\u0026rdquo; would automatically create Route 53 private DNS records, but we want explicit control for learning purposes.\nStep 5: Select Availability Zones and Subnets\nConfigure High Availability:\nSelect 2 subnets in different Availability Zones: Availability Zone: us-east-1a Choose the corresponding private subnet in VPC Cloud Availability Zone: us-east-1b Choose the corresponding private subnet in VPC Cloud Why Multiple AZs?\nDeploying Interface Endpoint ENIs in multiple Availability Zones provides:\nHigh Availability: If one AZ experiences issues, traffic can route through the other Fault Tolerance: Automatic failover between ENIs in different AZs Geographic Redundancy: Reduces latency by having endpoints in multiple locations Production Best Practice: AWS recommends multi-AZ deployment for critical services Best Practice: Always deploy Interface Endpoints across at least 2 Availability Zones for production workloads. This ensures your on-premises applications maintain S3 connectivity even during AZ failures.\nStep 6: Apply Security Group\nSelect Security Group: In the Security groups dropdown, choose SGforS3Endpoint This security group was created by the CloudFormation template It contains inbound rules allowing HTTPS (port 443) traffic from on-premises CIDR Security Group Purpose:\nThe SGforS3Endpoint security group controls which sources can communicate with the Interface Endpoint ENIs:\nInbound Rules: - Protocol: TCP - Port: 443 (HTTPS) - Source: VPC On-prem CIDR (e.g., 10.1.0.0/16) - Purpose: Allow S3 API calls from on-premises systems Security Layer: Security groups act as virtual firewalls for your Interface Endpoint. Even though traffic comes through a VPN, the security group provides an additional layer of control, following AWS defense-in-depth principles.\nStep 7: Configure Endpoint Policy\nPolicy Configuration:\nLeave the Policy set to Full access (default) This allows all S3 actions through the endpoint Understanding Endpoint Policies:\nEndpoint policies control what AWS API actions can be performed through the endpoint They work in conjunction with IAM policies (both must allow an action) For this workshop, full access simplifies testing Click Create endpoint\nProduction Consideration: In production environments, use restrictive endpoint policies following the principle of least privilege. For example, limit to specific S3 buckets or actions like s3:GetObject and s3:PutObject only.\nExample Restrictive Policy:\n{ \u0026#34;Statement\u0026#34;: [ { \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Principal\u0026#34;: \u0026#34;*\u0026#34;, \u0026#34;Action\u0026#34;: [ \u0026#34;s3:GetObject\u0026#34;, \u0026#34;s3:PutObject\u0026#34;, \u0026#34;s3:ListBucket\u0026#34; ], \u0026#34;Resource\u0026#34;: [ \u0026#34;arn:aws:s3:::my-company-data/*\u0026#34;, \u0026#34;arn:aws:s3:::my-company-data\u0026#34; ] } ] } Verify Endpoint Creation Check Endpoint Status:\nThe endpoint creation typically takes 2-3 minutes Refresh the Endpoints page periodically Wait until the Status changes from Pending to Available Once available, note the Endpoint ID (format: vpce-xxxxxxxxxxxxxxxxx) Inspect Endpoint Details:\nClick on the newly created endpoint to view:\nDNS names: Private DNS names assigned to the endpoint Subnets: Verify both AZs are listed with their ENI IDs Network interfaces: Each subnet has a dedicated ENI with a private IP Security groups: Confirm SGforS3Endpoint is attached Record Private IP Addresses:\nIn the Subnets section, note the private IP addresses assigned to each ENI. You\u0026rsquo;ll use these in DNS configuration (next step).\nUnderstanding What You Created Architecture Overview:\nYou\u0026rsquo;ve deployed an Interface VPC Endpoint with the following components:\nTwo Elastic Network Interfaces (ENIs):\nOne ENI in us-east-1a subnet with a private IP (e.g., 10.0.1.100) One ENI in us-east-1b subnet with a private IP (e.g., 10.0.2.100) Security Group Protection:\nENIs protected by SGforS3Endpoint security group Only allows HTTPS traffic from on-premises CIDR Private Connectivity Path:\nOn-Premises Systems ‚Üí VPN Tunnel ‚Üí VPC Cloud ‚Üí Interface Endpoint ENI ‚Üí AWS PrivateLink ‚Üí S3 Service How On-Premises Access Works:\nWhen an on-premises application makes an S3 API call:\nDNS query for s3.amazonaws.com is sent to Outbound Resolver Outbound Resolver forwards through VPN to Inbound Resolver Inbound Resolver queries Private Hosted Zone (to be configured next) Private Hosted Zone returns Interface Endpoint private IP Application sends HTTPS request to private IP through VPN Request reaches Interface Endpoint ENI PrivateLink routes request to S3 service Response follows reverse path back to on-premises Congratulations! You\u0026rsquo;ve successfully created an S3 Interface VPC Endpoint for hybrid cloud access. This endpoint provides a secure, private connection from your on-premises environment to Amazon S3, eliminating internet exposure and enabling enterprise-grade hybrid architectures.\nNext Steps With the Interface Endpoint created, you\u0026rsquo;ll proceed to:\nConfigure DNS records in Route 53 Private Hosted Zone to resolve S3 DNS names to endpoint IPs Test connectivity from the on-premises EC2 instance to S3 through the private endpoint Verify traffic flow using VPC Flow Logs and endpoint metrics Validate security by confirming all traffic stays within AWS private networks The Interface Endpoint is now ready to serve as your private gateway to S3 for hybrid workloads!\n"},{"uri":"https://thienluhoan.github.io/workshop-template/4-eventparticipated/4.2-event2/","title":"Event 2","tags":[],"description":"","content":" ‚ö†Ô∏è Note: The information below is for reference purposes only. Please do not copy it verbatim into your report, including this warning.\nSummary Report: \u0026ldquo;AWS AI/ML Workshop - Machine Learning and Generative AI\u0026rdquo; Date \u0026amp; Time: Saturday, November 15, 2025, 8:30 AM ‚Äì 12:00 PM\nLocation: FCJ Learning Center, Ho Chi Minh City\nRole: Workshop Participant\nOrganizer: First Cloud Journey, AWS User Group Vietnam\nEvent Purpose The workshop provided hands-on exposure to AWS AI/ML services, with emphasis on Amazon SageMaker for traditional machine learning and Amazon Bedrock for generative AI applications. The event helped participants understand practical implementation of AI/ML solutions on AWS and explore recent developments in generative AI technology.\nAgenda Overview 8:30 ‚Äì 9:00 AM | Welcome \u0026amp; Introduction\nParticipant registration and networking Workshop overview and learning objectives Introduction to AI/ML landscape in Vietnam Discussion of real-world AI/ML use cases 9:00 ‚Äì 10:30 AM | AWS AI/ML Services Overview\nAmazon SageMaker ‚Äì End-to-End ML Platform\nData preparation and labeling capabilities Model training, tuning, and deployment options Real-time and batch inference MLOps features: model versioning, monitoring, automated retraining Live Demo: SageMaker Studio Walkthrough\nJupyter notebook integration Experiment tracking and model registry Visual workflow builder Integration with AWS data services 10:30 ‚Äì 10:45 AM | Coffee Break\nNetworking and informal AI/ML discussions 10:45 AM ‚Äì 12:00 PM | Generative AI with Amazon Bedrock\nFoundation Models: Claude, Llama, Titan\nComparison of available foundation models Model capabilities and use cases Selection criteria for business needs Cost considerations Prompt Engineering Techniques\nCrafting effective prompts Chain-of-thought reasoning Few-shot learning examples Retrieval-Augmented Generation (RAG)\nRAG architecture overview Knowledge base integration (OpenSearch, Kendra) Building context-aware applications Bedrock Agents\nMulti-step workflow orchestration External API and tool integration Complex request handling Guardrails \u0026amp; Safety\nContent filtering and safety measures Custom policies and compliance Ethical AI practices Live Demo: Building a Chatbot with Bedrock\nSetting up foundation model Implementing RAG with knowledge base Configuring agents and guardrails Deployment process Key Highlights Comprehensive ML Platform: SageMaker provides end-to-end solution from data preparation to deployment Generative AI Access: Bedrock offers multiple foundation models for experimentation RAG Architecture: Enables AI applications to access specific knowledge bases Production MLOps: Integrated capabilities for deploying and maintaining models Safety First: Guardrails ensure AI applications are compliant and safe Key Learnings SageMaker Studio provides unified interface for entire ML lifecycle Foundation model selection depends on use case, performance, and cost Prompt engineering significantly improves model outputs RAG is essential for applications needing specific, current information Bedrock Agents enable sophisticated multi-step workflows Content safety must be built-in from the start Application to Internship Immediate Applications:\nExplore SageMaker Studio for ML model development Experiment with Bedrock foundation models Practice prompt engineering techniques Learn MLOps best practices Skills to Develop:\nUnderstanding ML model lifecycle Prompt engineering proficiency RAG architecture implementation AI safety and compliance awareness Cost Optimization:\nUse SageMaker notebook instances efficiently Choose appropriate foundation models Implement proper model versioning Monitor usage and costs Personal Experience Attending the AWS AI/ML Workshop provided valuable introduction to AWS AI/ML services.\nHands-on Learning:\nSageMaker Studio demo showed how unified platform streamlines ML workflow Practical exercises helped understand model training and deployment Workshop materials were well-structured with clear objectives Understanding AI/ML on AWS:\nLearned difference between SageMaker for traditional ML and Bedrock for GenAI RAG architecture demonstration was particularly insightful Understanding of how to build AI applications with specific knowledge Bedrock and GenAI:\nLearned about different foundation models and their use cases Prompt engineering techniques provide immediately applicable skills Guardrails importance for safe AI applications became clear MLOps Concepts:\nUnderstood model versioning and monitoring Learned about automated retraining pipelines Appreciated importance of proper MLOps practices Practical Skills Gained:\nBasic understanding of SageMaker workflow Prompt engineering fundamentals RAG architecture concepts Awareness of AI safety considerations Challenges:\nAI/ML concepts require significant learning curve Understanding which service to use for which scenario Cost optimization for AI/ML workloads Balancing model performance with practical constraints Event Photos Workshop photos will be added here\nOverall, the workshop provided practical introduction to AWS AI/ML services and helped me understand how to build AI applications on AWS. The combination of theory, demos, and hands-on exercises made complex concepts more accessible for learning and future application in projects.\n"},{"uri":"https://thienluhoan.github.io/workshop-template/5-workshop/5.2-prerequiste/","title":"Prerequisites","tags":[],"description":"","content":" ‚ö†Ô∏è Note: The information below is for reference purposes only. Please do not copy verbatim for your report, including this warning.\nRequired IAM Permissions Before starting this workshop, you must ensure your AWS IAM user or role has sufficient permissions to create and manage the necessary resources. Attach the following custom IAM policy to your user account or assume a role with these permissions.\nImportant Security Notes:\nReview all permissions before applying to understand what resources will be created These permissions are required for workshop deployment and cleanup operations After completing the workshop, consider removing these permissions if they\u0026rsquo;re no longer needed For production environments, always follow the principle of least privilege { \u0026#34;Version\u0026#34;: \u0026#34;2012-10-17\u0026#34;, \u0026#34;Statement\u0026#34;: [ { \u0026#34;Sid\u0026#34;: \u0026#34;VisualEditor0\u0026#34;, \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: [ \u0026#34;cloudformation:*\u0026#34;, \u0026#34;cloudwatch:*\u0026#34;, \u0026#34;ec2:AcceptTransitGatewayPeeringAttachment\u0026#34;, \u0026#34;ec2:AcceptTransitGatewayVpcAttachment\u0026#34;, \u0026#34;ec2:AllocateAddress\u0026#34;, \u0026#34;ec2:AssociateAddress\u0026#34;, \u0026#34;ec2:AssociateIamInstanceProfile\u0026#34;, \u0026#34;ec2:AssociateRouteTable\u0026#34;, \u0026#34;ec2:AssociateSubnetCidrBlock\u0026#34;, \u0026#34;ec2:AssociateTransitGatewayRouteTable\u0026#34;, \u0026#34;ec2:AssociateVpcCidrBlock\u0026#34;, \u0026#34;ec2:AttachInternetGateway\u0026#34;, \u0026#34;ec2:AttachNetworkInterface\u0026#34;, \u0026#34;ec2:AttachVolume\u0026#34;, \u0026#34;ec2:AttachVpnGateway\u0026#34;, \u0026#34;ec2:AuthorizeSecurityGroupEgress\u0026#34;, \u0026#34;ec2:AuthorizeSecurityGroupIngress\u0026#34;, \u0026#34;ec2:CreateClientVpnEndpoint\u0026#34;, \u0026#34;ec2:CreateClientVpnRoute\u0026#34;, \u0026#34;ec2:CreateCustomerGateway\u0026#34;, \u0026#34;ec2:CreateDhcpOptions\u0026#34;, \u0026#34;ec2:CreateFlowLogs\u0026#34;, \u0026#34;ec2:CreateInternetGateway\u0026#34;, \u0026#34;ec2:CreateLaunchTemplate\u0026#34;, \u0026#34;ec2:CreateNetworkAcl\u0026#34;, \u0026#34;ec2:CreateNetworkInterface\u0026#34;, \u0026#34;ec2:CreateNetworkInterfacePermission\u0026#34;, \u0026#34;ec2:CreateRoute\u0026#34;, \u0026#34;ec2:CreateRouteTable\u0026#34;, \u0026#34;ec2:CreateSecurityGroup\u0026#34;, \u0026#34;ec2:CreateSubnet\u0026#34;, \u0026#34;ec2:CreateSubnetCidrReservation\u0026#34;, \u0026#34;ec2:CreateTags\u0026#34;, \u0026#34;ec2:CreateTransitGateway\u0026#34;, \u0026#34;ec2:CreateTransitGatewayPeeringAttachment\u0026#34;, \u0026#34;ec2:CreateTransitGatewayPrefixListReference\u0026#34;, \u0026#34;ec2:CreateTransitGatewayRoute\u0026#34;, \u0026#34;ec2:CreateTransitGatewayRouteTable\u0026#34;, \u0026#34;ec2:CreateTransitGatewayVpcAttachment\u0026#34;, \u0026#34;ec2:CreateVpc\u0026#34;, \u0026#34;ec2:CreateVpcEndpoint\u0026#34;, \u0026#34;ec2:CreateVpcEndpointConnectionNotification\u0026#34;, \u0026#34;ec2:CreateVpcEndpointServiceConfiguration\u0026#34;, \u0026#34;ec2:CreateVpnConnection\u0026#34;, \u0026#34;ec2:CreateVpnConnectionRoute\u0026#34;, \u0026#34;ec2:CreateVpnGateway\u0026#34;, \u0026#34;ec2:DeleteCustomerGateway\u0026#34;, \u0026#34;ec2:DeleteFlowLogs\u0026#34;, \u0026#34;ec2:DeleteInternetGateway\u0026#34;, \u0026#34;ec2:DeleteNetworkInterface\u0026#34;, \u0026#34;ec2:DeleteNetworkInterfacePermission\u0026#34;, \u0026#34;ec2:DeleteRoute\u0026#34;, \u0026#34;ec2:DeleteRouteTable\u0026#34;, \u0026#34;ec2:DeleteSecurityGroup\u0026#34;, \u0026#34;ec2:DeleteSubnet\u0026#34;, \u0026#34;ec2:DeleteSubnetCidrReservation\u0026#34;, \u0026#34;ec2:DeleteTags\u0026#34;, \u0026#34;ec2:DeleteTransitGateway\u0026#34;, \u0026#34;ec2:DeleteTransitGatewayPeeringAttachment\u0026#34;, \u0026#34;ec2:DeleteTransitGatewayPrefixListReference\u0026#34;, \u0026#34;ec2:DeleteTransitGatewayRoute\u0026#34;, \u0026#34;ec2:DeleteTransitGatewayRouteTable\u0026#34;, \u0026#34;ec2:DeleteTransitGatewayVpcAttachment\u0026#34;, \u0026#34;ec2:DeleteVpc\u0026#34;, \u0026#34;ec2:DeleteVpcEndpoints\u0026#34;, \u0026#34;ec2:DeleteVpcEndpointServiceConfigurations\u0026#34;, \u0026#34;ec2:DeleteVpnConnection\u0026#34;, \u0026#34;ec2:DeleteVpnConnectionRoute\u0026#34;, \u0026#34;ec2:Describe*\u0026#34;, \u0026#34;ec2:DetachInternetGateway\u0026#34;, \u0026#34;ec2:DisassociateAddress\u0026#34;, \u0026#34;ec2:DisassociateRouteTable\u0026#34;, \u0026#34;ec2:GetLaunchTemplateData\u0026#34;, \u0026#34;ec2:GetTransitGatewayAttachmentPropagations\u0026#34;, \u0026#34;ec2:ModifyInstanceAttribute\u0026#34;, \u0026#34;ec2:ModifySecurityGroupRules\u0026#34;, \u0026#34;ec2:ModifyTransitGatewayVpcAttachment\u0026#34;, \u0026#34;ec2:ModifyVpcAttribute\u0026#34;, \u0026#34;ec2:ModifyVpcEndpoint\u0026#34;, \u0026#34;ec2:ReleaseAddress\u0026#34;, \u0026#34;ec2:ReplaceRoute\u0026#34;, \u0026#34;ec2:RevokeSecurityGroupEgress\u0026#34;, \u0026#34;ec2:RevokeSecurityGroupIngress\u0026#34;, \u0026#34;ec2:RunInstances\u0026#34;, \u0026#34;ec2:StartInstances\u0026#34;, \u0026#34;ec2:StopInstances\u0026#34;, \u0026#34;ec2:UpdateSecurityGroupRuleDescriptionsEgress\u0026#34;, \u0026#34;ec2:UpdateSecurityGroupRuleDescriptionsIngress\u0026#34;, \u0026#34;iam:AddRoleToInstanceProfile\u0026#34;, \u0026#34;iam:AttachRolePolicy\u0026#34;, \u0026#34;iam:CreateInstanceProfile\u0026#34;, \u0026#34;iam:CreatePolicy\u0026#34;, \u0026#34;iam:CreateRole\u0026#34;, \u0026#34;iam:DeleteInstanceProfile\u0026#34;, \u0026#34;iam:DeletePolicy\u0026#34;, \u0026#34;iam:DeleteRole\u0026#34;, \u0026#34;iam:DeleteRolePolicy\u0026#34;, \u0026#34;iam:DetachRolePolicy\u0026#34;, \u0026#34;iam:GetInstanceProfile\u0026#34;, \u0026#34;iam:GetPolicy\u0026#34;, \u0026#34;iam:GetRole\u0026#34;, \u0026#34;iam:GetRolePolicy\u0026#34;, \u0026#34;iam:ListPolicyVersions\u0026#34;, \u0026#34;iam:ListRoles\u0026#34;, \u0026#34;iam:PassRole\u0026#34;, \u0026#34;iam:PutRolePolicy\u0026#34;, \u0026#34;iam:RemoveRoleFromInstanceProfile\u0026#34;, \u0026#34;lambda:CreateFunction\u0026#34;, \u0026#34;lambda:DeleteFunction\u0026#34;, \u0026#34;lambda:DeleteLayerVersion\u0026#34;, \u0026#34;lambda:GetFunction\u0026#34;, \u0026#34;lambda:GetLayerVersion\u0026#34;, \u0026#34;lambda:InvokeFunction\u0026#34;, \u0026#34;lambda:PublishLayerVersion\u0026#34;, \u0026#34;logs:CreateLogGroup\u0026#34;, \u0026#34;logs:DeleteLogGroup\u0026#34;, \u0026#34;logs:DescribeLogGroups\u0026#34;, \u0026#34;logs:PutRetentionPolicy\u0026#34;, \u0026#34;route53:ChangeTagsForResource\u0026#34;, \u0026#34;route53:CreateHealthCheck\u0026#34;, \u0026#34;route53:CreateHostedZone\u0026#34;, \u0026#34;route53:CreateTrafficPolicy\u0026#34;, \u0026#34;route53:DeleteHostedZone\u0026#34;, \u0026#34;route53:DisassociateVPCFromHostedZone\u0026#34;, \u0026#34;route53:GetHostedZone\u0026#34;, \u0026#34;route53:ListHostedZones\u0026#34;, \u0026#34;route53domains:ListDomains\u0026#34;, \u0026#34;route53domains:ListOperations\u0026#34;, \u0026#34;route53domains:ListTagsForDomain\u0026#34;, \u0026#34;route53resolver:AssociateResolverEndpointIpAddress\u0026#34;, \u0026#34;route53resolver:AssociateResolverRule\u0026#34;, \u0026#34;route53resolver:CreateResolverEndpoint\u0026#34;, \u0026#34;route53resolver:CreateResolverRule\u0026#34;, \u0026#34;route53resolver:DeleteResolverEndpoint\u0026#34;, \u0026#34;route53resolver:DeleteResolverRule\u0026#34;, \u0026#34;route53resolver:DisassociateResolverEndpointIpAddress\u0026#34;, \u0026#34;route53resolver:DisassociateResolverRule\u0026#34;, \u0026#34;route53resolver:GetResolverEndpoint\u0026#34;, \u0026#34;route53resolver:GetResolverRule\u0026#34;, \u0026#34;route53resolver:ListResolverEndpointIpAddresses\u0026#34;, \u0026#34;route53resolver:ListResolverEndpoints\u0026#34;, \u0026#34;route53resolver:ListResolverRuleAssociations\u0026#34;, \u0026#34;route53resolver:ListResolverRules\u0026#34;, \u0026#34;route53resolver:ListTagsForResource\u0026#34;, \u0026#34;route53resolver:UpdateResolverEndpoint\u0026#34;, \u0026#34;route53resolver:UpdateResolverRule\u0026#34;, \u0026#34;s3:AbortMultipartUpload\u0026#34;, \u0026#34;s3:CreateBucket\u0026#34;, \u0026#34;s3:DeleteBucket\u0026#34;, \u0026#34;s3:DeleteObject\u0026#34;, \u0026#34;s3:GetAccountPublicAccessBlock\u0026#34;, \u0026#34;s3:GetBucketAcl\u0026#34;, \u0026#34;s3:GetBucketOwnershipControls\u0026#34;, \u0026#34;s3:GetBucketPolicy\u0026#34;, \u0026#34;s3:GetBucketPolicyStatus\u0026#34;, \u0026#34;s3:GetBucketPublicAccessBlock\u0026#34;, \u0026#34;s3:GetObject\u0026#34;, \u0026#34;s3:GetObjectVersion\u0026#34;, \u0026#34;s3:GetBucketVersioning\u0026#34;, \u0026#34;s3:ListAccessPoints\u0026#34;, \u0026#34;s3:ListAccessPointsForObjectLambda\u0026#34;, \u0026#34;s3:ListAllMyBuckets\u0026#34;, \u0026#34;s3:ListBucket\u0026#34;, \u0026#34;s3:ListBucketMultipartUploads\u0026#34;, \u0026#34;s3:ListBucketVersions\u0026#34;, \u0026#34;s3:ListJobs\u0026#34;, \u0026#34;s3:ListMultipartUploadParts\u0026#34;, \u0026#34;s3:ListMultiRegionAccessPoints\u0026#34;, \u0026#34;s3:ListStorageLensConfigurations\u0026#34;, \u0026#34;s3:PutAccountPublicAccessBlock\u0026#34;, \u0026#34;s3:PutBucketAcl\u0026#34;, \u0026#34;s3:PutBucketPolicy\u0026#34;, \u0026#34;s3:PutBucketPublicAccessBlock\u0026#34;, \u0026#34;s3:PutObject\u0026#34;, \u0026#34;secretsmanager:CreateSecret\u0026#34;, \u0026#34;secretsmanager:DeleteSecret\u0026#34;, \u0026#34;secretsmanager:DescribeSecret\u0026#34;, \u0026#34;secretsmanager:GetSecretValue\u0026#34;, \u0026#34;secretsmanager:ListSecrets\u0026#34;, \u0026#34;secretsmanager:ListSecretVersionIds\u0026#34;, \u0026#34;secretsmanager:PutResourcePolicy\u0026#34;, \u0026#34;secretsmanager:TagResource\u0026#34;, \u0026#34;secretsmanager:UpdateSecret\u0026#34;, \u0026#34;sns:ListTopics\u0026#34;, \u0026#34;ssm:DescribeInstanceProperties\u0026#34;, \u0026#34;ssm:DescribeSessions\u0026#34;, \u0026#34;ssm:GetConnectionStatus\u0026#34;, \u0026#34;ssm:GetParameters\u0026#34;, \u0026#34;ssm:ListAssociations\u0026#34;, \u0026#34;ssm:ResumeSession\u0026#34;, \u0026#34;ssm:StartSession\u0026#34;, \u0026#34;ssm:TerminateSession\u0026#34; ], \u0026#34;Resource\u0026#34;: \u0026#34;*\u0026#34; } ] } Policy Coverage: This IAM policy grants permissions across multiple AWS services essential for the workshop:\nEC2 \u0026amp; VPC Networking: Create and manage VPCs, subnets, route tables, security groups, Transit Gateway, VPN connections, and VPC endpoints CloudFormation: Deploy and manage infrastructure as code stacks IAM: Create roles and instance profiles for EC2 instances S3: Create buckets and manage objects for workshop resources Lambda \u0026amp; CloudWatch: Deploy serverless functions and monitor resources Route53: Configure DNS and resolver endpoints SSM (Systems Manager): Enable secure instance access via Session Manager Secrets Manager: Store and retrieve sensitive configuration data Environment Setup Using Infrastructure as Code Workshop Region: This lab is designed for the US East (N. Virginia) region (us-east-1). Ensure you select this region before proceeding.\nAutomated Deployment Overview: Instead of manually creating each resource, we\u0026rsquo;ll use AWS CloudFormation to automate the entire infrastructure setup. This Infrastructure as Code (IaC) approach provides:\nConsistency: All participants work with identical configurations Speed: Deploy complex multi-resource environments in minutes Repeatability: Easy to recreate or clean up the environment Best Practices: Pre-configured with AWS recommended settings Deployment Steps:\nLaunch the CloudFormation Stack:\nClick this link to open the CloudFormation quick-create console: Deploy Workshop Infrastructure The template URL is pre-populated with the workshop configuration Stack name is automatically set to PLCloudSetup Review Stack Parameters: Review the pre-configured parameters (all defaults are optimized for this workshop):\nAcknowledge IAM Resource Creation: Scroll to the bottom of the page Check both acknowledgment boxes: ‚úì \u0026ldquo;I acknowledge that AWS CloudFormation might create IAM resources.\u0026rdquo; ‚úì \u0026ldquo;I acknowledge that AWS CloudFormation might create IAM resources with custom names.\u0026rdquo; Click Create stack Monitor Deployment Progress: CloudFormation will begin provisioning resources Expected deployment time: approximately 15-20 minutes You can monitor progress in the CloudFormation console under the \u0026ldquo;Events\u0026rdquo; tab The stack uses nested stacks to organize resource creation logically What Gets Created: The CloudFormation template automatically provisions:\n2 Virtual Private Clouds (VPCs): One simulating cloud environment, one simulating on-premises Multiple Subnets: Public and private subnets across availability zones AWS Transit Gateway: Central hub for VPC connectivity Site-to-Site VPN: Pre-configured VPN connection between VPCs 3 EC2 Instances: Test instances in different network segments Security Groups: Pre-configured for workshop traffic IAM Roles: Instance profiles for EC2 instances Route Tables: Properly configured routing between environments Verify Deployment:\nOnce the stack status shows CREATE_COMPLETE, verify the resources:\nCheck VPCs Created: Navigate to VPC Dashboard and confirm two VPCs are present: Cloud VPC (for AWS cloud resources) On-Premises VPC (simulating datacenter) Check EC2 Instances: Navigate to EC2 Dashboard and verify three instances are running: Cloud Test Instance (in Cloud VPC) On-Prem Test Instance (in On-Prem VPC) VPN Gateway Instance (for Site-to-Site connectivity) Troubleshooting:\nIf stack creation fails, check the \u0026ldquo;Events\u0026rdquo; tab for error messages Ensure your IAM user has all required permissions from the policy above Verify you\u0026rsquo;re in the us-east-1 region Check AWS service quotas for EC2, VPC, and CloudFormation if you encounter limits You\u0026rsquo;re now ready to proceed with the workshop exercises!\n"},{"uri":"https://thienluhoan.github.io/workshop-template/5-workshop/5.3-s3-vpc/5.3.2-test-gwe/","title":"Testing Gateway Endpoint Connectivity","tags":[],"description":"","content":" ‚ö†Ô∏è Note: The information below is for reference purposes only. Please do not copy verbatim for your report, including this warning.\nNow that you\u0026rsquo;ve created the Gateway Endpoint, it\u0026rsquo;s time to verify that it works correctly. In this section, you\u0026rsquo;ll create an S3 bucket, connect to an EC2 instance in your VPC, and upload a test file to S3 - all without traffic leaving the AWS network.\nTesting Objectives:\nVerify private connectivity to S3 through the Gateway Endpoint Confirm that traffic does not traverse the public internet Demonstrate successful S3 operations from a private subnet Validate route table configuration Step 1: Create an S3 Bucket for Testing First, you need an S3 bucket to test uploads through the Gateway Endpoint.\nNavigate to S3 Console: Open the S3 Management Console Click the Create bucket button Configure Bucket Settings:\nBucket Name:\nEnter a globally unique name (S3 bucket names must be unique across all AWS accounts) Suggested format: gateway-endpoint-test-\u0026lt;your-initials\u0026gt;-\u0026lt;random-number\u0026gt; Example: gateway-endpoint-test-jd-20241207 AWS Region:\nEnsure the region is US East (N. Virginia) us-east-1 This must match the region where your VPC and Gateway Endpoint are deployed Object Ownership:\nLeave as default (ACLs disabled - Recommended) Block Public Access settings:\nLeave all options checked (default) This ensures the bucket remains private Bucket Versioning:\nLeave as default (Disabled) Default Encryption:\nLeave as default (Server-side encryption with Amazon S3 managed keys - SSE-S3) Create the Bucket: Scroll to the bottom Click Create bucket Verify Creation: You should see a success message Your new bucket appears in the buckets list Security Best Practice: Notice that the bucket is created with all public access blocked by default. This aligns with AWS security best practices. Access will only be possible through the VPC endpoint and with proper IAM permissions.\nStep 2: Connect to EC2 Instance Using Session Manager You\u0026rsquo;ll use AWS Systems Manager Session Manager to securely connect to the EC2 instance without requiring SSH keys, bastion hosts, or open inbound ports.\nWhat is AWS Session Manager? Session Manager is a fully managed Systems Manager capability that provides:\nSecure Access: Browser-based shell access without opening inbound ports Audit Trail: All sessions are logged in CloudWatch Logs No SSH Keys: No need to manage or distribute SSH keys Cross-Platform: Works with Linux and Windows instances Architecture Context: The EC2 instance you\u0026rsquo;ll connect to is running in a private subnet within \u0026ldquo;VPC Cloud\u0026rdquo;. It has no direct internet access - connectivity to AWS services (including Systems Manager) is provided through the SSM VPC Interface Endpoints that were created during CloudFormation deployment.\nOpen Systems Manager Console: In the AWS Management Console search bar, type Systems Manager Press Enter or click on the Systems Manager service Navigate to Session Manager: In the left navigation pane, locate Node Management section Click on Session Manager Start a New Session: Click the Start session button From the list of available instances, select the instance named Test-Gateway-Endpoint Instance Details: This EC2 instance is deployed in a private subnet of \u0026ldquo;VPC Cloud\u0026rdquo; and is configured with an IAM instance profile that grants necessary permissions for Session Manager and S3 access. The instance will be used to verify that S3 traffic flows through the Gateway Endpoint you created, rather than the public internet.\nSession Established: Session Manager opens a new browser tab with a shell prompt You should see a prompt like: sh-4.2$ or similar This indicates you\u0026rsquo;re successfully connected to the EC2 instance Connectivity Verification: The fact that Session Manager works confirms that the SSM Interface Endpoints are functioning correctly. If you couldn\u0026rsquo;t connect, it would indicate an issue with the Interface Endpoints or IAM permissions.\nStep 3: Create and Upload Test File to S3 Now you\u0026rsquo;ll create a test file on the EC2 instance and upload it to S3 through the Gateway Endpoint.\nNavigate to Home Directory:\nExecute the following command to switch to the ssm-user\u0026rsquo;s home directory:\ncd ~ Why this step? Working in the home directory ensures you have proper write permissions and a clean working environment.\nCreate a Test File:\nGenerate a 1GB test file using the following command:\nfallocate -l 1G testfile.xyz Command Explanation:\nfallocate: Efficiently allocates space for a file -l 1G: Specifies file size of 1 gigabyte testfile.xyz: The filename for our test file This creates a 1GB file almost instantly by allocating disk space without writing actual data. We use a large file to make the upload more observable and to demonstrate that the Gateway Endpoint can handle substantial data transfers.\nUpload File to S3 Bucket:\nUse the AWS CLI to upload the file to your S3 bucket:\naws s3 cp testfile.xyz s3://YOUR-BUCKET-NAME/ Important: Replace YOUR-BUCKET-NAME with the actual name of the bucket you created in Step 1.\nWhat\u0026rsquo;s Happening Behind the Scenes:\nThe EC2 instance makes an API call to S3 The VPC route table directs S3 traffic to the Gateway Endpoint (prefix list destination) Traffic flows through the Gateway Endpoint to S3, staying within the AWS network No internet gateway or NAT gateway is used Success Indicator: If the upload completes successfully, you\u0026rsquo;ll see output showing the file transfer progress and completion. This confirms that:\nThe Gateway Endpoint is configured correctly Route table entries are working as expected The EC2 instance has proper IAM permissions for S3 Private connectivity to S3 is functioning Terminate the Session: Type exit or close the browser tab The session will be cleanly terminated Step 4: Verify Object in S3 Bucket Finally, confirm that the file was successfully uploaded by checking the S3 console.\nReturn to S3 Console:\nNavigate back to the S3 Management Console Open Your Bucket:\nClick on the name of the bucket you created earlier Verify File Presence:\nYou should see testfile.xyz listed in the Objects tab The file size should show approximately 1 GB Note the upload timestamp Verification Checklist:\n‚úì File testfile.xyz appears in bucket ‚úì File size matches (1 GB) ‚úì Upload timestamp is recent ‚úì No errors in S3 console Understanding What You Just Accomplished Network Path Verification: In this test, you successfully:\nCreated Private Connectivity: The Gateway Endpoint enabled private connectivity from your VPC to S3 Bypassed Public Internet: The upload did not traverse the public internet - it stayed entirely within AWS\u0026rsquo;s network backbone Used Route-Based Routing: The route table automatically directed S3 traffic to the Gateway Endpoint based on the S3 prefix list Demonstrated Cost Savings: By using a Gateway Endpoint instead of a NAT Gateway, you avoided: NAT Gateway hourly charges NAT Gateway data processing charges Potential internet gateway data transfer costs Traffic Flow Diagram:\nEC2 Instance ‚Üí VPC Route Table ‚Üí Gateway Endpoint ‚Üí S3 Service (Private Subnet) (Routes S3 to vpce-xxx) (No internet) (AWS Network) Key Differences from Internet-Based Access:\nWithout Gateway Endpoint: EC2 ‚Üí NAT Gateway ‚Üí Internet Gateway ‚Üí Public Internet ‚Üí S3 With Gateway Endpoint: EC2 ‚Üí Gateway Endpoint ‚Üí S3 (all within AWS network) Performance Benefits: Gateway Endpoints not only save costs but also typically provide better performance than internet-based access due to:\nLower latency (direct AWS network path) Higher available bandwidth No NAT Gateway bottleneck Dedicated AWS backbone infrastructure Section Summary Congratulations! You\u0026rsquo;ve successfully completed the Gateway Endpoint testing exercise.\nWhat You Learned:\n‚úì How to create and configure S3 buckets with security best practices ‚úì Using AWS Systems Manager Session Manager for secure instance access ‚úì Uploading objects to S3 through a Gateway VPC Endpoint ‚úì Verifying private connectivity without internet gateway dependency ‚úì Understanding the traffic flow in a VPC with Gateway Endpoints Key Takeaways:\nGateway Endpoints provide cost-effective private connectivity to S3 and DynamoDB No additional charges for using Gateway Endpoints - you only pay for S3 storage and requests Traffic remains private - never exposed to the public internet Automatic routing through route table entries - no complex configuration needed Scalable and highly available - AWS manages the endpoint infrastructure Next Steps: In the following sections, you\u0026rsquo;ll explore:\nCreating Interface VPC Endpoints for other AWS services Implementing VPC Endpoint policies for fine-grained access control Testing connectivity from on-premises (simulated) environments Understanding the differences between Gateway and Interface Endpoints "},{"uri":"https://thienluhoan.github.io/workshop-template/1-worklog/1.2-week2/","title":"Week 2 Worklog","tags":[],"description":"","content":" ‚ö†Ô∏è Note: The following information is for reference purposes only. Please do not copy verbatim for your own report, including this warning.\nWeek 2 Objectives: Learn about AWS storage service (S3) and basic concepts. Understand Virtual Private Cloud (VPC) for networking. Study Identity and Access Management (IAM) for security. Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 2 - Create S3 bucket to host static website - Upload HTML/CSS demo files to S3 15/09/2025 15/09/2025 AWS Journey 3 - Enable Static Website Hosting feature in S3 - Configure bucket policy for public read access - Test website access via S3 link 16/09/2025 16/09/2025 AWS Journey 4 - Create RDS MySQL instance (Free Tier) - Configure VPC security group to allow DB connection - Note endpoint \u0026amp; credentials 17/09/2025 17/09/2025 AWS Journey 5 - Create EC2 instance, install MySQL client - Connect from EC2 to RDS via command line - Test creating database \u0026amp; simple table 18/09/2025 18/09/2025 AWS Journey 6 - Learn about Route53, create hosted zone - Add A/CNAME records to point domain to S3 static site - Test website access via domain 19/09/2025 19/09/2025 AWS Journey Week 2 Achievements: Learned about Amazon S3 storage:\nUnderstood S3 as object storage service for files and data Learned about different storage classes: S3 Standard - for frequently accessed data S3 Glacier - for archival and backup Studied bucket policies and access control Learned about versioning for data protection Practiced with S3:\nCreated S3 buckets with unique names Uploaded and downloaded files using Console and CLI: aws s3 cp file.txt s3://my-bucket/ aws s3 ls s3://my-bucket/ Configured bucket policies for access control Enabled versioning on buckets Set up simple static website hosting Tested file management operations Understood VPC networking basics:\nLearned VPC as virtual network in AWS Studied CIDR blocks and IP addressing Understood difference between public and private subnets: Public subnet - has internet access Private subnet - no direct internet access Learned about VPC components: Internet Gateway - for internet connectivity Route Tables - for traffic routing Security Groups - instance-level firewall NACLs - subnet-level firewall Practiced with VPC:\nCreated custom VPC with CIDR block Created public and private subnets Configured route tables for subnets Attached Internet Gateway to VPC Launched EC2 instance in custom VPC Configured Security Groups for access control Tested network connectivity Learned IAM security basics:\nUnderstood IAM for access management Learned about IAM components: Users - individual accounts Groups - collections of users Roles - for AWS services Policies - permission definitions Studied IAM best practices: Use MFA for security Follow least privilege principle Create separate users instead of using root Practiced with IAM:\nCreated IAM users and groups Attached policies to users and groups Created custom policies for specific permissions Set up IAM roles for EC2 Enabled MFA for users Tested user permissions and access Gained practical AWS skills:\nCan manage storage with S3 Understand basic networking with VPC Know how to control access with IAM Ready for more advanced topics in Week 3 "},{"uri":"https://thienluhoan.github.io/workshop-template/5-workshop/5.3-s3-vpc/","title":"Access S3 from VPC","tags":[],"description":"","content":"Using Gateway endpoint In this section, you will create a Gateway eendpoint to access Amazon S3 from an EC2 instance. The Gateway endpoint will allow upload an object to S3 buckets without using the Public Internet. To create an endpoint, you must specify the VPC in which you want to create the endpoint, and the service (in this case, S3) to which you want to establish the connection.\nContent Create gateway endpoint Test gateway endpoint "},{"uri":"https://thienluhoan.github.io/workshop-template/3-blogstranslated/3.3-blog3/","title":"Blog 3","tags":[],"description":"","content":" ‚ö†Ô∏è Note: The information below is for reference purposes only. Please do not copy verbatim for your report, including this warning.\nHow Launchpad from Pega Enables Secure SaaS Extensibility with AWS Lambda Authors: Anton Aleksandrov, Giridhar Ramadhenu, Rajesh Kumar Maram, and Anubhav Sharma\nPublished: 30 MAY 2025\nCategories: AWS Lambda, Best Practices, Customer Solutions, Serverless, Technical How-to\nLarge organizations increasingly adopt software as a service (SaaS) solutions to focus on business priorities, reduce infrastructure management overhead, and optimize costs. These organizations expect SaaS vendors to provide customizability facilities for tailoring the solution behavior according to their needs. Although traditional approaches like feature flags and webhooks offer some flexibility, they often fall short of providing a high degree of customizability. A new emerging pattern in this space is tenant-supplied custom code execution, which allows tenants to inject their own code into specific workflow points, enabling deep customization while preserving the core SaaS solutions\u0026rsquo; integrity and security.\nIn this post, we share how Pegasystems (Pega) built Launchpad, its new SaaS development platform, to solve a core challenge in multi-tenant environments: enabling secure customer customization. By running tenant code in isolated environments with AWS Lambda, Launchpad offers its customers a secure, scalable foundation, eliminating the need for bespoke code customizations.\nSolution Overview Launchpad, which is built on AWS, is an end-to-end platform on which software providers can build, launch, and operate workflow-centric B2B SaaS applications and AI solutions. It provides a managed, secure, scalable cloud environment for hosting multi-tenant applications and data. It accelerates the build experience with generative AI-powered low code tools, prebuilt capabilities, and subscriber-level configuration. Being a multi-tenant platform at its core, Launchpad had to maintain stringent isolation across tenants in its architecture.\nOne of the requirements Launchpad had was to allow their tenants to augment the workflows natively by providing custom code. Some common scenarios included:\nCommunicating with external systems with proprietary non-industry-standard protocols Reuse of existing business logic SDK-based custom code development The solution necessitated the ability for tenants to provide custom code that would implement the required business logic, which Launchpad would be executing. This required architecting a secure runtime environment for custom code execution that maintains the highest degree of cross-tenant isolation within the multi-tenant architecture, at the same time allowing sufficient access to platform APIs and services. It was essential to build an architecture that would decouple the environment running tenant code from the core SaaS platform.\nArchitecting the Solution Topology To achieve the required high level of compute isolation for running code provided by different tenants, Launchpad has adopted Lambda functions in its architecture as the secure ephemeral compute environment. Each untrusted code snippet provided by tenants is bootstrapped as a stand-alone Lambda function, with strong Firecracker-based isolation across different functions and execution environments addressing Launchpad\u0026rsquo;s requirements. This isolation provides:\nDedicated resources Customizable access permissions Independent monitoring and operations Automatic scaling for each function Complete separation from other functions and their execution environments With Lambda being a serverless compute service, adopting it for the Launchpad architecture yielded several significant benefits:\nMajor Business Benefit: Tenants could implement thousands of custom workflow augmentations on their own simply by providing code snippets, instead of the Launchpad engineering team being responsible for implementing them in the core platform code.\nTechnical Benefits:\nManaged runtimes ‚Äì AWS handles patching and updating the underlying infrastructure, operating system, and runtimes for customers, reducing the potential attack surface Fine-grained permissions ‚Äì Each function can have its own set of access policies to tightly control what resources and actions it can access No need to pre-provision and pay for overprovisioned capacity ‚Äì Lambda functions scale up and down automatically based on traffic patterns Built-in monitoring ‚Äì Lambda functions emit detailed metrics, logs, and traces through Amazon CloudWatch and AWS X-Ray out of the box, making it straightforward to monitor tenant code execution To further reduce risks, Launchpad runs these Lambda functions with untrusted code in a dedicated AWS account, separated from the core SaaS platform account. When end-users create a new function in the Launchpad authoring portal, they upload their code and specify the code handler to be executed during the invocation. Users can also map function input and output to Launchpad fields for further processing to enable an even higher degree of customizability and integration.\nThe multi-tenant authoring service is a Control Plane component that runs as a microservice on the Amazon Elastic Kubernetes Service (Amazon EKS) cluster and uses the Lambda API for function lifecycle management. After a function resource is created, it can be used for further invocations.\nRuntime Architecture At runtime, when Launchpad needs to invoke a function, it calls the Lambda Invoke API. Before the function is invoked, the multi-tenant runtime service performs a tenancy check to make sure the request is coming from an authorized tenant by doing the token validation. After a successful validation, the service invokes the required Lambda function. To invoke functions hosted in a different AWS account, the multi-tenant runtime service uses an AWS Identity and Access Management (IAM) role to assume the required permissions and invokes the Lambda service using the AWS SDK.\nThe workflow consists of the following steps:\nAn incoming user request reaches the application gateway service The application gateway authenticates the request using the tenancy security service After it\u0026rsquo;s authenticated, the request is forwarded to the multi-tenant runtime service The multi-tenant runtime service validates the supplied token and performs a tenancy check (ensuring tenants can only invoke own functions they have permissions for) The multi-tenant runtime service pod assumes the IAM role required for invoking the tenant-specific Lambda function in a different AWS account The multi-tenant runtime service pod invokes the required Lambda function Invoking the platform API from custom code is as straightforward as connecting to any external API. The custom code can authenticate with the platform using OAuth2. To facilitate the authentication, the developer can pass along the credentials as input parameters to the function from the core platform. Then the developer can create a corresponding record (isolated by tenant) in the platform that stores the credentials per function, and pass credentials as input parameters during invocation.\nDistributed Architecture Observability Operating a distributed architecture that runs untrusted code across multiple AWS accounts requires a comprehensive observability strategy. Launchpad\u0026rsquo;s approach combines centralized logging and monitoring with cross-account aggregation to provide a unified operational view of the platform.\nThe monitoring architecture uses CloudWatch Metrics to observe the Lambda functions, aggregating them through a centralized observability layer. This setup empowers platform operators to correlate Lambda function metrics with the core platform services running on Amazon EKS. Launchpad also collects per-function telemetry like:\nFunction invocations Error rates Execution time These telemetry dimensions enable both a platform-wide and tenant-specific monitoring perspective.\nFor logging and troubleshooting, Launchpad implements a unified logging pipeline that aggregates Lambda function logs with application gateway and runtime service logs. Each request flowing through the system carries a correlation ID, so operators can trace execution paths across the core SaaS services and into the tenant functions running in the AWS account running tenant Lambda functions.\nWith this multi-layer observability architecture, Launchpad can maintain operational excellence while running tenant code securely at scale. Regular operational reviews drive continuous improvements in monitoring coverage and incident response procedures. Having per-tenant Lambda functions make it possible for Launchpad to use tenant-specific cost allocation tags, further empowering them to understand the cost footprint of running tenant custom code.\nBest Practices When building a SaaS solution, maintaining a unified core code base is essential for scalability and manageability. Implementing per-tenant variations within the core platform code can lead to maintenance complexity and technical debt. Instead, architect your SaaS solution to have extension points, which allow your tenants to inject their custom code at specific points in the workflow, enabling customization without compromising the platform\u0026rsquo;s maintainability. This pattern makes sure the core SaaS platform remains clean and standardized while offering the flexibility that customers demand.\nAdditional best practices include:\nUse separate accounts for running Lambda functions with untrusted tenant-provided code to ensure it\u0026rsquo;s isolated from your core SaaS platform code Grant absolute minimum required access permissions to the execution role assigned to the function. The custom code running within the execution environment gets permissions defined in the execution role when making requests to AWS API endpoints. If the function doesn\u0026rsquo;t need to reach out to AWS API endpoints, remove all permissions from the execution role and add an explicit AWSDenyAll policy Use separate Lambda functions for each code snippet and each tenant. This will provide the highest degree of cross-tenant isolation. Resources are not reused across different functions and execution environments Use Lambda layers in case you need to add a layer of vendor-provided code in order to keep it separated from the untrusted tenant-provided code Implement additional security controls, such as using Amazon Virtual Private Cloud (Amazon VPC) constructs to restrict network access and VPC Flow Logs for network activity monitoring Conclusion The implementation of a secure untrusted code execution environment within SaaS platforms addresses a critical need for tenant customization while maintaining architectural integrity. Lambda offers a built-in isolation model, fine-grained security controls, and serverless scalability, so SaaS providers such as Launchpad can address the requirements of executing tenant-provided code in a multi-tenant environment and offer robust customization capabilities while maintaining strict security boundaries and operational efficiency.\nThis architectural pattern enables providers to focus on core platform development while confidently supporting tenant-specific workflows through the secure and scalable Lambda execution environment.\nTo learn more:\nSecurity Overview of AWS Lambda white paper Serverless architectural patterns at Serverlessland.com About the Authors Anton Aleksandrov - Principal Solutions Architect for AWS Serverless and Event-Driven architectures. With over two decades of hands-on engineering and architecture experience, Anton works with major ISV and SaaS customers to design highly scalable, innovative, and secure cloud solutions.\nGiridhar Ramadhenu - Seasoned software architect with over 2 decades of expertise, specializing in microservices, event-driven, and layered architectures. As a Fellow Software Architect for Launchpad at Pegasystems and an influential member of the Architecture Guild, Giridhar plays a pivotal role in shaping the architecture of various products.\nRajesh Kumar Maram - Senior Principal Software Engineer for Launchpad at Pegasystems with over a decade of experience. He leads with innovation in solving challenging problems and explores the latest AWS technologies for Pega business use cases.\nAnubhav Sharma - Principal Solutions Architect at AWS with over 2 decades of experience in coding and architecting business-critical applications. He specializes in guiding ISVs and enterprises through their journey of building, deploying, and operating SaaS solutions on AWS.\n"},{"uri":"https://thienluhoan.github.io/workshop-template/4-eventparticipated/4.3-event3/","title":"Event 3","tags":[],"description":"","content":" ‚ö†Ô∏è Note: The information below is for reference purposes only. Please do not copy it verbatim into your report, including this warning.\nSummary Report: \u0026ldquo;AWS Security \u0026amp; Compliance Workshop\u0026rdquo; Date \u0026amp; Time: Saturday, October 26, 2025, 2:00 PM ‚Äì 5:00 PM\nLocation: Online Event (Zoom)\nRole: Workshop Participant\nOrganizer: First Cloud Journey, AWS User Group Vietnam\nEvent Purpose This workshop focused on AWS security best practices and compliance requirements. The event aimed to help participants understand how to build secure cloud infrastructure, implement security monitoring, and maintain compliance with industry standards using AWS security services.\nAgenda Overview 2:00 ‚Äì 2:15 PM | Welcome \u0026amp; Introduction\nWorkshop overview and security fundamentals Introduction to AWS Shared Responsibility Model Overview of security landscape in cloud computing Discussion of common security challenges 2:15 ‚Äì 3:30 PM | AWS Security Services\nAWS IAM (Identity and Access Management)\nIAM best practices and policies Multi-Factor Authentication (MFA) IAM roles and service accounts Permission boundaries and SCPs AWS Security Monitoring\nAmazon GuardDuty for threat detection AWS Security Hub for centralized security view AWS CloudTrail for audit logging Amazon Macie for data discovery and protection Network Security\nVPC security best practices Security Groups and Network ACLs AWS WAF (Web Application Firewall) AWS Shield for DDoS protection 3:30 ‚Äì 3:45 PM | Break\nQ\u0026amp;A session and networking 3:45 ‚Äì 5:00 PM | Compliance \u0026amp; Data Protection\nAWS Encryption Services\nAWS KMS (Key Management Service) Encryption at rest and in transit AWS Certificate Manager AWS Secrets Manager Compliance Frameworks\nAWS compliance programs overview AWS Artifact for compliance reports GDPR, HIPAA, PCI DSS considerations Compliance automation with AWS Config Hands-on Lab: Securing a Web Application\nSetting up security monitoring with GuardDuty Implementing encryption with KMS Configuring WAF rules Testing security controls Key Highlights Defense in Depth: Multiple layers of security controls Automated Monitoring: GuardDuty and Security Hub provide continuous threat detection Encryption Everywhere: Easy implementation of encryption using AWS services Compliance Made Easier: AWS services help meet compliance requirements Audit Trail: CloudTrail provides complete activity logging Key Learnings Understanding the AWS Shared Responsibility Model is fundamental IAM policies should follow principle of least privilege Encryption should be enabled by default for sensitive data Security monitoring must be continuous, not periodic Compliance is a shared responsibility between AWS and customers Security automation reduces human error Application to Internship Immediate Applications:\nReview and improve IAM policies in practice projects Enable GuardDuty for threat detection Implement encryption for S3 buckets and RDS databases Set up CloudTrail for audit logging Security Best Practices:\nAlways use MFA for privileged accounts Regularly rotate access keys and credentials Implement least privilege access control Enable encryption at rest and in transit Monitor security events continuously Compliance Considerations:\nUnderstand which compliance frameworks apply Use AWS Config for compliance automation Document security controls and processes Regular security audits and reviews Personal Experience The AWS Security \u0026amp; Compliance Workshop provided valuable insights into cloud security.\nUnderstanding Security Fundamentals:\nThe Shared Responsibility Model clarified AWS vs customer responsibilities Learned that security must be built into architecture from the start Understanding of defense-in-depth approach Hands-on Security Tools:\nGuardDuty demo showed how threat detection works automatically Security Hub provides centralized security dashboard KMS encryption implementation was straightforward WAF configuration demonstrated web application protection Compliance Awareness:\nLearned about major compliance frameworks Understanding of how AWS helps with compliance AWS Artifact provides access to compliance reports Config rules can automate compliance checks Practical Skills:\nBasic understanding of security service architecture Ability to implement common security controls Knowledge of where to find compliance documentation Understanding of security monitoring tools Challenges:\nSecurity concepts can be complex and overwhelming Balancing security with usability and cost Understanding which security services to use when Keeping up with evolving security threats Event Photos Workshop screenshots will be added here\nOverall, this workshop emphasized that security is not optional but fundamental to cloud computing. The practical demonstrations and hands-on labs helped make security concepts more tangible and provided a foundation for implementing secure AWS infrastructure.\n"},{"uri":"https://thienluhoan.github.io/workshop-template/5-workshop/5.4-s3-onprem/5.4.3-test-endpoint/","title":"Testing Interface Endpoint Connectivity from On-Premises","tags":[],"description":"","content":" ‚ö†Ô∏è Important: The information provided here is for reference purposes only. Please do not copy verbatim for your report including this warning message.\nIn this section, you will validate that the Interface VPC Endpoint is working correctly by testing S3 connectivity from the simulated on-premises environment. This demonstrates how enterprise applications running in corporate datacenters can securely access AWS services through private connections.\nTesting Objectives:\nRetrieve the Interface Endpoint DNS name for S3 API calls Connect to an EC2 instance simulating an on-premises server Upload data to S3 using the private endpoint connection Verify successful data transfer through the hybrid architecture Why This Test Matters:\nThis validation confirms that your hybrid networking configuration is functioning correctly:\nDNS resolution from on-premises to endpoint private IPs VPN routing directing traffic to the cloud VPC Interface Endpoint ENIs receiving and processing S3 API requests Security groups allowing proper traffic flow End-to-end private connectivity without internet exposure Part 1: Retrieve Interface Endpoint DNS Information Before testing connectivity, you need to obtain the endpoint-specific DNS name that on-premises systems will use to reach S3.\nStep 1: Access VPC Endpoints Console\nOpen the Amazon VPC Console In the left navigation pane, click Endpoints You should see your recently created Interface Endpoint Step 2: Locate Endpoint DNS Names\nFind your endpoint:\nLook for the endpoint you created in the previous section It should have a name like S3-Interface-Endpoint-HybridAccess or similar Click on the Endpoint ID (format: vpce-xxxxxxxxx) to view details Access DNS configuration:\nIn the endpoint details page, scroll to the Details tab Look for the DNS names section Identify the Regional DNS Name:\nYou\u0026rsquo;ll see multiple DNS names listed Copy the first DNS name (the regional endpoint DNS name) Format: vpce-xxxxxxxxx.s3.us-east-1.vpce.amazonaws.com Save this to a text editor - you\u0026rsquo;ll need it shortly DNS Name Types: Interface Endpoints provide several DNS names:\nRegional DNS name: Works across all AZs (recommended for general use) Zonal DNS names: Specific to each AZ where you deployed ENIs Private DNS name: Only works if you enabled private DNS (which we didn\u0026rsquo;t) For this test, use the regional DNS name as it provides automatic failover between AZs.\nUnderstanding the DNS Name Structure:\nThe regional DNS name follows this pattern:\nvpce-\u0026lt;endpoint-id\u0026gt;.s3.\u0026lt;region\u0026gt;.vpce.amazonaws.com This DNS name resolves to the private IP addresses of the Interface Endpoint ENIs deployed in your VPC subnets.\nPart 2: Connect to Simulated On-Premises Server Now you\u0026rsquo;ll connect to an EC2 instance that simulates an on-premises server in your corporate datacenter.\nStep 1: Open AWS Systems Manager Session Manager\nNavigate to Systems Manager:\nIn the AWS Console search bar at the top, type Session Manager Select Session Manager from the results (under Systems Manager) Access Session Manager:\nYou\u0026rsquo;ll be taken to the Session Manager start page This service provides secure shell access without requiring SSH keys or bastion hosts Session Manager Advantages: Session Manager provides secure browser-based shell access to EC2 instances without:\nOpening inbound SSH ports (improved security) Managing SSH keys or credentials Deploying bastion hosts Configuring security group SSH rules Perfect for enterprise environments with strict security requirements.\nStep 2: Start Interactive Session\nInitiate connection:\nClick the Start session button You\u0026rsquo;ll see a list of available EC2 instances Select the on-premises instance:\nLook for an instance named Test-Interface-Endpoint This EC2 instance runs in \u0026ldquo;VPC On-prem\u0026rdquo; subnet It simulates a server in your corporate datacenter Click the radio button next to this instance Start the session:\nClick Start session button at the bottom Session Manager will open a new browser tab with a shell prompt You should see: sh-4.2$ or similar prompt Instance Context: The Test-Interface-Endpoint EC2 instance is configured to:\nRun in the \u0026ldquo;VPC On-prem\u0026rdquo; private subnet Route traffic through the VPN Gateway you configured Use the Route 53 Outbound Resolver for DNS queries Simulate an on-premises application server accessing AWS services Part 3: Prepare Test Data for Upload You\u0026rsquo;ll now create a test file to upload to S3, demonstrating data transfer through the private endpoint.\nStep 1: Navigate to User Home Directory\nIn the Session Manager terminal, execute:\ncd ~ This changes to the ssm-user home directory where you have write permissions.\nStep 2: Create a Test File\nGenerate a 1GB test file to simulate a realistic data transfer:\nfallocate -l 1G onprem-data.dat Command Breakdown:\nfallocate: Efficiently creates a file of specified size -l 1G: Allocates 1 gigabyte of space onprem-data.dat: Output filename This creates a sparse file instantly without writing actual data (perfect for testing).\nStep 3: Verify File Creation\nConfirm the file was created:\nls -lh onprem-data.dat You should see output showing a 1.0G file.\nFile Size Selection: Using a 1GB file provides:\nMeasurable transfer time to observe network performance Sufficient size to verify throughput through VPN and endpoint Realistic representation of enterprise data transfers Quick enough for workshop purposes (full upload in seconds over AWS network) Part 4: Upload Data Through Interface Endpoint Now comes the critical test: uploading data to S3 using the Interface Endpoint you created.\nStep 1: Construct the Endpoint URL\nFor Interface Endpoints, you must specify a custom endpoint URL. The format is:\nhttps://bucket.\u0026lt;regional-dns-name\u0026gt; Build your URL:\nStart with: https://bucket. Append the regional DNS name you copied earlier Example result: https://bucket.vpce-0a1b2c3d4e5f.s3.us-east-1.vpce.amazonaws.com DNS Name Format: When copying the regional DNS name from the VPC console:\nDo NOT include the leading asterisk (*.) if shown Use only the portion: vpce-xxxxxxx.s3.us-east-1.vpce.amazonaws.com The bucket. prefix is added in the endpoint URL Step 2: Identify Your S3 Bucket\nYou need the name of the S3 bucket created earlier in section 5.3 (Gateway Endpoint testing):\nFormat: Something like my-endpoint-test-bucket-123456 If you don\u0026rsquo;t remember, check the S3 console in another browser tab Step 3: Execute S3 Upload Command\nIn the Session Manager terminal, run:\naws s3 cp --endpoint-url https://bucket.\u0026lt;Regional-DNS-Name\u0026gt; onprem-data.dat s3://\u0026lt;your-bucket-name\u0026gt; Replace placeholders:\n\u0026lt;Regional-DNS-Name\u0026gt;: Your actual endpoint DNS name (without asterisk) \u0026lt;your-bucket-name\u0026gt;: Your actual S3 bucket name Example with real values:\naws s3 cp --endpoint-url https://bucket.vpce-0a1b2c3d4e.s3.us-east-1.vpce.amazonaws.com onprem-data.dat s3://my-endpoint-test-bucket-123456 Understanding the Command:\naws s3 cp: AWS CLI command to copy files to/from S3 --endpoint-url: Critical parameter - directs the S3 API call to the Interface Endpoint instead of public S3 endpoint onprem-data.dat: Source file (local file on the instance) s3://\u0026lt;bucket-name\u0026gt;: Destination S3 bucket Why \u0026ndash;endpoint-url is Required: By default, AWS CLI uses the public S3 endpoint (s3.amazonaws.com). The --endpoint-url parameter overrides this to use your Interface Endpoint\u0026rsquo;s private DNS name, ensuring traffic routes through the VPN and private endpoint rather than the internet.\nStep 4: Monitor Upload Progress\nYou should see output similar to:\nupload: ./onprem-data.dat to s3://my-bucket/onprem-data.dat Completed 1.0 GiB/1.0 GiB (50.0 MiB/s) The upload should complete within seconds given the high-speed AWS backbone network.\nWhat Just Happened:\nBehind the scenes, your data traveled through this path:\nEC2 On-Prem Instance ‚Üí Route Table Lookup ‚Üí VPN Gateway EC2 ‚Üí IPsec VPN Tunnel ‚Üí Transit Gateway ‚Üí VPC Cloud ‚Üí Interface Endpoint ENI (Private IP) ‚Üí AWS PrivateLink ‚Üí S3 Service All traffic remained on the AWS private network - no internet exposure!\nPart 5: Verify Successful Upload in S3 Console Finally, confirm the data arrived successfully in your S3 bucket.\nStep 1: Navigate to S3 Console\nOpen the Amazon S3 Console You\u0026rsquo;ll see a list of all your S3 buckets Step 2: Access Your Bucket\nLocate your bucket:\nFind the bucket you used in the upload command Click on the bucket name to view its contents View objects:\nYou should now see the file onprem-data.dat listed Check the Size column - it should show 1.0 GB Note the Last modified timestamp - should be recent Step 3: Inspect Object Details (Optional)\nClick on the object name to view detailed properties:\nStorage class: Likely Standard Server-side encryption: May show default encryption Metadata: Any custom metadata added Tags: Any tags applied during upload Test Successful! Your on-premises simulated environment successfully uploaded data to S3 through:\nRoute 53 Outbound Resolver (DNS query from on-prem) VPN Tunnel (network connectivity) Route 53 Inbound Resolver (DNS resolution to private IP) Interface VPC Endpoint (private S3 access point) AWS PrivateLink (secure service connection) All traffic remained private without traversing the public internet!\nUnderstanding Your Test Results Connectivity Validation:\nThis successful test confirms:\nDNS Resolution Working:\nOn-prem instance queried Outbound Resolver Query forwarded through VPN to Inbound Resolver Endpoint DNS name resolved to Interface Endpoint private IP Network Routing Functional:\nVPN tunnel carrying traffic between on-prem and cloud VPCs Route tables directing traffic correctly Transit Gateway (if used) routing between VPCs Security Configuration Correct:\nSecurity groups allowing HTTPS traffic to endpoint ENIs Endpoint policy permitting S3 operations IAM permissions on EC2 instance allowing S3 access Interface Endpoint Operational:\nENIs receiving requests on private IPs PrivateLink routing requests to S3 service Responses returning through private path Traffic Flow Diagram:\n‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê DNS Query ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îÇ On-Prem EC2 ‚îÇ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\u0026gt;‚îÇ Outbound Resolver‚îÇ ‚îÇ Instance ‚îÇ ‚îÇ (VPC On-prem) ‚îÇ ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îÇ ‚îÇ ‚îÇ Data Upload (HTTPS) ‚îÇ DNS Forward ‚îÇ ‚îÇ ‚ñº ‚ñº ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê VPN Tunnel ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îÇ VPN Gateway ‚îÇ‚óÑ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∫‚îÇ Inbound Resolver ‚îÇ ‚îÇ EC2 Instance ‚îÇ ‚îÇ (VPC Cloud) ‚îÇ ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îÇ ‚îÇ ‚îÇ ‚îÇ DNS Response ‚ñº ‚îÇ (Private IP) ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îÇ ‚îÇ Transit Gateway ‚îÇ ‚îÇ ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îÇ ‚îÇ ‚îÇ ‚ñº ‚ñº ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îÇ VPC Cloud ‚îÇ ‚îÇ Private Hosted ‚îÇ ‚îÇ Route Table ‚îÇ ‚îÇ Zone (Route 53) ‚îÇ ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îÇ ‚ñº ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îÇ Interface Endpoint ENI ‚îÇ ‚îÇ (Private IP: 10.0.x.x) ‚îÇ ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îÇ ‚îÇ AWS PrivateLink ‚ñº ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îÇ Amazon S3 Service ‚îÇ ‚îÇ (Backend in AWS Network) ‚îÇ ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò Cleanup for Next Section (Optional) If you want to test again or prepare for the next workshop section:\nIn Session Manager terminal:\n# Remove the local test file to free space rm onprem-data.dat In S3 Console (optional):\nYou can leave the object in S3 or delete it The object will be cleaned up in the final workshop cleanup section Leave Infrastructure Running: Do NOT delete the Interface Endpoint, VPN configuration, or Route 53 Resolvers yet. These will be used in subsequent workshop sections and cleaned up at the end.\nKey Takeaways What You Accomplished:\n‚úÖ Retrieved Interface Endpoint DNS names for API access configuration\n‚úÖ Connected to simulated on-premises server using Session Manager\n‚úÖ Created test data representing corporate files\n‚úÖ Uploaded data to S3 through private Interface Endpoint\n‚úÖ Verified successful transfer in S3 console\n‚úÖ Validated end-to-end hybrid connectivity across multiple AWS services\nEnterprise Architecture Patterns Demonstrated:\nHybrid Cloud Connectivity: VPN-based connection between on-prem and AWS Private Service Access: Using VPC Endpoints to avoid internet routing DNS Resolution: Hybrid DNS architecture with Route 53 Resolver Secure Remote Access: Session Manager for bastion-less connectivity Defense in Depth: Multiple layers (VPN, security groups, endpoint policies) Production Considerations:\nFor real-world deployments, enhance this architecture with:\nAWS Direct Connect instead of VPN for dedicated network connection Multiple VPN tunnels for redundancy and higher availability VPC Flow Logs to monitor and audit traffic patterns CloudWatch metrics for endpoint performance monitoring Restrictive endpoint policies limiting access to specific buckets Private DNS enablement for automatic DNS resolution Multi-region endpoints for disaster recovery scenarios This test validates that your hybrid architecture is production-ready for private S3 access from on-premises systems!\n"},{"uri":"https://thienluhoan.github.io/workshop-template/3-blogstranslated/","title":"Translated Blogs","tags":[],"description":"","content":" ‚ö†Ô∏è Note: The information below is for reference purposes only. Please do not copy verbatim for your own report, including this warning.\nThis section will list and introduce the blogs you have translated. For example:\nBlog 1 - Getting started with healthcare data lakes: Using microservices This blog introduces how to start building a data lake in the healthcare sector by applying a microservices architecture. You will learn why data lakes are important for storing and analyzing diverse healthcare data (electronic medical records, lab test data, medical IoT devices‚Ä¶), how microservices help make the system more flexible, scalable, and easier to maintain. The article also guides you through the steps to set up the environment, organize the data processing pipeline, and ensure compliance with security \u0026amp; privacy standards such as HIPAA.\nBlog 2 - Advanced Techniques in AWS Glue for Complex ETL Operations This blog covers advanced techniques in AWS Glue, focusing on complex ETL operations to transform and process data at scale. Learn about dynamic frames, custom transformations, job bookmarks, and performance optimization strategies for handling large datasets efficiently.\nBlog 3 - How Launchpad from Pega Enables Secure SaaS Extensibility with AWS Lambda This blog explores how Pegasystems built Launchpad, a SaaS development platform using AWS Lambda for secure tenant code execution. Discover the architecture patterns, security best practices, and observability strategies for running untrusted code in multi-tenant environments at scale. This blog introduces how to start building a data lake in the healthcare sector by applying a microservices architecture. You will learn why data lakes are important for storing and analyzing diverse healthcare data (electronic medical records, lab test data, medical IoT devices‚Ä¶), how microservices help make the system more flexible, scalable, and easier to maintain. The article also guides you through the steps to set up the environment, organize the data processing pipeline, and ensure compliance with security \u0026amp; privacy standards such as HIPAA.\n"},{"uri":"https://thienluhoan.github.io/workshop-template/1-worklog/1.3-week3/","title":"Week 3 Worklog","tags":[],"description":"","content":" ‚ö†Ô∏è Note: The following information is for reference purposes only. Please do not copy verbatim for your own report, including this warning.\nWeek 3 Objectives: Learn about Amazon RDS for managed databases. Understand AWS Lambda and serverless basics. Study CloudWatch for monitoring and logging. Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 2 - Introduction to CDN concepts and CloudFront benefits - Create CloudFront Distribution to distribute S3 website content 22/09/2025 22/09/2025 AWS Journey 3 - Configure CloudFront behavior and cache policy - Test website access via CloudFront URL - Perform invalidation to update new content 23/09/2025 23/09/2025 AWS Journey 4 - Introduction to DynamoDB (NoSQL Database) - Create DynamoDB tables (Users, Products, etc.) - Practice CRUD operations on Console 24/09/2025 24/09/2025 AWS Journey 5 - Connect and query DynamoDB using AWS CLI - Write small scripts to add and read data from tables 25/09/2025 25/09/2025 AWS Journey 6 - Learn about ElastiCache (Redis \u0026amp; Memcached) - Create basic Redis cluster - Test connection from EC2 to store and read cache data 26/09/2025 26/09/2025 AWS Journey Week 3 Achievements: Learned about Amazon RDS:\nUnderstood RDS as managed database service Learned benefits of managed databases: Automated backups Automatic updates Easy scaling Studied database engines: MySQL - popular for web applications PostgreSQL - advanced features Understood Multi-AZ for high availability Learned about Read Replicas for scaling Practiced with RDS:\nCreated RDS MySQL instance (db.t3.micro) Configured security groups for database access Connected to RDS from EC2 instance: mysql -h mydb.ap-southeast-1.rds.amazonaws.com -u admin -p Created sample database and tables Configured automated backups Tested snapshot creation and restore Monitored database metrics in CloudWatch Understood AWS Lambda basics:\nLearned serverless computing concept: No server management Auto scaling Pay per use Understood Lambda function structure: Handler function Event input Execution role Studied Lambda triggers: S3 events API Gateway CloudWatch Events Learned Lambda limitations: 15-minute timeout Memory limits Practiced with Lambda:\nCreated Lambda function in Python: def lambda_handler(event, context): return { \u0026#39;statusCode\u0026#39;: 200, \u0026#39;body\u0026#39;: \u0026#39;Hello from Lambda!\u0026#39; } Configured IAM execution role Set up S3 trigger for image processing Tested function execution Monitored logs in CloudWatch Analyzed function performance Learned CloudWatch monitoring:\nUnderstood CloudWatch components: Metrics - performance data Logs - application logs Alarms - automated alerts Learned about metric types: EC2 metrics (CPU, disk, network) RDS metrics (connections, storage) Lambda metrics (invocations, errors) Practiced with CloudWatch:\nCreated alarms for EC2 instances: High CPU usage Low disk space Set up alarms for RDS: High connections Low storage Viewed and analyzed CloudWatch Logs Built simple dashboard for monitoring Configured log retention policies Gained practical experience:\nCan set up and manage RDS databases Understand serverless with Lambda Know how to monitor with CloudWatch Ready for Week 4 advanced topics "},{"uri":"https://thienluhoan.github.io/workshop-template/5-workshop/5.4-s3-onprem/","title":"Access S3 from on-premises","tags":[],"description":"","content":"Overview In this section, you will create an Interface endpoint to access Amazon S3 from a simulated on-premises environment. The Interface endpoint will allow you to route to Amazon S3 over a VPN connection from your simulated on-premises environment.\nWhy using Interface endpoint:\nGateway endpoints only work with resources running in the VPC where they are created. Interface endpoints work with resources running in VPC, and also resources running in on-premises environments. Connectivty from your on-premises environment to the cloud can be provided by AWS Site-to-Site VPN or AWS Direct Connect. Interface endpoints allow you to connect to services powered by AWS PrivateLink. These services include some AWS services, services hosted by other AWS customers and partners in their own VPCs (referred to as PrivateLink Endpoint Services), and supported AWS Marketplace Partner services. For this workshop, we will focus on connecting to Amazon S3. "},{"uri":"https://thienluhoan.github.io/workshop-template/5-workshop/5.4-s3-onprem/5.4.4-dns-simulation/","title":"Configuring Hybrid DNS for Private Endpoint Resolution","tags":[],"description":"","content":" ‚ö†Ô∏è Important: The information provided here is for reference purposes only. Please do not copy verbatim for your report including this warning message.\nIn this section, you will configure the DNS infrastructure to enable seamless name resolution for S3 API calls from your on-premises environment. This simulates how enterprise networks use conditional DNS forwarding to resolve AWS service endpoints to private IP addresses.\nWhy DNS Configuration Is Critical:\nAWS PrivateLink endpoints use elastic network interfaces (ENIs) with fixed private IP addresses in each Availability Zone. While these IPs remain stable for the endpoint\u0026rsquo;s lifecycle, AWS strongly recommends using DNS resolution rather than hardcoding IPs because:\nDynamic AZ Management: ENIs may be added to new AZs or removed over time Automatic Failover: DNS allows automatic switching between AZ-specific IPs during outages Service Updates: AWS may update endpoint infrastructure requiring new IPs Best Practice: DNS provides abstraction and flexibility for enterprise architectures What You\u0026rsquo;ll Configure:\nDNS Alias Records: Map S3 service domains to Interface Endpoint IPs in Route 53 Private Hosted Zone Resolver Forwarding Rule: Direct on-premises DNS queries for S3 to the cloud VPC End-to-End Testing: Validate DNS resolution and S3 access using the simulated on-premises environment This configuration mimics real-world hybrid DNS architectures where on-premises DNS servers conditionally forward specific domains to AWS for private resolution.\nPart 1: Create DNS Alias Records for Interface Endpoint You\u0026rsquo;ll now create DNS records in the Route 53 Private Hosted Zone that map S3 service names to your Interface Endpoint\u0026rsquo;s private IP addresses.\nUnderstanding the Private Hosted Zone:\nThe CloudFormation stack you deployed earlier created a Private Hosted Zone for s3.us-east-1.amazonaws.com. This zone:\nIs associated only with your VPCs (not public) Overrides public DNS resolution for S3 in your VPCs Allows you to return private IPs instead of public IPs for S3 domains Integrates with Route 53 Resolver for hybrid DNS Step 1: Access Route 53 Hosted Zones\nOpen the Route 53 Management Console (Hosted Zones section) You should see a Private Hosted Zone named s3.us-east-1.amazonaws.com Click on the hosted zone name to view its DNS records Private Hosted Zone Indicator: You\u0026rsquo;ll see \u0026ldquo;Private\u0026rdquo; listed under the Type column, and the Associated VPCs will show which VPCs use this zone for DNS resolution. This ensures only resources in your VPCs receive the private endpoint IPs.\nStep 2: Create Primary Alias Record\nYou\u0026rsquo;ll create an Alias record that points the base S3 domain to your Interface Endpoint.\nInitiate record creation:\nClick the Create record button at the top Configure the first record:\nRecord name: Leave blank (this creates a record for the zone apex: s3.us-east-1.amazonaws.com)\nRecord type: Select A ‚Äì IPv4 address (default)\nAlias configuration:\nToggle the Alias switch to ON (enabled) This tells Route 53 to alias to another AWS resource Route traffic to:\nChoose Alias to VPC endpoint from the dropdown Region:\nSelect US East (N. Virginia) [us-east-1] Choose endpoint:\nPaste the Regional VPC Endpoint DNS name from your text editor This is the DNS name you saved in section 5.4.3 Format: vpce-xxxxxxxxx.s3.us-east-1.vpce.amazonaws.com Alias vs CNAME: Using Alias records (instead of CNAME) provides benefits:\nWorks at the zone apex (bare domain without subdomain) No charge for Alias queries to AWS resources Better performance with AWS service integration Automatic health checking and failover support Step 3: Create Wildcard Alias Record\nNow create a second record to handle all subdomains of the S3 service (e.g., bucket.s3.us-east-1.amazonaws.com).\nAdd another record:\nClick Add another record button (do not click Create yet) Configure the wildcard record:\nRecord name: Enter * (asterisk - this creates a wildcard)\nRecord type: Keep A ‚Äì IPv4 address\nAlias configuration:\nToggle Alias to ON Route traffic to:\nChoose Alias to VPC endpoint Region:\nSelect US East (N. Virginia) [us-east-1] Choose endpoint:\nPaste the same Regional VPC Endpoint DNS name Create both records:\nClick Create records button Both records will be created simultaneously Why Two Records?\nThe two DNS records serve different purposes:\nApex Record (s3.us-east-1.amazonaws.com):\nHandles queries to the base S3 regional endpoint Used by commands like: aws s3 ls --endpoint-url https://s3.us-east-1.amazonaws.com Wildcard Record (*.s3.us-east-1.amazonaws.com):\nHandles queries with bucket names in the subdomain Matches: bucket.s3.us-east-1.amazonaws.com, my-bucket.s3.us-east-1.amazonaws.com, etc. Used by path-style S3 URLs Step 4: Verify Record Creation\nAfter creation, you should see both records in the hosted zone:\nVerify:\nTwo new A records (apex and wildcard) Both show Type: A - Alias Value points to your VPC endpoint DNS Records Configured! These records ensure that when systems query for S3 domain names, they receive the private IP addresses of your Interface Endpoint ENIs instead of public S3 IPs.\nPart 2: Create Route 53 Resolver Forwarding Rule Now you\u0026rsquo;ll configure conditional DNS forwarding from the on-premises VPC to the cloud VPC. This simulates how on-premises DNS servers forward specific domains to AWS for resolution.\nUnderstanding Resolver Forwarding Rules:\nRoute 53 Resolver Forwarding Rules enable:\nConditional forwarding: Only specific domains are forwarded (e.g., s3.us-east-1.amazonaws.com) Hybrid DNS: On-premises systems query their local DNS, which forwards to AWS Centralized management: Rules define which domains resolve through AWS Bidirectional resolution: Can forward both from AWS to on-prem and vice versa In this workshop:\nOn-prem EC2 queries the Outbound Resolver in VPC On-prem Outbound Resolver forwards S3 queries through VPN to Inbound Resolver in VPC Cloud Inbound Resolver queries the Private Hosted Zone Response with private IPs travels back to on-prem Step 1: Retrieve Inbound Resolver IP Addresses\nFirst, you need the IP addresses where the Inbound Resolver is listening in VPC Cloud.\nNavigate to Inbound Endpoints:\nFrom Route 53 console, click Resolver in left sidebar Click Inbound endpoints Access endpoint details:\nClick on the Endpoint ID of the inbound endpoint CloudFormation created this with a name like VPCCloudInboundEndpoint Copy IP addresses: In the endpoint details, scroll to the IP addresses section You\u0026rsquo;ll see 2 IP addresses (one per AZ) Copy both IPs to your text editor Format: 10.0.x.x (within VPC Cloud CIDR) Multi-AZ Resolver: The Inbound Resolver has IPs in multiple AZs for high availability. The forwarding rule will use both, automatically failing over if one AZ becomes unavailable.\nStep 2: Create Forwarding Rule\nNavigate to Rules: In Route 53 console, click Resolver \u0026gt; Rules in left sidebar Click Create rule button Configure rule basics:\nName: Enter S3-PrivateEndpoint-ForwardingRule Description (optional): \u0026ldquo;Forward S3 DNS queries to VPC Cloud for private endpoint resolution\u0026rdquo; Rule type: Select Forward (not System) Domain name: Enter s3.us-east-1.amazonaws.com This domain name tells the rule: \u0026ldquo;Forward any DNS queries for this domain and its subdomains.\u0026rdquo;\nStep 3: Associate VPC and Outbound Endpoint\nVPC Configuration:\nVPCs to associate this rule with:\nSelect VPC On-prem from the dropdown This applies the rule to DNS queries originating in the on-premises VPC Outbound endpoint:\nSelect VPCOnpremOutboundEndpoint This is the resolver endpoint that forwards queries from VPC On-prem Outbound Endpoint Role: The Outbound Resolver in VPC On-prem receives DNS queries from EC2 instances and forwards them through the VPN tunnel to the target IP addresses (the Inbound Resolver in VPC Cloud).\nStep 4: Specify Target IP Addresses\nAdd target IPs:\nTarget IP addresses: This is where forwarded queries will be sent Click Add target IP address IP address: Paste the first Inbound Resolver IP from your text editor Click Add target IP address again IP address: Paste the second Inbound Resolver IP Both IPs should now be listed Create the rule:\nClick Submit button Understanding the Target IPs:\nThese are the Inbound Resolver IPs in VPC Cloud:\nThey listen for DNS queries coming through the VPN They query the Private Hosted Zone for answers They return results back through the VPN to the Outbound Resolver Step 5: Verify Rule Creation\nAfter creation, you should see your new forwarding rule:\nCheck that:\nStatus shows Active Type shows Forward Domain name is s3.us-east-1.amazonaws.com VPC shows VPC On-prem DNS Forwarding Configured! Queries from VPC On-prem for s3.us-east-1.amazonaws.com now forward through the Outbound Resolver, across the VPN tunnel, to the Inbound Resolver, and finally resolve through the Private Hosted Zone.\nPart 3: Test On-Premises DNS Resolution Now validate that the entire DNS resolution chain is working correctly.\nStep 1: Connect to On-Premises Instance\nOpen AWS Systems Manager \u0026gt; Session Manager Click Start session Select the Test-Interface-Endpoint EC2 instance (in VPC On-prem) Click Start session Step 2: Test DNS Resolution with Dig Command\nIn the Session Manager terminal, run:\ndig +short s3.us-east-1.amazonaws.com Expected Output:\nYou should see 2 IP addresses returned:\n10.0.1.100 10.0.2.100 (Your actual IPs will differ but should be in the VPC Cloud CIDR range)\nImportant Distinction: The IP addresses returned are the VPC Interface Endpoint ENI IPs, NOT the Inbound Resolver IPs you configured. Both sets of IPs are in the VPC Cloud CIDR block, but they serve different purposes:\nInbound Resolver IPs: Handle DNS queries (not returned in dig results) Interface Endpoint IPs: Where S3 API traffic is sent (returned by DNS queries) What Just Happened:\nThe dig command triggered this DNS resolution flow:\nEC2 On-prem ‚Üí Outbound Resolver (VPC On-prem) ‚Üí VPN Tunnel ‚Üí Inbound Resolver (VPC Cloud) ‚Üí Private Hosted Zone ‚Üí Alias Record ‚Üí Interface Endpoint IPs ‚Üí Response back to EC2 Step 3: Verify IPs Match Interface Endpoint\nCross-check the IPs returned by dig against the actual Interface Endpoint:\nOpen VPC Console:\nNavigate to VPC \u0026gt; Endpoints Click on your S3 Interface Endpoint Check Subnets tab:\nClick the Subnets tab You\u0026rsquo;ll see ENI IDs and their associated private IPs Verify these IPs match those returned by the dig command The IPs should match perfectly, confirming DNS resolution is working correctly.\nDNS Resolution Validated! Your on-premises environment is successfully resolving S3 domain names to private Interface Endpoint IPs through the hybrid DNS architecture.\nStep 4: Test S3 API Access via DNS\nNow test that applications can use the standard S3 endpoint URL and automatically connect through the private endpoint.\nIn the Session Manager terminal, run:\naws s3 ls --endpoint-url https://s3.us-east-1.amazonaws.com Expected Output:\nYou should see a list of your S3 buckets, including the test bucket created earlier.\nKey Observation:\nNotice you\u0026rsquo;re now using the standard S3 endpoint URL (https://s3.us-east-1.amazonaws.com), not the VPC endpoint-specific URL you used in section 5.4.3. This is possible because:\nDNS resolves s3.us-east-1.amazonaws.com to Interface Endpoint private IPs HTTPS traffic routes through VPN to the Interface Endpoint Applications don\u0026rsquo;t need to be aware of the private endpoint configuration Standard AWS SDKs and CLI work without modification This demonstrates the power of DNS-based private endpoint resolution!\nProduction Benefit: In enterprise environments, applications can use standard AWS service endpoints in their code. The DNS infrastructure automatically directs traffic through private endpoints when running on-premises or in VPCs, and through internet when running elsewhere (with appropriate routing).\nStep 5: Clean Up Session\nTerminate your Session Manager session by typing:\nexit Or close the browser tab\nUnderstanding Your Hybrid DNS Architecture Complete Traffic Flow:\nWhen an on-premises application accesses S3, this is the complete flow:\nDNS Resolution Path:\n‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îÇ On-Prem Application ‚îÇ ‚îÇ Queries: s3.us-... ‚îÇ ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îÇ ‚ñº ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îÇ EC2 Instance ‚îÇ ‚îÇ DNS Resolver Config ‚îÇ ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îÇ ‚ñº ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îÇ Outbound Resolver ‚îÇ ‚îÇ (VPC On-prem) ‚îÇ ‚îÇ Forwarding Rule: ‚îÇ ‚îÇ s3.us-east-1.* ‚îÇ ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îÇ ‚îÇ VPN Tunnel ‚ñº ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îÇ Inbound Resolver ‚îÇ ‚îÇ (VPC Cloud) ‚îÇ ‚îÇ IPs: 10.0.x.x ‚îÇ ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îÇ ‚ñº ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îÇ Private Hosted Zone ‚îÇ ‚îÇ s3.us-east-1.*.com ‚îÇ ‚îÇ Alias Records ‚îÇ ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îÇ ‚ñº ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îÇ Returns: 10.0.1.100 ‚îÇ ‚îÇ 10.0.2.100 ‚îÇ ‚îÇ (Endpoint ENI IPs) ‚îÇ ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò Data Path (After DNS Resolution):\nOn-Prem Application ‚Üí Interface Endpoint Private IP ‚Üí VPN Tunnel ‚Üí VPC Cloud ‚Üí Interface Endpoint ENI ‚Üí AWS PrivateLink ‚Üí S3 Service Key Accomplishments What You\u0026rsquo;ve Built:\n‚úÖ DNS Alias Records: Mapped S3 domains to Interface Endpoint private IPs\n‚úÖ Conditional Forwarding: Configured forwarding rule for S3 domain from on-prem to cloud\n‚úÖ Hybrid DNS Resolution: Enabled seamless DNS resolution across VPN\n‚úÖ Transparent S3 Access: Applications use standard S3 endpoints with private routing\n‚úÖ High Availability: Multi-AZ DNS and endpoint configuration for fault tolerance\nEnterprise Architecture Patterns:\nConditional DNS Forwarding: Split-brain DNS with domain-specific forwarding Private Hosted Zones: Override public DNS for AWS services with private IPs Alias Records: Efficient DNS routing to AWS resources Resolver Endpoints: Hybrid DNS bridge between on-prem and cloud Multi-AZ Resilience: Automatic failover for DNS and endpoints Production Enhancements:\nFor real-world deployments, consider:\nDNS caching strategies to reduce resolver query load Monitoring CloudWatch metrics for Resolver query counts and latency Multiple forwarding rules for different AWS services (EC2, RDS, etc.) DNS failback to public resolution if private endpoints are unavailable Integration with on-premises DNS servers (BIND, Active Directory, etc.) DNS query logging for security auditing and troubleshooting Route 53 Resolver Query Logging enabled for compliance This hybrid DNS architecture provides enterprise-grade private connectivity to AWS services while maintaining standard application code and AWS SDK usage!\n"},{"uri":"https://thienluhoan.github.io/workshop-template/4-eventparticipated/4.4-event4/","title":"Event 4","tags":[],"description":"","content":" ‚ö†Ô∏è Note: The information below is for reference purposes only. Please do not copy it verbatim into your report, including this warning.\nSummary Report: \u0026ldquo;AWS Well-Architected Framework Workshop\u0026rdquo; Date \u0026amp; Time: Saturday, November 29, 2025, 8:30 AM ‚Äì 12:00 PM\nLocation: FCJ Learning Center, Ho Chi Minh City\nRole: Workshop Participant\nOrganizer: First Cloud Journey, AWS User Group Vietnam\nEvent Purpose This workshop provided comprehensive overview of the AWS Well-Architected Framework, focusing on the five pillars: Operational Excellence, Security, Reliability, Performance Efficiency, and Cost Optimization. The session helped participants understand how to design and evaluate cloud architectures using AWS best practices.\nAgenda Overview 8:30 ‚Äì 8:50 AM | Introduction \u0026amp; Framework Overview\nWelcome and workshop objectives Introduction to Well-Architected Framework Overview of the five pillars Benefits of using the framework 8:50 ‚Äì 9:30 AM | Operational Excellence Pillar\nKey Concepts:\nInfrastructure as Code (IaC) with CloudFormation and CDK Continuous improvement and experimentation Learning from operational events Monitoring and observability Best Practices:\nAutomate deployment processes Make frequent, small, reversible changes Anticipate failure scenarios Document standard operating procedures 9:30 ‚Äì 10:00 AM | Security Pillar\nSecurity Fundamentals:\nIdentity and Access Management (IAM) Detection and response mechanisms Data protection strategies Incident response planning Key Services:\nAWS IAM, Security Hub, GuardDuty AWS KMS for encryption CloudTrail for auditing AWS Config for compliance 10:00 ‚Äì 10:15 AM | Coffee Break\n10:15 ‚Äì 10:45 AM | Reliability Pillar\nReliability Concepts:\nHigh availability architecture patterns Fault tolerance and disaster recovery Auto Scaling and load balancing Backup and restore strategies Implementation:\nMulti-AZ deployments Cross-region replication Health checks and monitoring Automated recovery procedures 10:45 ‚Äì 11:15 AM | Performance Efficiency Pillar\nPerformance Optimization:\nRight-sizing resources Choosing appropriate services Monitoring performance metrics Using caching strategies AWS Services:\nAmazon CloudFront for content delivery ElastiCache for caching Lambda for serverless computing Auto Scaling for dynamic workloads 11:15 ‚Äì 11:45 AM | Cost Optimization Pillar\nCost Management:\nUnderstanding AWS pricing models Right-sizing and instance selection Using reserved instances and savings plans Monitoring and controlling costs Tools and Practices:\nAWS Cost Explorer AWS Budgets and alerts Tagging strategies S3 storage class optimization 11:45 ‚Äì 12:00 PM | Wrap-up \u0026amp; Q\u0026amp;A\nSummary of key takeaways Well-Architected Tool demonstration Resources for further learning Certificate distribution Key Highlights Holistic Approach: Framework covers all aspects of cloud architecture Best Practices: Based on AWS experience with thousands of customers Trade-offs: Understanding when to prioritize one pillar over another Continuous Improvement: Regular reviews and updates are essential Well-Architected Tool: Free tool to evaluate your architectures Key Learnings Framework Application:\nThe five pillars are interconnected and should be considered together Trade-offs are inevitable - understanding them is key Regular architecture reviews help identify improvement opportunities The framework applies to workloads of all sizes Operational Excellence:\nAutomation reduces human error and increases efficiency Learning from failures improves operations over time Documentation and runbooks are crucial Security:\nSecurity should be built into every layer Principle of least privilege for access control Continuous monitoring is essential Reliability:\nDesign for failure and nothing fails Testing recovery procedures is as important as having them Distribute workloads across multiple availability zones Performance Efficiency:\nMonitor and measure to understand performance Choose the right service for the workload Serverless can simplify architecture Cost Optimization:\nMeasure and analyze spending regularly Use the right resources for the job Take advantage of AWS pricing models Application to Internship Immediate Applications:\nReview existing projects using Well-Architected principles Implement IaC for infrastructure deployment Set up cost monitoring and budgets Improve security posture with IAM best practices Architecture Design:\nApply the framework when designing new solutions Document architectural decisions and trade-offs Consider all five pillars in design discussions Use the Well-Architected Tool for self-assessment Best Practices:\nImplement infrastructure as code Enable comprehensive monitoring Set up automated backups Apply proper tagging for cost allocation Personal Experience The AWS Well-Architected Framework Workshop provided valuable architectural guidance.\nUnderstanding the Framework:\nLearned that good architecture balances all five pillars Understanding trade-offs helps make better decisions The framework provides structure for architectural discussions Real-world examples made concepts clearer Practical Insights:\nEach pillar has specific best practices and design patterns The Well-Architected Tool can help assess current architectures Regular reviews help maintain and improve quality Documentation is crucial for operational success Multi-Pillar Thinking:\nRealized that optimizing for one pillar may impact others Security and cost often require careful balancing Performance improvements may increase costs Understanding these relationships is important Architecture Review Process:\nLearned how to conduct architecture reviews Understanding of key questions for each pillar Importance of documenting decisions Value of continuous improvement Tools and Resources:\nAWS Well-Architected Tool for assessments Whitepapers and documentation for each pillar Reference architectures and patterns Community resources and workshops Challenges:\nBalancing all five pillars can be complex Understanding when to prioritize which pillar Implementing best practices requires time and effort Keeping up with AWS service updates Event Photos Workshop photos will be added here\nOverall, this workshop provided a comprehensive framework for thinking about cloud architecture. The five pillars offer structured approach to designing, building, and maintaining AWS workloads. Understanding these principles will help in making better architectural decisions for future projects.\n"},{"uri":"https://thienluhoan.github.io/workshop-template/4-eventparticipated/","title":"Events Participated","tags":[],"description":"","content":" ‚ö†Ô∏è Note: The information below is for reference purposes only. Please do not copy verbatim for your report, including this warning.\nAWS Events Participated During Internship Throughout my internship at AWS, I actively participated in various technical events, workshops, and community gatherings to expand my knowledge and network with industry professionals.\nOverview Participating in AWS events has been an integral part of my professional development journey. These experiences provided opportunities to:\nLearn about the latest AWS services and features directly from AWS experts Network with cloud professionals and fellow interns Gain hands-on experience through workshops and bootcamps Understand real-world use cases and best practices Stay updated with industry trends and innovations Content This section documents the key events I participated in during my internship:\nEvent 1 Event 2 Event 3 Event 4 Each event provided unique learning opportunities and contributed to my overall growth as a cloud computing professional.\n"},{"uri":"https://thienluhoan.github.io/workshop-template/1-worklog/1.4-week4/","title":"Week 4 Worklog","tags":[],"description":"","content":" ‚ö†Ô∏è Note: The following information is for reference purposes only. Please do not copy verbatim for your own report, including this warning.\nWeek 4 Objectives: Learn about AWS Lambda and serverless computing. Understand API Gateway for building REST APIs. Study SNS and SQS for messaging services. Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 2 - Learn Migration concepts (Lift \u0026amp; Shift, Replatform, Refactor) - Introduction to AWS Database Migration Service (DMS) 29/09/2025 29/09/2025 AWS Journey 3 - Practice creating Replication Instance in DMS - Configure source data (on-premise) and target (RDS) - Perform test data migration 30/09/2025 30/09/2025 AWS Journey 4 - Introduction to Elastic Disaster Recovery (EDR) - Learn how to set up replication server and recovery instance 01/10/2025 01/10/2025 AWS Journey 5 - Practice simulating failures: shut down main EC2 and start recovery instance from EDR - Evaluate recovery time (RTO/RPO) 02/10/2025 02/10/2025 AWS Journey 6 - Create basic Disaster Recovery plan (backup, restore, failover) - Write documentation summarizing Migration + DR processes - Summarize Week 4 knowledge 03/10/2025 03/10/2025 AWS Journey Week 4 Achievements: Learned about serverless computing and benefits:\nNo server management required Pay only for execution time Automatic scaling Created Lambda functions using Python:\nBasic hello world function Function with environment variables Tested with different event sources Configured Lambda settings:\nTimeout (default 3 seconds) Memory allocation Execution role with basic permissions Built REST API with API Gateway:\nCreated simple GET/POST endpoints Connected API to Lambda backend Tested API calls using browser and Postman Practiced with SNS:\nCreated SNS topic Added email subscription Sent test notifications Worked with SQS:\nCreated standard queue Sent messages to queue Retrieved and deleted messages from queue Understood message retention "},{"uri":"https://thienluhoan.github.io/workshop-template/5-workshop/5.5-policy/","title":"Implementing Fine-Grained Access Control with VPC Endpoint Policies","tags":[],"description":"","content":" ‚ö†Ô∏è Important: The information provided here is for reference purposes only. Please do not copy verbatim for your report including this warning message.\nIn this section, you will implement security best practices by applying restrictive VPC endpoint policies to control which S3 buckets can be accessed through your Gateway VPC Endpoint. This demonstrates the principle of least privilege in cloud architecture.\nUnderstanding VPC Endpoint Policies:\nVPC endpoint policies are IAM resource policies that provide granular access control for traffic flowing through VPC endpoints. They work as an additional security layer:\nDefault Behavior: When you create an endpoint without a custom policy, AWS applies a permissive default policy allowing full access to all resources of the target service Policy Type: JSON-based IAM resource policies attached directly to the endpoint Enforcement Point: Evaluated when traffic passes through the endpoint (not on the AWS service side) Combined Evaluation: Works in conjunction with IAM user/role policies and S3 bucket policies (all must allow an action) Scope: Can restrict access by resource (specific buckets), principal (specific IAM identities), or action (specific S3 operations) Why Use Endpoint Policies?\nEndpoint policies are critical for enterprise security architectures:\nDefense in Depth: Additional security layer beyond IAM and resource policies Network-Level Control: Restrict what can be accessed through specific network paths Compliance Requirements: Meet regulatory requirements for data access controls Prevent Data Exfiltration: Limit which buckets can be accessed from VPC resources Cost Optimization: Restrict access to authorized resources only, preventing misuse Workshop Scenario:\nYou\u0026rsquo;ll create a restrictive endpoint policy that:\nAllows access to a specific S3 bucket only Blocks access to all other S3 buckets Demonstrates how endpoint policies enforce access controls Part 1: Establish Baseline Connectivity Before applying a restrictive policy, verify current access to demonstrate the before/after comparison.\nStep 1: Connect to Test Instance\nOpen AWS Systems Manager \u0026gt; Session Manager Click Start session Select the EC2 instance named Test-Gateway-Endpoint This instance is in the VPC with the Gateway Endpoint It routes S3 traffic through the Gateway Endpoint via route table configuration Click Start session Step 2: Verify Access to Original S3 Bucket\nIn the Session Manager terminal, list the contents of the bucket you created in section 5.3:\naws s3 ls s3://\u0026lt;your-bucket-name\u0026gt; Replace \u0026lt;your-bucket-name\u0026gt; with your actual bucket name (e.g., my-endpoint-test-bucket-123456).\nExpected Output:\nYou should see the two 1GB test files uploaded in earlier sections:\n2024-12-07 10:30:15 1073741824 testfile.txt 2024-12-07 11:45:22 1073741824 onprem-data.dat Current Access: This works because the Gateway Endpoint currently has the default policy allowing full access (\u0026quot;*\u0026quot;) to all S3 resources. You\u0026rsquo;re about to change this to demonstrate policy-based restrictions.\nPart 2: Create Second S3 Bucket for Policy Testing You\u0026rsquo;ll create a second bucket to demonstrate selective access control.\nStep 1: Navigate to S3 Console\nOpen the Amazon S3 Console Click Create bucket Step 2: Configure New Bucket\nBucket name:\nFollow your existing naming pattern, but append -restricted Example: If your first bucket is my-endpoint-test-bucket-123456, name this one my-endpoint-test-bucket-123456-restricted Must be globally unique across all AWS accounts Region:\nEnsure it\u0026rsquo;s US East (N. Virginia) us-east-1 (same as your VPC and first bucket) Other settings:\nLeave all other settings at default values Default encryption: Enabled (SSE-S3) Block Public Access: Enabled (recommended) Versioning: Disabled (for workshop simplicity) Create bucket:\nScroll to bottom and click Create bucket Step 3: Verify Bucket Creation\nYou should see a success message and the new bucket listed in the S3 console.\nNaming Strategy: Using a consistent naming pattern (original name + suffix) makes it easy to identify related resources and simplifies policy ARN construction. In production, use naming conventions that reflect environment, application, and purpose.\nPart 3: Apply Restrictive Endpoint Policy Now you\u0026rsquo;ll modify the Gateway Endpoint policy to allow access only to the new bucket.\nStep 1: Navigate to VPC Endpoints\nOpen the VPC Console Click Endpoints in the left sidebar Locate and select the Gateway Endpoint you created in section 5.3 Type should show \u0026ldquo;Gateway\u0026rdquo; Service name should be com.amazonaws.us-east-1.s3 Step 2: Access Endpoint Policy Editor\nWith the endpoint selected, click the Policy tab in the bottom details pane You\u0026rsquo;ll see the current policy (default allows all) Click Edit policy button Current Default Policy:\nThe existing policy looks like this (allowing unrestricted access):\n{ \u0026#34;Statement\u0026#34;: [ { \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Principal\u0026#34;: \u0026#34;*\u0026#34;, \u0026#34;Action\u0026#34;: \u0026#34;*\u0026#34;, \u0026#34;Resource\u0026#34;: \u0026#34;*\u0026#34; } ] } Step 3: Apply Restrictive Policy\nClear the existing policy in the policy editor\nCopy and paste the following restrictive policy:\n{ \u0026#34;Version\u0026#34;: \u0026#34;2012-10-17\u0026#34;, \u0026#34;Id\u0026#34;: \u0026#34;RestrictToSpecificBucketPolicy\u0026#34;, \u0026#34;Statement\u0026#34;: [ { \u0026#34;Sid\u0026#34;: \u0026#34;AllowAccessToRestrictedBucketOnly\u0026#34;, \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Principal\u0026#34;: \u0026#34;*\u0026#34;, \u0026#34;Action\u0026#34;: \u0026#34;s3:*\u0026#34;, \u0026#34;Resource\u0026#34;: [ \u0026#34;arn:aws:s3:::YOUR-RESTRICTED-BUCKET-NAME\u0026#34;, \u0026#34;arn:aws:s3:::YOUR-RESTRICTED-BUCKET-NAME/*\u0026#34; ] } ] } Replace the ARN placeholders:\nFind YOUR-RESTRICTED-BUCKET-NAME (appears twice) Replace with your actual second bucket name (the one ending in -restricted) Example result: \u0026#34;Resource\u0026#34;: [ \u0026#34;arn:aws:s3:::my-endpoint-test-bucket-123456-restricted\u0026#34;, \u0026#34;arn:aws:s3:::my-endpoint-test-bucket-123456-restricted/*\u0026#34; ] Save the policy:\nClick Save button Understanding the Policy:\nLet\u0026rsquo;s break down each component:\nVersion: IAM policy language version (always use \u0026ldquo;2012-10-17\u0026rdquo;) Id: Optional identifier for the policy Statement: Array of policy statements (can have multiple) Sid: Statement ID (descriptive label) Effect: Allow (grants permissions) or Deny (explicitly denies) Principal: \u0026quot;*\u0026quot; means any IAM identity (still subject to IAM policies) Action: s3:* allows all S3 operations (GetObject, PutObject, ListBucket, etc.) Resource: Two ARNs: First ARN: The bucket itself (for operations like ListBucket) Second ARN with /*: All objects within the bucket (for GetObject, PutObject, etc.) Policy Applied! The Gateway Endpoint now enforces that only the restricted bucket can be accessed through it. All attempts to access other buckets (including your original bucket) will be denied.\nPolicy Evaluation Logic:\nWhen an EC2 instance tries to access S3 through the endpoint:\n1. IAM policy on EC2 instance role: Must Allow 2. VPC Endpoint policy: Must Allow ‚Üê We just restricted this 3. S3 bucket policy: Must Allow (or not explicitly Deny) 4. S3 ACLs: Must Allow (or not explicitly Deny) Only if ALL checks pass ‚Üí Access granted Part 4: Test Policy Enforcement Now validate that the endpoint policy is enforcing access restrictions.\nStep 1: Test Access to Original Bucket (Should Fail)\nIn your still-open Session Manager terminal on Test-Gateway-Endpoint, try to list the original bucket:\naws s3 ls s3://\u0026lt;your-original-bucket-name\u0026gt; Expected Output:\nYou should receive an Access Denied error:\nAn error occurred (AccessDenied) when calling the ListObjectsV2 operation: Access Denied Why Access Denied? The VPC endpoint policy no longer includes the original bucket in its allowed resources. Even though your EC2 instance IAM role has S3 permissions, the network path (through the endpoint) blocks access.\nStep 2: Prepare Test Data\nCreate a test file to upload to the restricted bucket:\nNavigate to home directory:\ncd ~ Create a 1GB test file:\nfallocate -l 1G policy-test-file.dat Step 3: Test Access to Restricted Bucket (Should Succeed)\nUpload the file to the restricted bucket:\naws s3 cp policy-test-file.dat s3://\u0026lt;your-restricted-bucket-name\u0026gt; Replace \u0026lt;your-restricted-bucket-name\u0026gt; with your second bucket name.\nExpected Output:\nThe upload should succeed:\nupload: ./policy-test-file.dat to s3://my-bucket-restricted/policy-test-file.dat Completed 1.0 GiB/1.0 GiB Step 4: Verify File Upload in S3 Console\nGo back to the S3 Console Click on your restricted bucket You should see the policy-test-file.dat object Policy Enforcement Confirmed! The endpoint policy successfully:\n‚úÖ Allowed access to the restricted bucket ‚ùå Blocked access to the original bucket This demonstrates fine-grained network-level access control.\nStep 5: Confirm Original Bucket Is Still Blocked\nAs a final test, try to upload to the original bucket (should fail):\naws s3 cp policy-test-file.dat s3://\u0026lt;your-original-bucket-name\u0026gt; Expected Output:\nAccess denied error:\nupload failed: ./policy-test-file.dat to s3://my-original-bucket/policy-test-file.dat An error occurred (AccessDenied) when calling the PutObject operation: Access Denied This confirms the endpoint policy is consistently enforcing restrictions.\nUnderstanding Policy Behavior Access Patterns Summary:\nOperation Target Result Reason List objects Original bucket ‚ùå Denied Not in endpoint policy Resource list Upload file Original bucket ‚ùå Denied Not in endpoint policy Resource list List objects Restricted bucket ‚úÖ Allowed Explicitly allowed in endpoint policy Upload file Restricted bucket ‚úÖ Allowed Explicitly allowed in endpoint policy How Traffic Flows:\nWhen EC2 accesses S3 through the Gateway Endpoint with a restrictive policy:\n‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îÇ EC2 Instance ‚îÇ ‚îÇ Test-Gateway-EP ‚îÇ ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îÇ ‚îÇ S3 API Request ‚ñº ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îÇ Route Table ‚îÇ ‚îÇ Directs S3 traffic ‚îÇ ‚îÇ to Gateway Endpoint‚îÇ ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îÇ ‚ñº ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îÇ Gateway VPC Endpoint ‚îÇ ‚îÇ Evaluates Endpoint Policy ‚îÇ ‚îÇ ‚îÇ ‚îÇ IF bucket IN allowed list: ‚îÇ ‚îÇ ‚Üí Forward to S3 ‚îÇ ‚îÇ ELSE: ‚îÇ ‚îÇ ‚Üí Return Access Denied ‚îÇ ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îÇ ‚îÇ (If allowed) ‚ñº ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îÇ Amazon S3 Service ‚îÇ ‚îÇ (Backend) ‚îÇ ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò Key Observations:\nNetwork-Level Enforcement: The endpoint policy blocks traffic before it even reaches S3 Independent of IAM: Even if IAM policies allow access, endpoint policy can block it All-or-Nothing per Bucket: You cannot selectively allow only certain operations (e.g., read-only); the policy applies to all allowed actions Principal Agnostic: With \u0026quot;Principal\u0026quot;: \u0026quot;*\u0026quot;, the restriction applies to all identities using the endpoint Production Use Cases for Endpoint Policies Enterprise Scenarios:\nData Residency Compliance:\n{ \u0026#34;Statement\u0026#34;: [{ \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Principal\u0026#34;: \u0026#34;*\u0026#34;, \u0026#34;Action\u0026#34;: \u0026#34;s3:*\u0026#34;, \u0026#34;Resource\u0026#34;: \u0026#34;arn:aws:s3:::eu-compliant-data-bucket/*\u0026#34;, \u0026#34;Condition\u0026#34;: { \u0026#34;StringEquals\u0026#34;: { \u0026#34;s3:ExistingObjectTag/DataClassification\u0026#34;: \u0026#34;EUPersonalData\u0026#34; } } }] } Prevent Data Exfiltration:\nWhitelist only approved corporate buckets Block access to personal or external buckets Audit endpoint policy changes with CloudTrail Multi-Tenant Environments:\n{ \u0026#34;Statement\u0026#34;: [{ \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Principal\u0026#34;: \u0026#34;*\u0026#34;, \u0026#34;Action\u0026#34;: \u0026#34;s3:*\u0026#34;, \u0026#34;Resource\u0026#34;: [ \u0026#34;arn:aws:s3:::tenant-a-data/*\u0026#34;, \u0026#34;arn:aws:s3:::shared-resources/*\u0026#34; ] }] } Development vs Production Isolation:\nDev VPC endpoint: Access only to dev/test buckets Prod VPC endpoint: Access only to production buckets Prevents accidental cross-environment access Advanced Policy Patterns:\nCondition-Based Access:\n{ \u0026#34;Statement\u0026#34;: [{ \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Principal\u0026#34;: \u0026#34;*\u0026#34;, \u0026#34;Action\u0026#34;: [\u0026#34;s3:GetObject\u0026#34;, \u0026#34;s3:PutObject\u0026#34;], \u0026#34;Resource\u0026#34;: \u0026#34;arn:aws:s3:::sensitive-bucket/*\u0026#34;, \u0026#34;Condition\u0026#34;: { \u0026#34;StringEquals\u0026#34;: { \u0026#34;aws:SourceVpce\u0026#34;: \u0026#34;vpce-xxxxxxxx\u0026#34; } } }] } Read-Only Access:\n{ \u0026#34;Statement\u0026#34;: [{ \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Principal\u0026#34;: \u0026#34;*\u0026#34;, \u0026#34;Action\u0026#34;: [ \u0026#34;s3:GetObject\u0026#34;, \u0026#34;s3:ListBucket\u0026#34; ], \u0026#34;Resource\u0026#34;: [ \u0026#34;arn:aws:s3:::readonly-bucket\u0026#34;, \u0026#34;arn:aws:s3:::readonly-bucket/*\u0026#34; ] }] } Key Takeaways What You Accomplished:\n‚úÖ Baseline Testing: Verified unrestricted access with default endpoint policy\n‚úÖ Bucket Creation: Created second S3 bucket for policy testing\n‚úÖ Policy Application: Applied restrictive endpoint policy limiting access to specific bucket\n‚úÖ Enforcement Validation: Confirmed policy blocks unauthorized buckets and allows authorized bucket\n‚úÖ Security Best Practice: Demonstrated principle of least privilege at network level\nSecurity Principles Demonstrated:\nDefense in Depth: Endpoint policies add network-level control layer Least Privilege: Grant only necessary access, not broad permissions Explicit Allow: Whitelist approach (specify what\u0026rsquo;s allowed, block everything else) Network Segmentation: Control data flow at VPC endpoint level Auditability: Policy changes logged in CloudTrail for compliance Production Recommendations:\nVersion Control: Store endpoint policies in Git with code review process Policy Testing: Test policies in dev/staging before production deployment Monitoring: Set up CloudWatch alarms for AccessDenied errors indicating policy violations Documentation: Maintain clear documentation of which buckets are accessible through which endpoints Regular Review: Audit endpoint policies quarterly to remove unnecessary access Condition Keys: Use advanced IAM condition keys for time-based, IP-based, or tag-based restrictions Deny Statements: Consider explicit Deny statements for high-security requirements This section demonstrated how VPC endpoint policies provide critical access control for enterprise cloud architectures, enabling you to enforce organizational security policies at the network infrastructure level!\n"},{"uri":"https://thienluhoan.github.io/workshop-template/1-worklog/1.5-week5/","title":"Week 5 Worklog","tags":[],"description":"","content":" ‚ö†Ô∏è Note: The following information is for reference purposes only. Please do not copy verbatim for your own report, including this warning.\nWeek 5 Objectives: Learn about container services on AWS. Understand ECS and basic Docker concepts. Study Load Balancer for distributing traffic. Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 2 - Introduction to Infrastructure as Code (IaC) concepts and benefits compared to manual deployment - Get familiar with AWS CloudFormation: template, stack, parameter 06/10/2025 06/10/2025 AWS Journey 3 - Write CloudFormation template to deploy S3 bucket and EC2 instance - Create, update, and delete stack via AWS Console 07/10/2025 07/10/2025 AWS Journey 4 - Introduction to AWS CDK (Cloud Development Kit) - Install AWS CDK, create CDK project using Python or TypeScript - Write CDK code to deploy EC2 instance 08/10/2025 08/10/2025 AWS Journey 5 - Introduction to AWS Systems Manager (SSM) and key features: Parameter Store, Run Command, Automation, Session Manager - Create Parameter Store to store configuration variables 09/10/2025 09/10/2025 AWS Journey 6 - Practice creating Automation Document in SSM to automatically start/stop EC2 - Test Session Manager (access EC2 without SSH key) - Week summary: IaC + SSM demo 10/10/2025 10/10/2025 AWS Journey Week 5 Achievements: Learned about containerization and Docker:\nUnderstood benefits of containers Learned difference between containers and VMs Studied Docker images and containers Practiced with Docker locally:\nInstalled Docker Desktop Pulled images from Docker Hub Ran basic containers: docker pull nginx docker run -d -p 80:80 nginx Learned basic Docker commands Understood Amazon ECS:\nLearned ECS as managed container service Studied ECS components: Clusters Task definitions Services Compared Fargate (serverless) vs EC2 launch types Deployed application on ECS:\nCreated ECS cluster Created task definition with simple web app Deployed service on Fargate Viewed container logs in CloudWatch Worked with Application Load Balancer:\nCreated ALB for distributing traffic Configured target groups Integrated ALB with ECS service Tested load balancing with multiple tasks "},{"uri":"https://thienluhoan.github.io/workshop-template/5-workshop/","title":"Workshop","tags":[],"description":"","content":" ‚ö†Ô∏è Note: The information below is for reference purposes only. Please do not copy verbatim for your report, including this warning.\nSecure Hybrid Access to S3 using VPC Endpoints Overview AWS PrivateLink provides private connectivity to AWS services from VPCs and your on-premises networks, without exposing your traffic to the Public Internet.\nIn this lab, you will learn how to create, configure, and test VPC endpoints that enable your workloads to reach AWS services without traversing the Public Internet.\nYou will create two types of endpoints to access Amazon S3: a Gateway VPC endpoint, and an Interface VPC endpoint. These two types of VPC endpoints offer different benefits depending on if you are accessing Amazon S3 from the cloud or your on-premises location\nGateway - Create a gateway endpoint to send traffic to Amazon S3 or DynamoDB using private IP addresses.You route traffic from your VPC to the gateway endpoint using route tables. Interface - Create an interface endpoint to send traffic to endpoint services that use a Network Load Balancer to distribute traffic. Traffic destined for the endpoint service is resolved using DNS. Content Workshop overview Prerequiste Access S3 from VPC Access S3 from On-premises VPC Endpoint Policies (Bonus) Resource Cleanup Clean up "},{"uri":"https://thienluhoan.github.io/workshop-template/5-workshop/5.6-cleanup/","title":"Resource Cleanup and Workshop Completion","tags":[],"description":"","content":" ‚ö†Ô∏è Important: The information provided here is for reference purposes only. Please do not copy verbatim for your report including this warning message.\nCongratulations on completing this comprehensive VPC Endpoints workshop! You\u0026rsquo;ve successfully explored enterprise-grade architectures for private AWS service connectivity.\nWhat You\u0026rsquo;ve Accomplished:\nThroughout this workshop, you implemented multiple AWS networking best practices:\n‚úÖ Gateway VPC Endpoint Architecture:\nCreated a Gateway Endpoint for S3 access from VPC resources Configured route table entries for private S3 connectivity Eliminated Internet Gateway dependencies for S3 traffic Implemented network-level access controls with endpoint policies ‚úÖ Interface VPC Endpoint with Hybrid Connectivity:\nDeployed Interface Endpoints with multi-AZ redundancy Configured Route 53 Resolver for hybrid DNS resolution Established VPN-based connectivity simulating on-premises access Created Private Hosted Zones for endpoint DNS resolution Implemented conditional DNS forwarding rules ‚úÖ Security Best Practices:\nApplied restrictive VPC endpoint policies for least privilege access Configured security groups for endpoint traffic control Demonstrated defense-in-depth with multiple policy layers Validated policy enforcement through practical testing ‚úÖ Production-Ready Skills:\nMulti-AZ deployment patterns for high availability Hybrid cloud DNS architecture design Private connectivity without internet exposure Cost optimization through VPC endpoint usage Why Resource Cleanup Is Critical Proper resource cleanup is essential for:\nCost Avoidance: Prevent ongoing charges for unused resources (VPC endpoints, Route 53 Resolver endpoints, CloudFormation stacks) Account Hygiene: Maintain a clean AWS environment for future projects Security Best Practice: Remove test infrastructure that may have permissive configurations Service Limits: Free up service quotas for production workloads Learning Reinforcement: Understanding deletion dependencies teaches AWS resource relationships Estimated Cleanup Time: 10-15 minutes if following the recommended order. Deletion dependencies require sequential processing for some resources.\nCleanup Order and Dependencies AWS resources have dependencies that require deletion in a specific order. Follow this sequence to avoid errors:\nDeletion Order:\n1. Route 53 DNS Records (depend on Hosted Zone) 2. Route 53 Resolver Rules (depend on Resolver Endpoints) 3. Route 53 Hosted Zones (depend on VPCs) 4. VPC Endpoints (depend on VPCs and Security Groups) 5. S3 Bucket Objects (must be empty before bucket deletion) 6. S3 Buckets (depend on being empty) 7. CloudFormation Stacks (delete VPC infrastructure last) Step-by-Step Cleanup Process Step 1: Delete Route 53 Private Hosted Zone DNS Records\nBefore deleting the hosted zone, remove the custom DNS records you created.\nNavigate to Route 53:\nOpen the Route 53 Console Click Hosted Zones in the left sidebar Access the Private Hosted Zone:\nClick on the hosted zone named s3.us-east-1.amazonaws.com You\u0026rsquo;ll see the DNS records including the Alias records you created Delete Custom DNS Records:\nSelect the Alias A records you created (apex and wildcard records pointing to the Interface Endpoint) Do NOT select the default NS and SOA records (required for hosted zone) Click Delete records Confirm deletion Delete the Hosted Zone:\nGo back to Hosted Zones list Select the s3.us-east-1.amazonaws.com zone Click Delete hosted zone Type delete in the confirmation box to confirm Click Delete Hosted Zone Deletion: You cannot delete a hosted zone that still has DNS records (except the mandatory NS and SOA records). Ensure you\u0026rsquo;ve removed all custom records first.\nStep 2: Remove Route 53 Resolver Forwarding Rule\nThe resolver rule must be disassociated from VPCs before deletion.\nNavigate to Resolver Rules:\nIn Route 53 console, click Resolver \u0026gt; Rules in the left sidebar Locate Your Forwarding Rule:\nFind the rule you created (e.g., S3-PrivateEndpoint-ForwardingRule) The rule forwards s3.us-east-1.amazonaws.com queries Disassociate from VPC:\nSelect the rule Click View details Click the VPC associations tab Select the associated VPC On-prem Click Disassociate Confirm the disassociation Delete the Rule:\nOnce disassociated, go back to the Rules list Select the rule Click Delete Confirm deletion Rule Dependencies: Resolver rules must be disassociated from all VPCs before deletion. Attempting to delete an associated rule will result in an error.\nStep 3: Delete VPC Endpoints\nRemove both the Gateway and Interface endpoints you created.\nNavigate to VPC Endpoints:\nOpen the VPC Console Click Endpoints in the left sidebar Delete Interface Endpoint:\nSelect the Interface Endpoint for S3 (the one in VPC Cloud) Endpoint type should show \u0026ldquo;Interface\u0026rdquo; Click Actions \u0026gt; Delete VPC endpoints Type delete to confirm Click Delete Delete Gateway Endpoint:\nSelect the Gateway Endpoint for S3 (also in VPC Cloud) Endpoint type should show \u0026ldquo;Gateway\u0026rdquo; Click Actions \u0026gt; Delete VPC endpoints Type delete to confirm Click Delete Immediate Deletion: VPC endpoints delete quickly (usually within 1-2 minutes). You can proceed to the next step while they\u0026rsquo;re being removed.\nStep 4: Empty and Delete S3 Buckets\nS3 buckets must be completely empty before they can be deleted.\nNavigate to S3 Console:\nOpen the S3 Console Empty the First Bucket:\nSelect your original test bucket (e.g., my-endpoint-test-bucket-123456) Click Empty Type permanently delete in the confirmation box Click Empty Wait for the empty operation to complete Delete the First Bucket:\nWith the bucket still selected, click Delete Type the exact bucket name to confirm Click Delete bucket Repeat for Second Bucket:\nSelect your restricted bucket (e.g., my-endpoint-test-bucket-123456-restricted) Click Empty, confirm with permanently delete Click Delete, type the bucket name, confirm deletion Data Loss Warning: Emptying and deleting S3 buckets permanently removes all objects. Ensure you don\u0026rsquo;t need any test data before proceeding. There is no recovery after deletion.\nStep 5: Delete CloudFormation Stacks\nCloudFormation stacks will automatically delete all resources they created (VPCs, subnets, EC2 instances, Route 53 Resolver endpoints, etc.).\nNavigate to CloudFormation:\nOpen the CloudFormation Console Delete PLOnpremSetup Stack:\nSelect the PLOnpremSetup stack This stack contains the DNS infrastructure (Resolver endpoints, Private Hosted Zone association) Click Delete Confirm deletion Deletion takes 3-5 minutes Wait for First Stack Deletion:\nMonitor the stack status - it will show \u0026ldquo;DELETE_IN_PROGRESS\u0026rdquo; Wait until it reaches \u0026ldquo;DELETE_COMPLETE\u0026rdquo; or disappears from the list You can view deletion events in the Events tab Delete PLCloudSetup Stack:\nSelect the PLCloudSetup stack This stack contains the VPC infrastructure (VPCs, subnets, EC2 instances, VPN gateway, etc.) Click Delete Confirm deletion Deletion takes 5-10 minutes due to the number of resources Stack Deletion Process: CloudFormation automatically handles resource dependencies within the stack, deleting resources in the correct order. You\u0026rsquo;ll see individual resources being deleted in the Events tab.\nWhat Gets Deleted by CloudFormation Stacks:\nPLOnpremSetup Stack Removes:\nRoute 53 Inbound Resolver Endpoint (in VPC Cloud) Route 53 Outbound Resolver Endpoint (in VPC On-prem) Associated elastic network interfaces IAM roles and policies for Resolver PLCloudSetup Stack Removes:\nVPC Cloud and VPC On-prem Public and private subnets in multiple AZs Internet Gateways and NAT Gateways Route tables and route table associations Security groups EC2 instances (Test-Gateway-Endpoint, Test-Interface-Endpoint, VPN Gateway) Transit Gateway and attachments VPN connections Elastic IPs Verification of Complete Cleanup After completing all deletion steps, verify that resources are fully removed:\nChecklist:\nRoute 53:\nNo private hosted zone for s3.us-east-1.amazonaws.com No resolver rules for S3 domain Inbound and Outbound Resolver endpoints gone VPC:\nNo endpoints in the Endpoints list VPC Cloud and VPC On-prem removed (after CloudFormation deletion) No custom security groups related to the workshop S3:\nBoth test buckets deleted No objects remaining from the workshop CloudFormation:\nBoth stacks show \u0026ldquo;DELETE_COMPLETE\u0026rdquo; or are no longer listed No failed deletion states EC2:\nNo instances named Test-Gateway-Endpoint or Test-Interface-Endpoint No VPN Gateway EC2 instance Cleanup Complete! If all items in the checklist are verified, you\u0026rsquo;ve successfully removed all workshop resources. Your AWS account is now clean and you won\u0026rsquo;t incur ongoing charges for this workshop.\nTroubleshooting Cleanup Issues Common Deletion Problems:\nIssue: Hosted Zone Won\u0026rsquo;t Delete\nCause: Custom DNS records still present Solution: Delete all A records except NS and SOA records, then retry Issue: Resolver Rule Won\u0026rsquo;t Delete\nCause: Still associated with VPC Solution: Disassociate from all VPCs first, then delete Issue: VPC Endpoint Stuck Deleting\nCause: Active connections or route table associations Solution: Wait 5 minutes and check again; should auto-resolve Issue: S3 Bucket Won\u0026rsquo;t Delete\nCause: Objects still in bucket or versioning enabled Solution: Empty bucket completely, disable versioning if enabled Issue: CloudFormation Stack Deletion Failed\nCause: Manual resource deletion or dependencies outside stack Solution: Check Events tab for specific resource errors; may need to manually delete blocking resources Cost Optimization Lessons Workshop Resource Costs (if left running):\nApproximate hourly costs for reference:\nInterface VPC Endpoint: ~$0.01/hour + $0.01/GB data processed Route 53 Resolver Endpoints: ~$0.125/hour per endpoint (2 endpoints = $0.25/hour) NAT Gateway: ~$0.045/hour + $0.045/GB data processed EC2 instances (t3.micro): ~$0.0104/hour each (3 instances = $0.03/hour) Monthly cost if not deleted: ~$200-250\nKey Takeaway: Always delete test resources promptly to avoid unnecessary charges!\nWhat You\u0026rsquo;ve Learned Technical Skills:\nVPC Endpoint architecture patterns (Gateway vs Interface) Hybrid DNS configuration with Route 53 Resolver VPN-based hybrid connectivity Multi-AZ high availability patterns Security policy implementation and testing CloudFormation infrastructure as code AWS Services Mastered:\nAmazon VPC (Endpoints, Subnets, Route Tables, Security Groups) Amazon S3 (Bucket policies, private access) Route 53 (Private Hosted Zones, Resolver endpoints, Forwarding rules) AWS PrivateLink (Interface Endpoints) CloudFormation (Stack management) Systems Manager (Session Manager) IAM (Policies, roles, endpoint policies) Enterprise Architecture Patterns:\nDefense in depth security Least privilege access control Network segmentation Private connectivity without internet exposure Hybrid cloud integration High availability design Next Steps for Continued Learning Advanced Topics to Explore:\nAWS Direct Connect: Replace VPN with dedicated network connection AWS Transit Gateway: Centralized hub for multi-VPC connectivity VPC Peering: Alternative connectivity pattern between VPCs PrivateLink for Custom Services: Expose your own services via PrivateLink Multi-Region Endpoints: Disaster recovery with cross-region endpoints AWS Network Firewall: Advanced traffic inspection and filtering VPC Flow Logs: Network traffic analysis and security monitoring Recommended AWS Documentation:\nVPC Endpoints Route 53 Resolver AWS PrivateLink VPC Endpoint Policies Practice Exercises:\nImplement similar patterns for other AWS services (RDS, DynamoDB, EC2 API) Design multi-region endpoint architectures Integrate with AWS Transit Gateway for enterprise hub-and-spoke Implement comprehensive logging and monitoring Thank you for completing this workshop! You now have practical experience with AWS private connectivity patterns essential for enterprise cloud architectures. Apply these skills to build secure, scalable, and cost-effective solutions in your real-world projects!\n"},{"uri":"https://thienluhoan.github.io/workshop-template/6-self-evaluation/","title":"Self-Assessment","tags":[],"description":"","content":" ‚ö†Ô∏è Note: The information below is for reference purposes only. Please do not copy it verbatim into your report, including this warning.\nDuring my internship at First Cloud Journey (FCJ) from 08/09/2025 to 28/11/2025, I had the opportunity to learn and practice AWS cloud services in a practical environment.\nI participated in learning core AWS services and built hands-on projects, through which I improved my skills in cloud computing, infrastructure design, serverless architecture, and technical documentation.\nThroughout the internship period, I maintained a consistent learning attitude, completed weekly tasks as planned, and actively explored AWS services to deepen my understanding beyond basic tutorials.\nTo objectively reflect on my internship period, I would like to evaluate myself based on the following criteria:\nNo. Criteria Description Good Fair Average 1 Professional knowledge \u0026amp; skills Understanding of the field, applying knowledge in practice, proficiency with tools, work quality ‚òê ‚úÖ ‚òê 2 Ability to learn Ability to absorb new knowledge and learn quickly ‚úÖ ‚òê ‚òê 3 Proactiveness Taking initiative, seeking out tasks without waiting for instructions ‚òê ‚úÖ ‚òê 4 Sense of responsibility Completing tasks on time and ensuring quality ‚úÖ ‚òê ‚òê 5 Discipline Adhering to schedules, rules, and work processes ‚òê ‚úÖ ‚òê 6 Progressive mindset Willingness to receive feedback and improve oneself ‚úÖ ‚òê ‚òê 7 Communication Presenting ideas and reporting work clearly ‚òê ‚úÖ ‚òê 8 Teamwork Working effectively with colleagues and participating in teams ‚òê ‚úÖ ‚òê 9 Professional conduct Respecting colleagues, partners, and the work environment ‚úÖ ‚òê ‚òê 10 Problem-solving skills Identifying problems, proposing solutions, and showing creativity ‚òê ‚úÖ ‚òê 11 Contribution to project/team Work effectiveness, innovative ideas, recognition from the team ‚òê ‚úÖ ‚òê 12 Overall General evaluation of the entire internship period ‚òê ‚úÖ ‚òê Needs Improvement Improve practical troubleshooting skills when encountering complex AWS configuration issues Develop deeper understanding of cost optimization strategies beyond basic concepts Enhance ability to design complete architectures independently without reference materials Strengthen knowledge of security best practices and compliance requirements Build more confidence in explaining technical concepts to both technical and non-technical audiences "},{"uri":"https://thienluhoan.github.io/workshop-template/1-worklog/1.6-week6/","title":"Week 6 Worklog","tags":[],"description":"","content":" ‚ö†Ô∏è Note: The following information is for reference purposes only. Please do not copy verbatim for your own report, including this warning.\nWeek 6 Objectives: Learn about monitoring and logging with CloudWatch. Understand Auto Scaling for automatic scaling resources. Study cost optimization and billing management. Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 2 - Review basic IAM knowledge - Learn advanced IAM Policy (JSON structure, Effect, Action, Resource, Condition) - Create custom policies and attach to users/groups 13/10/2025 13/10/2025 AWS Journey 3 - Introduction to AWS Key Management Service (KMS) - Create a Customer Managed Key (CMK) and test file encryption/decryption - Apply KMS to encrypt an S3 bucket or an EBS volume 14/10/2025 14/10/2025 AWS Journey 4 - Get familiar with AWS Secrets Manager - Create a secret to store Database connection information - Write a small Lambda script to read the secret from Secrets Manager 15/10/2025 15/10/2025 AWS Journey 5 - Explore AWS Billing Dashboard and Cost Explorer - View costs by service, region, and time period - Set up Cost Anomaly Detection 16/10/2025 16/10/2025 AWS Journey 6 - Create an AWS Budget and configure cost alerts via email - Write a weekly cost summary report with optimization proposals (stop EC2, cleanup EBS, reduce log retention, etc.) - Wrap up Week 6 learnings 17/10/2025 17/10/2025 AWS Journey Week 6 Achievements: Learned about CloudWatch monitoring:\nUnderstood CloudWatch metrics for AWS services Learned how to view and analyze metrics Studied CloudWatch dashboards for visualization Created CloudWatch alarms:\nSet up alarm for EC2 CPU utilization \u0026gt; 80% Configured SNS topic for email notifications Tested alarm by increasing CPU load Viewed CloudWatch Logs for Lambda executions Understood Auto Scaling concepts:\nLearned benefits of automatic scaling Studied Auto Scaling Groups components Understood scaling policies: Target tracking (maintain metric at target) Step scaling (scale based on thresholds) Practiced with Auto Scaling:\nCreated Launch Template for EC2 instances Set up Auto Scaling Group (min: 2, max: 5) Configured scaling policy based on CPU Generated load to test automatic scaling Observed instances launching and terminating Learned about AWS billing:\nExplored AWS Billing Dashboard Set up billing alerts for cost threshold Reviewed Cost Explorer for spending patterns Learned about cost allocation tags "},{"uri":"https://thienluhoan.github.io/workshop-template/7-feedback/","title":"Sharing and Feedback","tags":[],"description":"","content":" ‚ö†Ô∏è Note: The information below is for reference purposes only. Please do not copy verbatim for your report, including this warning.\nHere, you can freely share your personal opinions about your experience participating in the First Cloud Journey program. This will help the FCJ team improve any shortcomings based on the following aspects:\nOverall Evaluation 1. Working Environment\nThe learning environment at FCJ is well-structured with clear weekly objectives and comprehensive learning materials. The online documentation and workshop content are detailed and easy to follow. I appreciate the flexibility to learn at my own pace while still having a roadmap to follow. One suggestion would be to add more interactive elements like live Q\u0026amp;A sessions or study groups where interns can discuss challenges together.\n2. Support from Mentor / Team Admin\nThe mentors are knowledgeable and responsive when I have questions about AWS services or architecture decisions. They provide helpful resources and point me in the right direction rather than just giving answers, which encourages independent learning. The admin team is organized in managing documents and communication channels. However, sometimes response time can be slower during peak periods, which is understandable.\n3. Relevance of Work to Academic Major\nThe AWS cloud computing content is highly relevant to my IT/Computer Science background. It bridges the gap between theoretical knowledge from university and practical industry skills. Learning services like EC2, S3, Lambda, and VPC directly applies to what I studied about networking, databases, and distributed systems. The serverless architecture concepts were particularly new and valuable.\n4. Learning \u0026amp; Skill Development Opportunities\nThis internship significantly improved my technical skills in cloud infrastructure, not just AWS-specific knowledge but also broader concepts like high availability, disaster recovery, and cost optimization. I also developed soft skills like technical writing (for reports), time management (weekly tasks), and self-directed learning. The hands-on workshops were especially effective for skill building.\n5. Company Culture \u0026amp; Team Spirit\nFCJ promotes a culture of continuous learning and knowledge sharing. The emphasis on AWS Journey content and community-driven learning creates a supportive atmosphere. While most interaction is remote/online, the community channels make it easy to connect with other learners and share experiences. The open-source nature of the learning materials reflects a collaborative spirit.\n6. Internship Policies / Benefits\nThe program structure is clear with defined weekly goals and milestones. The access to comprehensive AWS learning resources and workshop materials is valuable. The flexibility to complete tasks on your own schedule while meeting weekly deadlines is practical for students balancing other commitments. Having documentation in both English and Vietnamese is also helpful.\nAdditional Questions What did you find most satisfying during your internship?\nThe hands-on experience building actual AWS infrastructure was most satisfying. Moving from just reading about services to actually deploying VPCs, configuring Lambda functions, and setting up databases gave me real confidence. Completing the final project that integrated multiple AWS services felt like a genuine achievement.\nWhat do you think the company should improve for future interns?\nAdd more troubleshooting scenarios and \u0026ldquo;debugging labs\u0026rdquo; where interns practice fixing broken configurations Provide sample interview questions or certification preparation materials to help interns transition to jobs or AWS certifications Create a peer review system where interns can review each other\u0026rsquo;s architecture designs Offer optional advanced tracks for those who finish early or want deeper specialization If recommending to a friend, would you suggest they intern here? Why or why not?\nYes, I would definitely recommend FCJ to friends interested in cloud computing. The structured learning path, quality materials, and practical experience make it an excellent starting point for AWS. It\u0026rsquo;s especially good for self-motivated learners who want flexibility. However, I\u0026rsquo;d note that it requires discipline since much of the learning is self-paced.\nSuggestions \u0026amp; Expectations Do you have any suggestions to improve the internship experience?\nAdd monthly \u0026ldquo;architecture review\u0026rdquo; sessions where interns present their solutions and get feedback Create a Discord or Slack channel for real-time discussions among current interns Provide cost tracking templates to help interns monitor their AWS spending during practice Include more case studies from real companies showing how they use AWS services Offer optional capstone project ideas for different interests (data analytics, web apps, IoT, etc.) Would you like to continue this program in the future?\nI would be interested in an advanced program focused on specialized areas like AWS security (GuardDuty, Security Hub), data engineering (EMR, Redshift), or DevOps practices (ECS, EKS, CI/CD pipelines). A mentorship program where experienced alumni guide new interns would also be valuable.\nAny other comments (free sharing):\nOverall, this internship exceeded my expectations. I came in with basic AWS knowledge and left with practical skills I can immediately apply. The progression from fundamental services (EC2, S3) to advanced topics (serverless, containers, data analytics) was well-paced. The emphasis on documentation and reporting also taught me the importance of explaining technical decisions clearly - a skill often overlooked in technical training.\nI\u0026rsquo;m grateful for the opportunity to learn in a structured yet flexible environment. The experience has motivated me to pursue AWS certifications and consider cloud engineering as a career path. Thank you to the FCJ team for creating and maintaining such valuable learning resources.\n"},{"uri":"https://thienluhoan.github.io/workshop-template/1-worklog/1.7-week7/","title":"Week 7 Worklog","tags":[],"description":"","content":" ‚ö†Ô∏è Note: The following information is for reference purposes only. Please do not copy verbatim for your own report, including this warning.\nWeek 7 Objectives: Learn about CI/CD concepts and AWS CodePipeline. Understand CodeCommit, CodeBuild, and CodeDeploy. Build a simple deployment pipeline. Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 2 - Learn about High Availability, Fault Tolerance and Elasticity concepts - Introduction to Auto Scaling Group (ASG) and Elastic Load Balancer (ELB) 20/10/2025 20/10/2025 AWS Journey 3 - Practice creating Auto Scaling Group for EC2 instance - Set up launch template, scaling policy and target tracking 21/10/2025 21/10/2025 AWS Journey 4 - Create and configure Application Load Balancer (ALB) - Connect ALB with ASG for load distribution - Test website access through ALB DNS 22/10/2025 22/10/2025 AWS Journey 5 - Get familiar with Amazon SQS and SNS services - Create SQS queue, SNS topic and subscription - Send and receive notifications between components 23/10/2025 23/10/2025 AWS Journey 6 - Enable VPC Flow Logs to monitor network traffic - Analyze logs in CloudWatch Logs - Summarize knowledge about reliability \u0026amp; scaling 24/10/2025 24/10/2025 AWS Journey Week 7 Achievements: Learned about CI/CD concepts:\nUnderstood Continuous Integration Understood Continuous Deployment Learned benefits of automation Worked with AWS CodeCommit:\nCreated Git repository in CodeCommit Configured Git credentials Pushed sample web application code Learned basic Git commands Practiced with AWS CodeBuild:\nCreated buildspec.yml configuration file Set up CodeBuild project Configured build environment (Ubuntu, Node.js) Built application successfully Viewed build logs Deployed with AWS CodeDeploy:\nCreated appspec.yml file Set up deployment group with EC2 instances Installed CodeDeploy agent on EC2 Deployed application to EC2 Verified deployment success Built complete pipeline with CodePipeline:\nCreated pipeline with 3 stages: Source, Build, Deploy Connected CodeCommit as source Added CodeBuild for building Added CodeDeploy for deployment Tested automatic deployment on code push "},{"uri":"https://thienluhoan.github.io/workshop-template/1-worklog/1.8-week8/","title":"Week 8 Worklog","tags":[],"description":"","content":" ‚ö†Ô∏è Note: The following information is for reference purposes only. Please do not copy verbatim for your own report, including this warning.\nWeek 8 Objectives: Learn about containers and Docker basics. Understand Amazon ECS and container orchestration. Deploy containerized applications on AWS. Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 2 - Overview of AWS Well-Architected Framework, 5 pillars: Operational Excellence, Security, Reliability, Performance Efficiency, Cost Optimization - Identify the role and importance of each pillar in system design 27/10/2025 27/10/2025 AWS Journey 3 - Review Secure Architecture Design ‚Üí IAM, MFA, SCP, Encryption (KMS, TLS/ACM), Security Groups, NACLs, GuardDuty, Shield, WAF, Secrets Manager 28/10/2025 28/10/2025 AWS Journey 4 - Review Resilient Architecture Design ‚Üí Multi-AZ, Multi-Region, DR Strategies, Auto Scaling, Route 53, Load Balancing, Backup \u0026amp; Restore 29/10/2025 29/10/2025 AWS Journey 5 - Review Performance and Cost Optimization (High-Performing \u0026amp; Cost-Optimized Architectures) ‚Üí EC2 Auto Scaling, Lambda, Fargate, CloudFront, Global Accelerator, Cost Explorer, Budgets, Savings Plans, Storage Tiering 30/10/2025 30/10/2025 AWS Journey 6 - Comprehensive Practice: + Build a sample architecture combining EC2, S3, RDS, IAM, VPC, CloudFront, Lambda, CloudWatch + Evaluate according to 5 Well-Architected Framework criteria + Write weekly summary report 31/10/2025 31/10/2025 AWS Journey Week 8 Achievements: Learned about containerization:\nUnderstood benefits of containers vs VMs Learned Docker basics and terminology Studied container use cases Practiced with Docker locally:\nInstalled Docker Desktop Pulled images from Docker Hub (nginx, node, python) Created simple Dockerfile: FROM node:14 WORKDIR /app COPY package*.json ./ RUN npm install COPY . . EXPOSE 3000 CMD [\u0026#34;npm\u0026#34;, \u0026#34;start\u0026#34;] Built and ran containers locally Understood Amazon ECS:\nLearned ECS architecture and components Compared Fargate (serverless) vs EC2 launch types Studied task definitions and services Worked with ECR:\nCreated ECR repository Tagged and pushed Docker image to ECR Configured authentication with ECR Deployed on ECS:\nCreated ECS cluster Created task definition with container settings Deployed service on Fargate Configured networking and security groups Accessed application via public IP Viewed container logs in CloudWatch "},{"uri":"https://thienluhoan.github.io/workshop-template/1-worklog/1.9-week9/","title":"Week 9 Worklog","tags":[],"description":"","content":" ‚ö†Ô∏è Note: The following information is for reference purposes only. Please do not copy verbatim for your own report, including this warning.\nWeek 9 Objectives: Learn about serverless application development on AWS. Understand AWS Lambda and event-driven architecture. Build and deploy serverless applications. Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 2 - Introduction to Data \u0026amp; Analytics ecosystem on AWS - Understand Data Lake concepts, ETL pipeline, and how to connect data from multiple sources 03/11/2025 03/11/2025 AWS Journey 3 - Create Data Lake on Amazon S3 - Directory structure, access permissions - Set up AWS Glue Crawler to identify data schema 04/11/2025 04/11/2025 AWS Journey 4 - Practice AWS Athena to query data in Data Lake - Write basic SQL queries and export results to S3 05/11/2025 05/11/2025 AWS Journey 5 - Introduction and practice with Amazon QuickSight - Connect QuickSight with Athena to visualize data - Create simple dashboard with charts and summary tables 06/11/2025 06/11/2025 AWS Journey 6 - Review \u0026amp; consolidate weekly knowledge: + Data collection ‚Üí processing ‚Üí analysis process on AWS + Compare Glue, Athena, QuickSight with traditional tools + Write summary report of practice process 07/11/2025 07/11/2025 AWS Journey Week 9 Achievements: Learned about serverless computing:\nUnderstood serverless benefits (no server management, auto-scaling) Learned AWS Lambda pricing model (pay per execution) Studied serverless use cases Practiced with AWS Lambda:\nCreated Lambda functions in Python and Node.js Configured function settings: Memory: 128MB - 512MB Timeout: 3 seconds - 30 seconds Environment variables for configuration Tested functions with sample events Viewed execution logs in CloudWatch Worked with Lambda triggers:\nSet up S3 trigger for file processing Created EventBridge rule for scheduled execution Configured API Gateway integration Tested automatic function invocation Used AWS SAM:\nInstalled SAM CLI Created SAM project with template.yaml Defined Lambda functions and API Gateway in SAM Deployed application with sam deploy Tested local development with sam local Built serverless API:\nCreated REST API with API Gateway Implemented CRUD operations with Lambda Connected to DynamoDB for data storage Tested API with Postman Configured CORS for web access Integrated with DynamoDB:\nCreated DynamoDB table Implemented Lambda functions to read/write data Used boto3 (Python) for DynamoDB operations Tested data persistence Monitored DynamoDB metrics ‚îÇ S3: Raw data, all formats ‚îÇ ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚Üì ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îÇ Processing Layer ‚îÇ ‚îÇ - AWS Glue ETL Jobs ‚îÇ ‚îÇ - Glue Data Quality ‚îÇ ‚îÇ - EMR for complex transformations ‚îÇ ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚Üì ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îÇ Silver Layer (Cleansed Zone) ‚îÇ ‚îÇ S3: Parquet, validated, standardized ‚îÇ ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚Üì ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îÇ Aggregation Layer ‚îÇ ‚îÇ - Glue ETL Jobs (aggregations) ‚îÇ ‚îÇ - Athena CTAS queries ‚îÇ ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚Üì ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îÇ Gold Layer (Business Zone) ‚îÇ ‚îÇ S3: Optimized data marts ‚îÇ ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚Üì ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îÇ Consumption Layer ‚îÇ ‚îÇ - Amazon Athena (SQL analytics) ‚îÇ ‚îÇ - QuickSight (BI dashboards) ‚îÇ ‚îÇ - SageMaker (ML models) ‚îÇ ‚îÇ - Redshift Spectrum (DW integration) ‚îÇ ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò Data Governance with AWS Lake Formation:\nAccess Control:\nCentralized permissions management Column-level security for PII fields Row-level security based on user attributes Tag-based access control (TBAC) Cross-account sharing with AWS RAM Permissions Matrix:\nRole Bronze Silver Gold PII Columns Data Engineers Read/Write Read/Write Read/Write Masked Data Analysts Read Read Read Masked Data Scientists Read Read Read Masked Business Users No Access No Access Read Denied Compliance Team Read Read Read Unmasked Data Catalog:\n347 tables cataloged 1,245 columns documented 89 data sources registered Schema versioning enabled Automated metadata extraction 2. Enterprise Data Lake Implementation S3 Data Lake Configuration: Bucket Structure:\ndatalake-bronze-prod/ ‚îú‚îÄ‚îÄ application-logs/ ‚îÇ ‚îî‚îÄ‚îÄ year=2026/month=01/day=27/ ‚îú‚îÄ‚îÄ database-cdc/ ‚îÇ ‚îî‚îÄ‚îÄ source=postgresql/table=orders/ ‚îú‚îÄ‚îÄ api-data/ ‚îÇ ‚îî‚îÄ‚îÄ provider=stripe/endpoint=payments/ ‚îú‚îÄ‚îÄ file-uploads/ ‚îÇ ‚îî‚îÄ‚îÄ type=customer-data/ ‚îî‚îÄ‚îÄ iot-sensors/ ‚îî‚îÄ‚îÄ device-type=temperature/location=warehouse-1/ datalake-silver-prod/ ‚îú‚îÄ‚îÄ customers/ ‚îÇ ‚îî‚îÄ‚îÄ year=2026/month=01/day=27/ ‚îú‚îÄ‚îÄ orders/ ‚îÇ ‚îî‚îÄ‚îÄ year=2026/month=01/day=27/ ‚îú‚îÄ‚îÄ products/ ‚îÇ ‚îî‚îÄ‚îÄ year=2026/month=01/ ‚îî‚îÄ‚îÄ transactions/ ‚îî‚îÄ‚îÄ year=2026/month=01/day=27/hour=10/ datalake-gold-prod/ ‚îú‚îÄ‚îÄ sales-analytics/ ‚îÇ ‚îî‚îÄ‚îÄ report-date=2026-01-27/ ‚îú‚îÄ‚îÄ customer-360/ ‚îÇ ‚îî‚îÄ‚îÄ snapshot-date=2026-01-27/ ‚îî‚îÄ‚îÄ product-performance/ ‚îî‚îÄ‚îÄ analysis-period=2026-01/ Security Configuration:\nBucket encryption: SSE-KMS with automatic key rotation Versioning: Enabled on all buckets MFA Delete: Enabled for production buckets Access logging: Enabled to audit bucket Object Lock: Enabled for compliance data (WORM) Block public access: Enforced at bucket and account level VPC Endpoints: S3 access through private network only Cost Optimization:\nS3 Intelligent-Tiering: Automatic tiering for Bronze layer Lifecycle policies: Bronze: 30 days Standard ‚Üí Glacier Instant Retrieval Silver: 90 days Standard ‚Üí Glacier Flexible Retrieval Gold: 365 days Standard ‚Üí Glacier Deep Archive Compression: Parquet with Snappy (70% size reduction) Partitioning: Reduced scan costs by 85% S3 Select: Push-down filtering (60% cost reduction) Storage costs: $450/month for 8.5 TB (down from $1,200 with Standard storage) Data Quality Metrics:\nData freshness: 99.2% within SLA (15 minutes) Data completeness: 98.7% (missing field validation) Data accuracy: 99.5% (business rule validation) Schema compliance: 100% (automated checks) Duplicate rate: 0.3% (post-deduplication) 3. AWS Glue ETL Pipeline Implementation Glue Crawlers Configuration:\nBronze Crawler:\nName: bronze-data-crawler Schedule: Every 6 hours Data sources: All Bronze layer S3 paths Classifiers: JSON, CSV, Parquet, Avro custom classifiers Tables created: 45 tables Partitions discovered: 2,834 partitions Schema inference: Automatic with conflict resolution Silver Crawler:\nName: silver-data-crawler Schedule: Daily at 2 AM Data sources: Silver layer S3 paths Partition projection: Enabled for time-based partitions Tables created: 28 tables Schema evolution: Track and version changes Glue ETL Jobs:\nJob 1 - Bronze to Silver Transformation:\nimport sys from awsglue.transforms import * from awsglue.utils import getResolvedOptions from pyspark.context import SparkContext from awsglue.context import GlueContext from awsglue.job import Job from awsglue.dynamicframe import DynamicFrame from pyspark.sql.functions import * args = getResolvedOptions(sys.argv, [\u0026#39;JOB_NAME\u0026#39;, \u0026#39;database_name\u0026#39;, \u0026#39;table_name\u0026#39;]) sc = SparkContext() glueContext = GlueContext(sc) spark = glueContext.spark_session job = Job(glueContext) job.init(args[\u0026#39;JOB_NAME\u0026#39;], args) # Read from Bronze layer bronze_dyf = glueContext.create_dynamic_frame.from_catalog( database=args[\u0026#39;database_name\u0026#39;], table_name=args[\u0026#39;table_name\u0026#39;], transformation_ctx=\u0026#34;bronze_dyf\u0026#34; ) # Convert to Spark DataFrame for transformations df = bronze_dyf.toDF() # Data Quality Checks df = df.filter(col(\u0026#34;order_id\u0026#34;).isNotNull()) # Remove null order IDs df = df.filter(col(\u0026#34;order_amount\u0026#34;) \u0026gt; 0) # Valid amounts only df = df.dropDuplicates([\u0026#34;order_id\u0026#34;]) # Deduplicate # Data Transformations df = df.withColumn(\u0026#34;order_date\u0026#34;, to_date(col(\u0026#34;created_at\u0026#34;))) df = df.withColumn(\u0026#34;order_amount\u0026#34;, col(\u0026#34;order_amount\u0026#34;).cast(\u0026#34;decimal(10,2)\u0026#34;)) df = df.withColumn(\u0026#34;customer_email\u0026#34;, lower(trim(col(\u0026#34;customer_email\u0026#34;)))) # PII Masking df = df.withColumn(\u0026#34;customer_phone_masked\u0026#34;, regexp_replace(col(\u0026#34;customer_phone\u0026#34;), \u0026#34;\\\\d{4}$\u0026#34;, \u0026#34;XXXX\u0026#34;)) # Add metadata columns df = df.withColumn(\u0026#34;processing_timestamp\u0026#34;, current_timestamp()) df = df.withColumn(\u0026#34;data_source\u0026#34;, lit(\u0026#34;order_system\u0026#34;)) # Add partitioning columns df = df.withColumn(\u0026#34;year\u0026#34;, year(col(\u0026#34;order_date\u0026#34;))) df = df.withColumn(\u0026#34;month\u0026#34;, month(col(\u0026#34;order_date\u0026#34;))) df = df.withColumn(\u0026#34;day\u0026#34;, dayofmonth(col(\u0026#34;order_date\u0026#34;))) # Convert back to DynamicFrame silver_dyf = DynamicFrame.fromDF(df, glueContext, \u0026#34;silver_dyf\u0026#34;) # Write to Silver layer in Parquet format with partitioning glueContext.write_dynamic_frame.from_options( frame=silver_dyf, connection_type=\u0026#34;s3\u0026#34;, connection_options={ \u0026#34;path\u0026#34;: \u0026#34;s3://datalake-silver-prod/orders/\u0026#34;, \u0026#34;partitionKeys\u0026#34;: [\u0026#34;year\u0026#34;, \u0026#34;month\u0026#34;, \u0026#34;day\u0026#34;] }, format=\u0026#34;parquet\u0026#34;, format_options={ \u0026#34;compression\u0026#34;: \u0026#34;snappy\u0026#34; }, transformation_ctx=\u0026#34;silver_write\u0026#34; ) job.commit() Job 2 - Silver to Gold Aggregation:\n# Daily sales aggregation job from pyspark.sql.functions import sum, count, avg, max, min from pyspark.sql.window import Window # Read from Silver layer orders_df = spark.read.parquet(\u0026#34;s3://datalake-silver-prod/orders/\u0026#34;) customers_df = spark.read.parquet(\u0026#34;s3://datalake-silver-prod/customers/\u0026#34;) products_df = spark.read.parquet(\u0026#34;s3://datalake-silver-prod/products/\u0026#34;) # Join datasets sales_df = orders_df \\ .join(customers_df, \u0026#34;customer_id\u0026#34;, \u0026#34;left\u0026#34;) \\ .join(products_df, \u0026#34;product_id\u0026#34;, \u0026#34;left\u0026#34;) # Daily sales aggregation daily_sales = sales_df.groupBy(\u0026#34;order_date\u0026#34;, \u0026#34;product_category\u0026#34;) \\ .agg( sum(\u0026#34;order_amount\u0026#34;).alias(\u0026#34;total_revenue\u0026#34;), count(\u0026#34;order_id\u0026#34;).alias(\u0026#34;total_orders\u0026#34;), avg(\u0026#34;order_amount\u0026#34;).alias(\u0026#34;avg_order_value\u0026#34;), countDistinct(\u0026#34;customer_id\u0026#34;).alias(\u0026#34;unique_customers\u0026#34;) ) # Calculate running totals with window functions window_spec = Window.partitionBy(\u0026#34;product_category\u0026#34;) \\ .orderBy(\u0026#34;order_date\u0026#34;) \\ .rowsBetween(Window.unboundedPreceding, Window.currentRow) daily_sales = daily_sales.withColumn( \u0026#34;cumulative_revenue\u0026#34;, sum(\u0026#34;total_revenue\u0026#34;).over(window_spec) ) # Write to Gold layer daily_sales.write \\ .mode(\u0026#34;overwrite\u0026#34;) \\ .partitionBy(\u0026#34;order_date\u0026#34;) \\ .parquet(\u0026#34;s3://datalake-gold-prod/sales-analytics/daily-sales/\u0026#34;) Glue Data Quality Rules:\nRules = [ # Completeness checks ColumnExists \u0026#34;order_id\u0026#34;, ColumnExists \u0026#34;customer_id\u0026#34;, ColumnExists \u0026#34;order_amount\u0026#34;, # Uniqueness checks IsUnique \u0026#34;order_id\u0026#34;, # Validity checks ColumnValues \u0026#34;order_amount\u0026#34; \u0026gt; 0, ColumnValues \u0026#34;order_status\u0026#34; in [\u0026#34;pending\u0026#34;, \u0026#34;completed\u0026#34;, \u0026#34;cancelled\u0026#34;], ColumnDataType \u0026#34;order_date\u0026#34; = \u0026#34;date\u0026#34;, # Consistency checks ColumnLength \u0026#34;customer_email\u0026#34; \u0026lt;= 255, RowCount \u0026gt; 0, # Custom rules CustomSql \u0026#34;SELECT COUNT(*) FROM primary WHERE order_amount \u0026gt; 100000\u0026#34; = 0 ] Glue Job Monitoring:\nJob execution metrics tracked in CloudWatch Average job duration: 8 minutes (processing 4.5GB data) Success rate: 99.1% Failed jobs: Auto-retry 3 times with exponential backoff Cost per job run: $0.65 (2 DPU for 8 minutes) Jobs triggered: 124 executions this week Data processed: 562 GB total 4. Advanced Analytics with Amazon Athena Athena Workgroup Configuration:\nProduction Workgroup:\nData scanned limit: 10 TB per month Query timeout: 30 minutes Results encryption: SSE-KMS Results location: s3://athena-results-prod/ Results retention: 30 days Engine version: Athena engine version 3 Cost allocation tags: Environment=Production, Team=Analytics Development Workgroup:\nData scanned limit: 1 TB per month Query timeout: 10 minutes Enforce workgroup configuration Query result reuse: 60 minutes Advanced SQL Queries Examples:\nQuery 1 - Customer Cohort Analysis:\nWITH customer_cohorts AS ( SELECT customer_id, DATE_TRUNC(\u0026#39;month\u0026#39;, MIN(order_date)) AS cohort_month, DATE_TRUNC(\u0026#39;month\u0026#39;, order_date) AS order_month, SUM(order_amount) AS revenue FROM gold_layer.orders WHERE order_date \u0026gt;= DATE \u0026#39;2025-01-01\u0026#39; GROUP BY 1, 3 ), cohort_metrics AS ( SELECT cohort_month, order_month, DATE_DIFF(\u0026#39;month\u0026#39;, cohort_month, order_month) AS months_since_first_order, COUNT(DISTINCT customer_id) AS customers, SUM(revenue) AS total_revenue FROM customer_cohorts GROUP BY 1, 2 ) SELECT cohort_month, months_since_first_order, customers, total_revenue, ROUND(100.0 * customers / FIRST_VALUE(customers) OVER (PARTITION BY cohort_month ORDER BY months_since_first_order), 2) AS retention_rate FROM cohort_metrics ORDER BY cohort_month, months_since_first_order; Query 2 - Product Affinity Analysis:\nWITH product_pairs AS ( SELECT o1.order_id, o1.product_id AS product_a, o2.product_id AS product_b FROM gold_layer.order_items o1 JOIN gold_layer.order_items o2 ON o1.order_id = o2.order_id AND o1.product_id \u0026lt; o2.product_id ) SELECT pa.product_name AS product_a_name, pb.product_name AS product_b_name, COUNT(*) AS frequency, ROUND(100.0 * COUNT(*) / ( SELECT COUNT(DISTINCT order_id) FROM gold_layer.order_items ), 2) AS support_percentage FROM product_pairs pp JOIN gold_layer.products pa ON pp.product_a = pa.product_id JOIN gold_layer.products pb ON pp.product_b = pb.product_id GROUP BY 1, 2 HAVING COUNT(*) \u0026gt; 50 ORDER BY frequency DESC LIMIT 20; Query 3 - Time Series Forecasting Preparation:\nWITH daily_metrics AS ( SELECT order_date, SUM(order_amount) AS daily_revenue, COUNT(DISTINCT order_id) AS daily_orders, COUNT(DISTINCT customer_id) AS daily_customers, AVG(order_amount) AS avg_order_value FROM gold_layer.orders WHERE order_date \u0026gt;= CURRENT_DATE - INTERVAL \u0026#39;365\u0026#39; DAY GROUP BY order_date ), moving_averages AS ( SELECT order_date, daily_revenue, AVG(daily_revenue) OVER ( ORDER BY order_date ROWS BETWEEN 6 PRECEDING AND CURRENT ROW ) AS ma_7day, AVG(daily_revenue) OVER ( ORDER BY order_date ROWS BETWEEN 29 PRECEDING AND CURRENT ROW ) AS ma_30day, STDDEV(daily_revenue) OVER ( ORDER BY order_date ROWS BETWEEN 29 PRECEDING AND CURRENT ROW ) AS stddev_30day FROM daily_metrics ) SELECT order_date, daily_revenue, ma_7day, ma_30day, -- Z-score for anomaly detection (daily_revenue - ma_30day) / NULLIF(stddev_30day, 0) AS z_score, -- Day of week patterns day_of_week(order_date) AS day_of_week, -- Month of year seasonality month(order_date) AS month_num FROM moving_averages ORDER BY order_date DESC; Query Performance Optimization:\nPartitioning strategy: Reduced scan by 85% Before: 2.4 TB scanned per query After: 360 GB scanned per query Columnar format (Parquet): 70% compression ratio Partition projection: Eliminated Glue Catalog calls CTAS (Create Table As Select): Materialized frequent queries Bucketing: Optimized join performance Query result reuse: 60-minute caching Average query cost: $0.18 (down from $1.20) Athena Federated Queries:\nConnected data sources: RDS PostgreSQL (operational database) DynamoDB (user sessions) DocumentDB (product catalog) On-premises MySQL (via Lambda connector) Cross-source analytics enabled Query example: Join S3 data lake with live RDS data 5. Enterprise BI with Amazon QuickSight QuickSight Configuration:\nEdition: Enterprise (SAML SSO, row-level security, hourly refresh) Users: 45 authors, 150 readers SPICE capacity: 50 GB allocated Data sources connected: Amazon Athena (primary) Amazon S3 (direct query) Amazon RDS PostgreSQL Salesforce (via connector) Google Analytics (via connector) Dashboard Implementations:\nDashboard 1 - Executive KPI Dashboard:\nMetrics displayed: Total Revenue (current vs previous period) Active Customers (MoM growth %) Average Order Value (trend) Customer Lifetime Value Churn Rate Net Promoter Score (NPS) Visualizations: KPI cards with comparison indicators Line charts for trends Gauge charts for goal tracking Heat map for regional performance Funnel chart for conversion analysis Interactivity: Date range filters Region/Category drill-downs Export to PDF/Excel Email subscriptions (daily/weekly) Auto-refresh: Every 1 hour Users: C-level executives (15 users) Dashboard 2 - Sales Analytics Dashboard:\nAnalysis sections: Sales by product category (bar chart) Geographic sales distribution (map) Top 10 products by revenue (table with sparklines) Sales vs target (combo chart) Customer segmentation (tree map) Month-over-month comparison (line chart) Calculated fields: Revenue Growth % = (sum({revenue}) - sumOver(sum({revenue}), [{period} ASC], 1, 1)) / sumOver(sum({revenue}), [{period} ASC], 1, 1) * 100 YTD Revenue = sumOver(sum({revenue}), [{year}], [{year} = ${current_year}], PRE_AGG) Customer Segment = ifelse({lifetime_value} \u0026gt; 10000, \u0026#34;VIP\u0026#34;, ifelse({lifetime_value} \u0026gt; 5000, \u0026#34;High Value\u0026#34;, ifelse({lifetime_value} \u0026gt; 1000, \u0026#34;Medium Value\u0026#34;, \u0026#34;Low Value\u0026#34;))) Parameters: Date range selector Product category filter Region multi-select Minimum order value threshold Row-Level Security: Sales managers: See only their region Product managers: See only their category Analysts: See all data Users: Sales team (35 users) Dashboard 3 - Real-Time Operations Dashboard:\nLive metrics (1-minute refresh): Current active users Transactions per minute System error rate API response times Inventory alerts Data source: Kinesis Data Streams ‚Üí Athena live query Alerts configured: Error rate \u0026gt; 2% ‚Üí Email + Slack notification Response time \u0026gt; 2s ‚Üí PagerDuty alert Inventory below threshold ‚Üí Email to ops team Users: Operations team (12 users) QuickSight ML Insights:\nAnomaly detection: Automatically detect unusual patterns Detected 3 anomalies this week: Unexpected revenue spike (holiday promotion - expected) Low traffic on Tuesday (system maintenance - known) Product category shift (new product launch - explained) Forecasting: Revenue forecast for next 30 days Predicted revenue: $1.2M ¬± $150K (confidence interval) Accuracy on last month: 92% Contribution analysis: Identify key drivers Top revenue drivers: Product category (45%), Region (30%), Customer segment (15%) Embedded Analytics:\nQuickSight dashboards embedded in internal web application SSO integration with corporate IdP Custom branding and white-labeling API-based dashboard provisioning Row-level security enforced Usage: 2,500 dashboard views/week 6. Streaming Analytics Implementation Kinesis Data Streams Configuration:\nStream: clickstream-events Shards: 4 shards (4 MB/sec write, 8 MB/sec read capacity) Retention: 7 days Encryption: KMS encryption enabled Data producers: Web application (user clicks, page views) Mobile app (app events, user actions) IoT devices (sensor readings) Throughput: 3.2 MB/sec average, 6.5 MB/sec peak Records/sec: 850 average, 1,800 peak Kinesis Data Firehose Delivery:\nDelivery streams created: S3 Delivery Stream:\nDestination: s3://datalake-bronze-prod/clickstream/ Buffer size: 5 MB Buffer interval: 300 seconds Compression: Gzip Format conversion: JSON to Parquet Partitioning: year/month/day/hour Data transformation: Lambda function for enrichment OpenSearch Delivery Stream:\nDestination: OpenSearch domain analytics-cluster Index: clickstream-{yyyy-MM-dd} Buffer size: 5 MB Buffer interval: 60 seconds Retry duration: 3600 seconds Use case: Real-time dashboards and search Real-Time Processing with Lambda:\nimport json import base64 from datetime import datetime def lambda_handler(event, context): output = [] for record in event[\u0026#39;records\u0026#39;]: # Decode Kinesis data payload = base64.b64decode(record[\u0026#39;data\u0026#39;]).decode(\u0026#39;utf-8\u0026#39;) data = json.loads(payload) # Enrich data data[\u0026#39;processed_timestamp\u0026#39;] = datetime.utcnow().isoformat() data[\u0026#39;partition_key\u0026#39;] = f\u0026#34;{data[\u0026#39;user_id\u0026#39;]}_{data[\u0026#39;session_id\u0026#39;]}\u0026#34; # Classify user action if \u0026#39;purchase\u0026#39; in data.get(\u0026#39;event_type\u0026#39;, \u0026#39;\u0026#39;): data[\u0026#39;event_category\u0026#39;] = \u0026#39;conversion\u0026#39; elif \u0026#39;add_to_cart\u0026#39; in data.get(\u0026#39;event_type\u0026#39;, \u0026#39;\u0026#39;): data[\u0026#39;event_category\u0026#39;] = \u0026#39;engagement\u0026#39; else: data[\u0026#39;event_category\u0026#39;] = \u0026#39;browsing\u0026#39; # Calculate session duration if data.get(\u0026#39;event_type\u0026#39;) == \u0026#39;session_end\u0026#39;: session_duration = data.get(\u0026#39;timestamp\u0026#39;) - data.get(\u0026#39;session_start_time\u0026#39;) data[\u0026#39;session_duration_sec\u0026#39;] = session_duration # Encode back to base64 output_record = { \u0026#39;recordId\u0026#39;: record[\u0026#39;recordId\u0026#39;], \u0026#39;result\u0026#39;: \u0026#39;Ok\u0026#39;, \u0026#39;data\u0026#39;: base64.b64encode(json.dumps(data).encode(\u0026#39;utf-8\u0026#39;)).decode(\u0026#39;utf-8\u0026#39;) } output.append(output_record) return {\u0026#39;records\u0026#39;: output} Kinesis Data Analytics Application:\nApplication type: SQL-based analytics Use case: Real-time aggregations and windowing SQL query example: CREATE OR REPLACE STREAM \u0026#34;DESTINATION_SQL_STREAM\u0026#34; ( window_start TIMESTAMP, event_type VARCHAR(50), event_count INTEGER, unique_users INTEGER, avg_session_duration DOUBLE ); CREATE OR REPLACE PUMP \u0026#34;STREAM_PUMP\u0026#34; AS INSERT INTO \u0026#34;DESTINATION_SQL_STREAM\u0026#34; SELECT STREAM STEP(\u0026#34;SOURCE_SQL_STREAM_001\u0026#34;.ROWTIME BY INTERVAL \u0026#39;1\u0026#39; MINUTE) AS window_start, \u0026#34;event_type\u0026#34;, COUNT(*) AS event_count, COUNT(DISTINCT \u0026#34;user_id\u0026#34;) AS unique_users, AVG(\u0026#34;session_duration_sec\u0026#34;) AS avg_session_duration FROM \u0026#34;SOURCE_SQL_STREAM_001\u0026#34; GROUP BY STEP(\u0026#34;SOURCE_SQL_STREAM_001\u0026#34;.ROWTIME BY INTERVAL \u0026#39;1\u0026#39; MINUTE), \u0026#34;event_type\u0026#34;; Output: Send aggregated data to: Kinesis Data Streams (for further processing) Lambda (for alerting) OpenSearch (for real-time dashboards) Streaming Analytics Metrics:\nEvents processed: 2.4M events/day Average latency: 1.2 seconds (ingestion to dashboard) Processing cost: $85/day ($2,550/month) Data volume: 3.8 GB/day compressed 7. Comprehensive Monitoring and Operations CloudWatch Dashboards:\nData Pipeline Health Dashboard:\nGlue job success/failure rates ETL job durations Data quality check pass rates S3 bucket storage growth Athena query metrics Kinesis stream throughput Cost Monitoring Dashboard:\nS3 storage costs by layer Glue job execution costs Athena query costs by workgroup Kinesis streaming costs QuickSight user costs Daily cost breakdown CloudWatch Alarms:\nGlue job failures ‚Üí SNS ‚Üí Email + Slack Athena cost threshold exceeded ‚Üí Budget alert Kinesis stream throttling ‚Üí Auto-scaling trigger Data freshness SLA breach ‚Üí PagerDuty S3 storage \u0026gt; 10 TB ‚Üí Cost review notification Data Lineage and Metadata:\nAWS Glue Data Catalog: Source of truth for schemas Table metadata: 347 tables documented Column-level metadata: Data types, descriptions, PII tags Lineage tracking: Source ‚Üí Bronze ‚Üí Silver ‚Üí Gold Impact analysis: Which dashboards affected by schema changes 8. Week 9 Summary and Key Metrics Data Platform Metrics:\nMetric Value Total data ingested 42 GB/day Data sources 12 sources Tables in catalog 347 tables Glue jobs 18 jobs Athena queries/day 450 queries QuickSight dashboards 15 dashboards QuickSight users 195 users Dashboard views/week 2,500 views Data freshness (avg) 12 minutes Query performance (p95) 8 seconds Monthly data platform cost $3,200 Cost per GB processed $0.07 Business Value Delivered:\nReduced time-to-insight from 2 days to 15 minutes (99% improvement) Self-service analytics: 85% of reports now self-generated by business users Data democratization: 195 users with data access (up from 12 analysts) Cost savings: $8,500/month vs traditional DW solution Decision speed: Real-time dashboards enable faster business decisions Technical Achievements:\nImplemented modern medallion architecture 100% serverless data platform (no infrastructure management) Scalable from GB to PB (tested up to 500 GB/day) Comprehensive data governance with Lake Formation Real-time and batch analytics coexistence Advanced ML insights integrated Lessons Learned:\nPartitioning strategy is critical for query performance and cost Parquet format significantly reduces storage and query costs Data quality checks should be implemented early in the pipeline Incremental processing is more efficient than full reprocessing SPICE in QuickSight provides fast dashboard performance Row-level security essential for data governance Athena workgroups enable cost control and usage tracking Streaming and batch architectures complement each other Next Steps:\nImplement ML models with SageMaker using Gold layer data Add more data sources (social media, marketing platforms) Implement data quality scorecards Expand QuickSight embedded analytics Deploy DataZone for data marketplace Implement automated data cataloging with AI "},{"uri":"https://thienluhoan.github.io/workshop-template/1-worklog/1.10-week10/","title":"Week 10 Worklog","tags":[],"description":"","content":" ‚ö†Ô∏è Note: The following information is for reference purposes only. Please do not copy verbatim for your own report, including this warning.\nWeek 10 Objectives: Learn about AI/ML services on AWS. Understand machine learning workflow using AWS services. Practice building and deploying ML models with SageMaker and other AI services. Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 2 - Overview of AI/ML on AWS - Learn about ML support services: SageMaker, Rekognition, Comprehend, Kendra, Translate, Polly 10/11/2025 10/11/2025 AWS Journey 3 - Practice with Amazon SageMaker: + Create Notebook Instance + Train simple models (Linear Regression / Image Classification) + Deploy endpoint and test predictions 11/11/2025 11/11/2025 AWS Journey 4 - Get familiar with Amazon Rekognition - Demo face and object recognition in images/videos - Integrate Rekognition API into a small web application 12/11/2025 12/11/2025 AWS Journey 5 - Practice Amazon Comprehend (natural language processing) - Experiment with Amazon Kendra (contextual intelligent search) - Compare advantages and limitations of each service 13/11/2025 13/11/2025 AWS Journey 6 - Summarize Week 10 knowledge: + AI/ML model development process on AWS + Real-world applications of AI/ML in business + Write practice results report and expansion directions 14/11/2025 14/11/2025 AWS Journey Week 10 Achievements: Learned about AI/ML on AWS:\nUnderstood machine learning concepts and workflow Reviewed AWS AI/ML service portfolio Studied SageMaker, Rekognition, Comprehend, Kendra capabilities Practiced with Amazon SageMaker:\nCreated Notebook Instance for ML development Trained simple models (Linear Regression, Image Classification) Deployed model endpoints for predictions Tested inference requests to endpoints Understood SageMaker workflow and pricing Worked with Amazon Rekognition:\nImplemented face detection and recognition Tested object and scene detection in images Analyzed video content for objects and activities Integrated Rekognition API into web application Explored use cases for computer vision Experimented with NLP services:\nUsed Amazon Comprehend for sentiment analysis Extracted key phrases and entities from text Tested Amazon Kendra for intelligent search Compared capabilities of different NLP services Identified real-world applications for AI/ML in business Used AWS GuardDuty:\nEnabled GuardDuty for threat detection Reviewed finding types Viewed sample findings Understood severity levels (Low, Medium, High) Learned about common threats detected Explored AWS Security Hub:\nEnabled Security Hub Viewed consolidated security findings Reviewed security scores Understood integration with GuardDuty and Config Exported findings report "},{"uri":"https://thienluhoan.github.io/workshop-template/1-worklog/1.11-week11/","title":"Week 11 Worklog","tags":[],"description":"","content":" ‚ö†Ô∏è Note: The following information is for reference purposes only. Please do not copy verbatim for your own report, including this warning.\nWeek 11 Objectives: Learn about Modernization and Serverless concepts. Understand monolithic vs microservices architectures. Practice building serverless applications with Lambda, API Gateway, and other AWS services. Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 2 - Introduction to Modernization and Serverless concepts - Compare monolithic and microservices architectures - Analyze the benefits of transitioning to serverless 17/11/2025 17/11/2025 https://cloudjourney.awsstudygroup.com/ 3 - Practice AWS Lambda: create functions, configure triggers, view logs in CloudWatch - Deploy basic API processing logic using Lambda 18/11/2025 18/11/2025 https://cloudjourney.awsstudygroup.com/ 4 - Integrate API Gateway with Lambda to create REST API - Connect data with DynamoDB (CRUD operations) - Test API using Postman 19/11/2025 19/11/2025 https://cloudjourney.awsstudygroup.com/ 5 - Configure Cognito for user authentication (user pool, token) - Integrate Cognito authentication into API Gateway - Manage access permissions via IAM Role 20/11/2025 20/11/2025 https://cloudjourney.awsstudygroup.com/ 6 - Practice deploying a complete Serverless App using AWS SAM (Serverless Application Model) - Testing, logging, and performance optimization - Summarize knowledge and weekly report 21/11/2025 21/11/2025 https://cloudjourney.awsstudygroup.com/ Week 11 Achievements: Understood Serverless Architecture:\nLearned serverless concepts and benefits Compared monolithic vs microservices architectures Studied advantages of serverless: No server management Auto-scaling Pay-per-use pricing Faster time to market Practiced AWS Lambda:\nCreated Lambda functions using Python Configured function triggers (API Gateway, S3, CloudWatch Events) Viewed execution logs in CloudWatch Set environment variables and timeout settings Deployed basic API processing logic Built REST API with API Gateway and Lambda:\nCreated REST API using API Gateway Integrated Lambda functions as backend Implemented CRUD operations with DynamoDB Tested API endpoints using Postman Configured request/response mapping Implemented Authentication with Cognito:\nCreated Cognito User Pool Configured user authentication flow Integrated Cognito with API Gateway Used JWT tokens for authorization Managed IAM roles for access control Deployed Serverless Application:\nLearned AWS SAM framework basics Created SAM template (template.yaml) Deployed complete serverless app using SAM CLI Tested application functionality Reviewed CloudWatch logs for monitoring "},{"uri":"https://thienluhoan.github.io/workshop-template/1-worklog/1.12-week12/","title":"Week 12 Worklog","tags":[],"description":"","content":" ‚ö†Ô∏è Note: The following information is for reference purposes only. Please do not copy verbatim for your own report, including this warning.\nWeek 12 Objectives: Review all core AWS services learned during the internship. Design and implement a final project integrating multiple AWS services. Prepare final report and presentation. Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 2 - Review all core services: EC2, S3, RDS, DynamoDB, IAM, VPC, Lambda, CloudWatch, CloudFront, API Gateway - Define requirements and architecture for the final project 24/11/2025 24/11/2025 https://cloudjourney.awsstudygroup.com/ 3 - Start project deployment: + Design VPC, subnet, security groups + Configure S3, CloudFront, RDS/DynamoDB (depending on project) 25/11/2025 25/11/2025 https://cloudjourney.awsstudygroup.com/ 4 - Continue project implementation: + Build backend using Lambda / API Gateway or EC2 (depending on architecture) + Connect database and process data + Integrate CloudWatch for monitoring 26/11/2025 26/11/2025 https://cloudjourney.awsstudygroup.com/ 5 - Complete the project: + Add Cognito authentication if needed + Finalize CI/CD pipeline (CodePipeline/CodeBuild) + End-to-end system testing 27/11/2025 27/11/2025 https://cloudjourney.awsstudygroup.com/ 6 - Write final report - Prepare presentation (architecture, service selection rationale, cost, security) - Summarize the entire learning journey and self-evaluate capabilities 28/11/2025 28/11/2025 https://cloudjourney.awsstudygroup.com/ Week 12 Achievements: Reviewed Core AWS Services:\nEC2, S3, RDS, DynamoDB IAM, VPC, Security Groups Lambda, API Gateway CloudWatch, CloudFront Cognito, CodePipeline Planned Final Project:\nDefined project requirements Designed system architecture Selected appropriate AWS services Created architecture diagram Planned implementation steps Implemented Project Infrastructure:\nDesigned VPC with public and private subnets Configured security groups and network ACLs Set up S3 buckets for storage Configured CloudFront for content delivery Created RDS database instance Built Application Backend:\nImplemented Lambda functions for business logic Created API Gateway REST endpoints Connected Lambda to database Configured CloudWatch logs and monitoring Set up error handling and retry logic Completed Project Features:\nIntegrated Cognito for user authentication Set up CodePipeline for CI/CD Performed end-to-end testing Fixed bugs and optimized performance Verified all features working correctly Prepared Final Deliverables:\nWrote comprehensive project report Created presentation slides covering: Architecture overview Service selection rationale Security implementation Cost estimation Summarized 12-week learning journey Self-evaluated technical capabilities gained "},{"uri":"https://thienluhoan.github.io/workshop-template/categories/","title":"Categories","tags":[],"description":"","content":""},{"uri":"https://thienluhoan.github.io/workshop-template/tags/","title":"Tags","tags":[],"description":"","content":""}]